{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to PyToolbox","text":"<p>This is a collection of tools and resources that I have found helpful for various jobs.  This is in no way complete and many sections have not been filled out yet.</p> <p>Go to ToolBox</p>"},{"location":"bi/bi/","title":"Business Intelligence","text":"<p>Business intelligence (BI) refers to the procedural and technical infrastructure that collects, stores, and analyzes the data produced by a company\u2019s activities.</p> <p>BI is a broad term that encompasses data mining, process analysis, performance benchmarking, and descriptive analytics. BI parses all the data generated by a business and presents easy-to-digest reports, performance measures, and trends that inform management decisions.</p> <ul> <li>BI represents the technical infrastructure that collects, stores, and analyzes company data.</li> <li>BI parses data and produces reports and information that help managers to make better decisions.</li> <li>Software companies produce BI solutions for companies that wish to make better use of their data.</li> <li>BI tools and software come in a wide variety of forms such as spreadsheets, reporting/query software, data visualization software, data mining tools, and online analytical processing (OLAP).</li> <li>Self-service BI is an approach to analytics that allows individuals without a technical background to access and explore data.</li> </ul> <p>Definition</p>"},{"location":"bi/bi/#bi-tools","title":"BI Tools","text":""},{"location":"bi/bi/#rawgraphs","title":"RawGraphs","text":"<p>RAW Graphs is an open source data visualization framework built with the goal of making the visual representation of complex data easy for everyone.</p> <p>Primarily conceived as a tool for designers and vis geeks, RAW Graphs aims at providing a missing link between spreadsheet applications (e.g. Microsoft Excel, Apple Numbers, OpenRefine) and vector graphics editors (e.g. Adobe Illustrator, Inkscape, Sketch).</p> <p>Documentation</p> <p>Examples </p> <p></p> <p>Use the Live App</p>"},{"location":"bi/bi/#sqliteviz","title":"Sqliteviz","text":"<p>Sqliteviz is a single-page offline-first PWA for fully client-side visualisation of SQLite databases or CSV files.</p> <p>With sqliteviz you can: - run SQL queries against a SQLite database and create Plotly charts and pivot tables based on the result sets - import a CSV file into a SQLite database and visualize imported data - export result set to CSV file - manage inquiries and run them against different databases - import/export inquiries from/to a JSON file - export a modified SQLite database - use it offline from your OS application menu like any other desktop app</p> <p>Documentation</p> <p>Examples</p> <p></p> <p>Use the Live App</p>"},{"location":"bi/bi/#streamlit","title":"Streamlit","text":"<p>Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes you can build and deploy powerful data apps.</p> <p>Documentation</p> <p>Examples</p> <p></p> <p><pre><code>import streamlit as st\nimport pandas as pd\nimport numpy as np\n\nst.title('Uber pickups in NYC')\nDATE_COLUMN = 'date/time'\nDATA_URL = ('https://s3-us-west-2.amazonaws.com/'\n         'streamlit-demo-data/uber-raw-data-sep14.csv.gz')\n\ndef load_data(nrows):\n    data = pd.read_csv(DATA_URL, nrows=nrows)\n    data[DATE_COLUMN] = pd.to_datetime(data[DATE_COLUMN])\n    return data\n\ndata = load_data(10000)\n</code></pre> Create a bar chart <pre><code>hist_values = np.histogram(\n    data[DATE_COLUMN].dt.hour, bins=24, range=(0,24)\n)[0]\n\nst.bar_chart(hist_values)\n</code></pre> Plot data on a map with sliding filter <pre><code>hour_to_filter = st.slider('hour', 0, 23, 17)  # min: 0h, max: 23h, default: 17h\nfiltered_data = data[data[DATE_COLUMN].dt.hour == hour_to_filter]\n\nst.subheader(f'Map of all pickups at {hour_to_filter}:00')\n\nst.map(filtered_data)\n</code></pre></p> <p>Use the Live App</p>"},{"location":"bi/bi/#lightdash","title":"LightDash","text":"<p>Connect Lightdash to your dbt project, add metrics directly in your data transformation layer, then create and share your insights with your team.</p> <p>Documentation</p> <p>Examples</p> <p></p> <p>Use the Live Demo App</p>"},{"location":"bi/bi/#apache-superset","title":"Apache Superset","text":"<p>Superset is fast, lightweight, intuitive, and loaded with options that make it easy for users of all skill sets to explore and visualize their data, from simple line charts to highly detailed geospatial charts.</p> <p>Documentation</p> <p>Examples</p> <p></p> <p></p>"},{"location":"blogs/blogs/","title":"Blogs and other External Resources","text":"<p>Here's some resources I've found and like to reference, I'll try not to list extremely common sources here.</p>"},{"location":"blogs/blogs/#python","title":"Python","text":"<ul> <li>https://www.fullstackpython.com/</li> <li>https://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g</li> <li>https://docs.python-guide.org/</li> <li>https://pbpython.com/</li> </ul>"},{"location":"blogs/blogs/#data-engineering","title":"Data Engineering","text":"<ul> <li>https://www.startdataengineering.com/</li> <li>https://medium.com/@rchang/a-beginners-guide-to-data-engineering-part-i-4227c5c457d7</li> <li>https://www.startdataengineering.com/</li> <li>https://github.com/zappa/Zappa</li> <li>https://shancarter.github.io/mr-data-converter/</li> </ul>"},{"location":"blogs/blogs/#aiml","title":"AI/ML","text":"<ul> <li>https://aegeorge42.github.io/</li> <li>https://www.eleuther.ai/</li> <li>https://riverml.xyz/latest/</li> <li>https://pypi.org/project/pybaobabdt/</li> <li>https://becominghuman.ai/</li> <li>https://explained.ai/decision-tree-viz/index.html</li> <li>https://medium.com/coders-camp/40-machine-learning-algorithms-with-python-3defd764b961</li> <li>https://happytransformer.com/</li> <li>https://pycaret.org/</li> <li>https://developers.google.com/machine-learning/guides/rules-of-ml</li> <li>https://realpython.com/python-ai-neural-network/</li> </ul>"},{"location":"blogs/blogs/#procedurally-generated-images","title":"Procedurally Generated Images","text":"<ul> <li>https://github.com/enjeck/Blobby</li> <li>https://www.generativehut.com/post/robots-and-generative-art-and-python-oh-my</li> <li>https://github.com/nft-fun/generate-bitbirds</li> <li>https://cryptopunksnotdead.github.io/pixelart.js/editor/</li> <li>https://robohash.org/</li> </ul>"},{"location":"blogs/blogs/#visualizationscolors","title":"Visualizations/Colors","text":"<ul> <li>https://www.python-graph-gallery.com/</li> <li>https://rampgenerator.com/</li> <li>https://imagecolorpicker.com/</li> <li>https://matplotlib.org/</li> <li>https://bokeh.org/</li> <li>https://plotly.com/python/</li> <li>https://seaborn.pydata.org/</li> <li>https://www.machinelearningplus.com</li> <li>https://altair-viz.github.io/index.html</li> <li>https://www.thecolorapi.com/</li> <li>https://feathericons.com/</li> <li>https://diagrams.mingrammer.com</li> <li>https://github.com/scottrogowski/code2flow</li> </ul>"},{"location":"blogs/blogs/#cryptography","title":"Cryptography","text":"<ul> <li>https://blog.cloudflare.com/a-relatively-easy-to-understand-primer-on-elliptic-curve-cryptography/</li> <li>https://blintzbase.com/posts/pir-and-fhe-from-scratch/</li> <li></li> </ul>"},{"location":"blogs/blogs/#jshtmlcss","title":"JS/HTML/CSS","text":"<ul> <li>https://mrcoles.com/bookmarklet/</li> <li>https://htmx.org/</li> <li>https://neutralino.js.org/</li> <li>https://mermaid-js.github.io/mermaid/#/</li> </ul>"},{"location":"blogs/blogs/#misc-for-now","title":"Misc (for now)","text":"<ul> <li>https://crontab.guru/</li> <li>https://github.com/sindresorhus/awesome</li> <li>https://ivizri.com/</li> <li>https://smirkygraphs.github.io/</li> <li>http://bluoceans.co/</li> <li>https://article-summary.herokuapp.com/</li> <li>https://projects.brianmccrillis.com/</li> <li>https://json-humans.fly.dev/</li> </ul>"},{"location":"blogs/snippets/","title":"Cool Snippets of Python Code","text":"<p>Here's some snippets of Code that are cool to reuse.</p>"},{"location":"blogs/snippets/#python","title":"Python","text":"<p>I'm convinced that <code>map</code>, <code>filter</code>, <code>lambda</code>, <code>zip</code>, <code>functools</code> and <code>itertools</code> are some of the most  powerful functions/tools within Python and I'd like to expand my capabilities with them and have been trying to solve coding  challenges using them exclusively. Here's some scripts using these, while they aren't all super readable in one-line  form, they are fast and easy to make into lambda functions.</p>"},{"location":"blogs/snippets/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Handling Inputs</li> <li>Combining / Splitting</li> <li>Filtering Data</li> <li>Sorting Data</li> <li>Outputting Data</li> <li>Matrix Operations</li> <li>Prime Numbers</li> <li>Regex and Text Handling</li> <li>Ciphers / Encryption</li> <li>Data Conversions</li> <li>Misc Math</li> </ul>"},{"location":"blogs/snippets/#handling-inputs","title":"Handling Inputs","text":""},{"location":"blogs/snippets/#input-multiple-lines-often-used-in-competitions","title":"Input multiple lines (often used in competitions)","text":"<pre><code># Takes in space separated integers as strings, maps to ints\nK, M = map(int, input().split())\n# 5 100\n\n# takes in multiple space separated values and creates a dictionary with index key and input as values\ninput_map = dict(enumerate(map(int, input().split())))\n# {0: 5, 1: 100}\n</code></pre>"},{"location":"blogs/snippets/#combiningsplitting-data","title":"Combining/Splitting Data","text":""},{"location":"blogs/snippets/#create-a-list-of-combined-strings-from-multiple-iterables","title":"Create a list of combined strings from multiple iterables","text":"<pre><code>list(map(lambda a,b: f\"{a}-{b}\", ('apple', 'banana', 'cherry'), ('orange', 'lemon', 'pineapple')))\n# ['apple-orange', 'banana-lemon', 'cherry-pineapple']\n</code></pre>"},{"location":"blogs/snippets/#join-two-lists-together-into-list-of-tuples","title":"Join two lists together into list of tuples","text":"<pre><code>zipped_list = list(zip([1,2,3], [4,5,6]))\n# [(1, 4), (2, 5), (3, 6)]\n</code></pre>"},{"location":"blogs/snippets/#join-two-lists-together-into-list-of-lists","title":"Join two lists together into list of lists","text":"<pre><code>zipped_list = list(map(list, zip([1,2,3], [4,5,6])))\n# [[1, 4], [2, 5], [3, 6]]\n</code></pre>"},{"location":"blogs/snippets/#unzip-a-list-of-tuples","title":"Unzip a list of tuples","text":"<pre><code>list(zip(*zipped_list))\n# [(1, 2, 3), (4, 5, 6)]\n</code></pre>"},{"location":"blogs/snippets/#unzip-list-of-lists","title":"Unzip list of lists","text":"<pre><code>list(map(list, zip(*zipped_list)))\n# [[1, 2, 3], [4, 5, 6]]\n</code></pre>"},{"location":"blogs/snippets/#split-list-into-list-of-n-sized-lists","title":"Split List into List of N sized Lists","text":"<pre><code>A = [5, 4, 3, 2, 1, 0]\n\n# split as is\nsplit_list_n = lambda A, n: [A[i:i + n] for i in range(0, len(A), n)]\n# [[5, 4, 3], [2, 1, 0]]\n\n# sort the results before split\nsplit_list_n_sorted = lambda A, n: [sorted(A[i:i+n]) for i in range(0, len(A), n)]\n\nsplit_list_n(A, 3)\n# [[3, 4, 5], [0, 1, 2]]\n</code></pre>"},{"location":"blogs/snippets/#reshape-list-into-n-lists-of-lists","title":"Reshape List into N Lists of Lists","text":"<pre><code>A = [0, 1, 2, 3, 4, 5]\n\nreshape = lambda A, n: [A[i:i + int(len(A)/n)] for i in range(0, len(A), int(len(A)/n))]\n\nreshape(A, 2)\n# [[0, 1, 2], [3, 4, 5]]\nreshape(A, 3)\n# [[0, 1], [2, 3], [4, 5]]\n</code></pre>"},{"location":"blogs/snippets/#create-dictionary-with-key-values-from-two-lists","title":"Create dictionary with key, values from two lists","text":"<pre><code>dict(zip(['a','b','c'], [1,2,3]))\n# {'a': 1, 'b': 2, 'c': 3}\n</code></pre>"},{"location":"blogs/snippets/#create-dictionary-with-index-number-keys","title":"Create dictionary with Index number keys","text":"<pre><code>some_list = ['Alice', 'Liz', 'Bob']\n# One-Line Statement Creating a Dict:\ndict(enumerate(some_list))\n# {0: 'Alice', 1: 'Liz', 2: 'Bob'}\n\n# or as a function\nindexed_dict = lambda some_list: dict(enumerate(some_list))\nindexed_dict(some_list)\n# {0: 'Alice', 1: 'Liz', 2: 'Bob'}\n</code></pre>"},{"location":"blogs/snippets/#extended-iterable-unpacking","title":"Extended Iterable Unpacking","text":"<p>Source <pre><code>first, *middle, last = [1, 2, 3, 4, 5]\n\nprint(first)  # 1\nprint(middle) # [2, 3, 4]\nprint(last)   # 5\n\n*the_first_three, second_last, last = [1, 2, 3, 4, 5]\n\nprint(the_first_three) # [1, 2, 3]\nprint(second_last)     # 4\nprint(last)            # 5\n</code></pre></p> <p></p>"},{"location":"blogs/snippets/#filtering-data","title":"Filtering Data","text":""},{"location":"blogs/snippets/#filter-a-list-by-min-value","title":"Filter a list by min value","text":"<pre><code>ages = [5, 12, 17, 18, 24, 32]\n\n[age for age in ages if age &gt;= 18]\n# or\nlist(filter(lambda age: age &gt;= 18, ages))\n# [18, 24, 32]\n</code></pre>"},{"location":"blogs/snippets/#sorting-data","title":"Sorting Data","text":""},{"location":"blogs/snippets/#sort-list-of-dictionaries-by-specific-value","title":"Sort list of dictionaries by specific value","text":"<pre><code>cars = [{'car': 'Ford', 'year': 2005}, {'car': 'Mitsubishi', 'year': 2000}, {'car': 'BMW', 'year': 2019}]\n\ncars.sort(key = lambda x: x['year'])\n# [{'car': 'Mitsubishi', 'year': 2000}, {'car': 'Ford', 'year': 2005}, {'car': 'VW', 'year': 2011}]\n\n# pretty print sorted list of dictionaries\n# Change order by adding `reverse=True` to `sorted()`\nprint(json.dumps(sorted(cars, key=lambda x: x['year']), indent=4))\n# [\n#     {\n#         \"car\": \"Mitsubishi\",\n#         \"year\": 2000\n#     },\n#     {\n#         \"car\": \"Ford\",\n#         \"year\": 2005\n#     },\n#     {\n#         \"car\": \"BMW\",\n#         \"year\": 2019\n#     }\n# ]\n</code></pre>"},{"location":"blogs/snippets/#sorting-dictionary-values","title":"Sorting dictionary values","text":"<pre><code>some_dict = {'a': 5, 'b': 8, 'c': 1}\nsort_dict = lambda some_dict: dict(sorted(some_dict.items(), key=lambda x: x[1], reverse=True))\n\nsort_dict(some_dict)\n# {'b': 8, 'a': 5, 'c': 1}\n</code></pre>"},{"location":"blogs/snippets/#outputting-data","title":"Outputting Data","text":""},{"location":"blogs/snippets/#one-liner-to-output-a-pretty-json-file","title":"One liner to output a pretty JSON file","text":"<pre><code>print(json.dumps(data, indent=4), file=open(\"path/to/data.json\", 'w'))\n</code></pre>"},{"location":"blogs/snippets/#one-liner-to-output-a-jsonl-file-run-multiple-times-to-keep-adding","title":"One liner to output a JSONL file, run multiple times to keep adding","text":"<pre><code>print(json.dumps(data), file=open(\"path/to/data.jsonl\", 'a'))\n</code></pre>"},{"location":"blogs/snippets/#cli-command-to-pretty-print-json-in-the-terminal","title":"CLI command to pretty print JSON in the terminal","text":"<pre><code>python -m json.tool data.json\n</code></pre>"},{"location":"blogs/snippets/#matrix-operations","title":"Matrix Operations","text":""},{"location":"blogs/snippets/#create-a-list-of-sums-of-two-lists-or-iterables","title":"Create a list of sums of two lists or iterables","text":"<pre><code>list_one, list_two = [1,2,3], [4,5,6]\ncross_sum = lambda list_one, list_two: list(map(sum, zip(list_one, list_two)))\n\ncross_sum(list_one, list_two)\n# [5, 7, 9]\n</code></pre>"},{"location":"blogs/snippets/#get-max-value-for-each-index-number-across-multiple-lists","title":"Get max value for each index number across multiple lists","text":"<pre><code>list_one, list_two = [10,12,32], [14,5,16]\nmax_index_values = lambda list_one, list_two: list(map(max, zip(list_one, list_two)))\n\nmax_index_values(list_one, list_two)\n# [14, 12, 32]\n</code></pre>"},{"location":"blogs/snippets/#get-pair-with-smallestlargest-difference-between-two-iterables","title":"Get pair with smallest/largest difference between two iterables","text":"<p>g4g <pre><code># smallest\nsmallest_pair = min(some_list, key = lambda sub: abs(sub[1] - sub[0]))\n# largest\nlargest_pair = max(some_list, key = lambda sub: abs(sub[1] + sub[0]))\n</code></pre></p>"},{"location":"blogs/snippets/#dot-product-of-two-lists","title":"Dot product of two lists","text":"<p> <pre><code>A, B = [2,7,1], [8,2,8]\n\n# Method 1\ndot = lambda A, B: sum(a * b for a, b in zip(A, B))\n\n# Method 2\ndot = lambda A, B: sum(map(lambda x: x[0] * x[1], zip(A, B)))\n\ndot(A, B)\n# 38\n</code></pre></p>"},{"location":"blogs/snippets/#matrix-product","title":"Matrix Product","text":"<p> <pre><code>A = [[1, 2], \n     [3, 4]]\n\nB = [[5, 6],\n     [7, 8]]\n\nmatrix_product = lambda A, B: [[sum(ea * eb for ea, eb in zip(a, b)) for b in zip(*B)] for a in A]\n\nmatrix_product(A,B)\n# [[19, 22],\n#  [43, 50]]\n</code></pre></p>"},{"location":"blogs/snippets/#matrix-addition","title":"Matrix Addition","text":"<p> <pre><code>A = [[1, 2], \n     [3, 4]]\n\nB = [[5, 6],\n     [7, 8]]\n\n# Method 1\nmatrix_add = lambda A, B: list(map(lambda x, y: [a + b for a, b in zip(x, y)], A, B))\n\n# Method 2\nmatrix_add = lambda a, b: [\n    list(map(sum, list(zip(*list(zip(a, b))[i])))) for i, v in enumerate(zip(a, b))\n]\n\nmatrix_add(A, B)\n# [[6,  8],\n#  [10, 12]]\n</code></pre></p>"},{"location":"blogs/snippets/#matrix-subtraction","title":"Matrix Subtraction","text":"<p> <pre><code>A = [[1,2,3],\n     [4,5,6],\n     [7,8,9]]\n\nB = [[9,8,7],\n     [6,5,4],\n     [3,2,1]]\n\n# Method 1\nmatrix_subtract = lambda A, B: list(map(lambda x, y: [a - b for a, b in zip(x, y)], A, B))\n\n# Method 2\nmatrix_subtract = lambda A, B: [\n    list(map(lambda x: x[0] - x[1], list(zip(*list(zip(A, B))[i])))) for i, v in enumerate(zip(A, B))\n]\n\nmatrix_subtract(A, B)\n# [[-8, -6, -4],\n#  [-2,  0,  2], \n#  [ 4,  6,  8]]\n</code></pre></p>"},{"location":"blogs/snippets/#scalar-multiplication","title":"Scalar Multiplication","text":"<p> <pre><code>A = [[2,1],\n     [6,5]]\n\nB = [1,2,3,4,5]\n# Method 2 (Broken)\n# scalar_multiply = lambda a, b: [\n#     list(map(lambda x: x[i] * b, zip(*a))) for i,v in enumerate(a)\n# ]\n\n# Method 1\nscalar_multiply = lambda A, B: [[sum(ea * eb for ea, eb in zip(a, b)) for b in zip(*B)] for a in A] if isinstance(B, list) else [sum(ea * B for ea in a) for a in A]\n\nscalar_multiply = (\n    lambda A, B: [[sum(ea * eb for ea, eb in zip(a, b)) for b in zip(*B)] for a in A]\n    if isinstance(B, list)\n    else [sum(ea * B for ea in a) for a in A]\n)\n\n\nscalar_multiply(A, 2)\n# [[4, 2],\n#  [12, 10]]\n\nscalar_multiply(B, 2)\n</code></pre></p>"},{"location":"blogs/snippets/#cosine-similarity","title":"Cosine Similarity","text":"<pre><code>A, B = [3, 2, 0, 5], [1, 0, 0, 0]\n\ndot = lambda A, B: sum(x * y for x, y in zip(A, B))\ncosine_similarity = (lambda A, B: sum(x * y for x, y in zip(A, B)) / (sum([i**2 for i in A])**(1/2) * sum([i**2 for i in B])**(1/2)))\n\ncosine_similarity(A, B)\n# 0.48666426339228763\n</code></pre>"},{"location":"blogs/snippets/#jaccard-similarity","title":"Jaccard Similarity","text":"<pre><code>A = [1, 2, 3, 4]\nB = [3, 4, 5, 6]\n\njaccard = lambda A, B: len(set(A).intersection(set(B))) / len(set(A).union(set(B)))\n\njaccard(A,B)\n# 0.3333333333333333\njaccard('dog', 'doggy')\n# 0.75\n</code></pre>"},{"location":"blogs/snippets/#jaccard-similarity-n-chunks-testing-doesnt-work-well-so-far","title":"Jaccard Similarity (N Chunks) Testing, doesn't work well so far","text":"<pre><code>A = [1, 2, 3, 4]\nB = [3, 4, 5, 6]\n\njaccard_chunk = lambda A, B, size: len(\n    set([A[i : i + size] for i in range(0, len(A), size)]).intersection(\n        set([B[i : i + size] for i in range(0, len(B), size)])\n    )\n) / len(\n    set([A[i : i + size] for i in range(0, len(A), size)]).union(\n        set([B[i : i + size] for i in range(0, len(B), size)])\n    )\n)\n</code></pre>"},{"location":"blogs/snippets/#hamming-distance","title":"Hamming Distance","text":"<pre><code>A = [1,0,1,0,1,0,1,0]\nB = [0,1,0,1,1,0,1,0]\n\nhamming = lambda a, b: len(list(filter(lambda x: x[0] == x[1], zip(a,b))))\n\nhamming(A, B)\n# 4\n</code></pre>"},{"location":"blogs/snippets/#chebyshev-distance","title":"Chebyshev Distance","text":"<pre><code>A = [5, 8, 12, 9]\nB = [3, 11, 10, 17]\n\nchebyshev = lambda A, B: max([abs(a - b) for a,b in zip(A, B)])\nchebyshev(A, B)\n# 8\n</code></pre>"},{"location":"blogs/snippets/#mean-squared-errors","title":"Mean Squared Errors","text":"<p>MSE formula = (1/n) * \u03a3(actual \u2013 forecast)^2 <pre><code>A, B = [41, 45, 49, 47, 44], [43.6, 44.4, 45.2, 46, 46.8]\n\nmse = lambda A, B: sum(map(lambda x: (x[0] - x[1])**2, zip(A,B))) / len(A)\n\nmse = lambda A, B: sum((a - b) ** 2 for a, b in zip(A, B)) / len(A)\nmse(A,B)\n# 6.079999999999994\n</code></pre></p>"},{"location":"blogs/snippets/#variance","title":"Variance","text":"<pre><code>A = [9, 2, 5, 4, 12, 7, 8, 11, 9, 3, 7, 4, 12, 5, 4, 10, 9, 6, 9, 4]\nvariance = lambda A: sum(map(lambda x: (x-sum(A)/len(A))**2, A))/len(A)\n\nvariance(A)\n# 8.9\n</code></pre>"},{"location":"blogs/snippets/#sample-variance","title":"Sample Variance","text":"<pre><code>A = [9, 2, 5, 4, 12, 7]\n\nvariance_sample = lambda A: sum(map(lambda x: (x-sum(A)/len(A)) ** 2, A)) / ((len(A)-1)/1)\n\nvariance_sample(A)\n# 13.1\n</code></pre>"},{"location":"blogs/snippets/#standard-deviation","title":"Standard Deviation","text":"<pre><code>A = [9, 2, 5, 4, 12, 7, 8, 11, 9, 3, 7, 4, 12, 5, 4, 10, 9, 6, 9, 4]\nstd = lambda A: (sum(map(lambda x: (x-sum(A)/len(A))**2, A))/len(A))**0.5\n\nstd(A)\n# 2.9832867780352594\n</code></pre>"},{"location":"blogs/snippets/#sample-standard-deviation-bessels-correction","title":"Sample Standard Deviation (Bessel's Correction)","text":"<pre><code>A = [9, 2, 5, 4, 12, 7, 8, 11, 9, 3, 7, 4, 12, 5, 4, 10, 9, 6, 9, 4]\nstd_sample = lambda A: (sum(map(lambda x: (x-sum(A)/len(A)) ** 2, A)) / ((len(A)-1)/1)) ** 0.5\n\nstd_sample(A)\n# 3.6193922141707713\n</code></pre>"},{"location":"blogs/snippets/#sum-of-squared-deviations-sdam","title":"Sum of Squared Deviations (SDAM)","text":"<pre><code>A = [4, 5, 9, 10]\n\nsdam = lambda A: sum([(a - sum(A)/len(A))**2 for a in A])\n</code></pre>"},{"location":"blogs/snippets/#prime-numbers","title":"Prime Numbers","text":""},{"location":"blogs/snippets/#prime-number-check-naive","title":"Prime Number Check (Naive)","text":"<pre><code>prime = lambda n: any([i for i in range(2, n) if n % i == 0]) != True\n\nprime(27)\n# False\nprimt(5)\n# True\n</code></pre>"},{"location":"blogs/snippets/#prime-number-check-optimized","title":"Prime Number Check (Optimized)","text":"<p>Optimization: checking if ending digit divisible by 2, reduced search space by limiting to square root of n <pre><code>prime = (\n    lambda n: False\n    if n / 2 % 1 == 0\n    else any([i for i in range(2, int(n / 2) + 1) if n % i == 0]) != True\n)\n\nprime(27)\n# False\nprime(4_589_407)\n# True\n</code></pre></p>"},{"location":"blogs/snippets/#mersenne-prime-number-finder","title":"Mersenne Prime Number Finder","text":"<p>Info on 2,147,483,647 <pre><code># Flat\nprime = lambda n: False if int(str(n)[-1]) % 2 == 0 else any([i for i in range(2, int(n**0.5)+1) if n % i == 0]) != True\nmersenne = lambda n: list(filter(lambda i: i is not None, list(map(lambda x: 2**x -1 if prime(2**x -1) else None, range(1, n+1)))))\n\n# Black Format\nprime = (\n    lambda n: False\n    if int(str(n)[-1]) % 2 == 0\n    else any([i for i in range(2, int(n / 2) + 1) if n % i == 0]) != True\n)\n\nmersenne = lambda n: list(\n    filter(\n        lambda i: i is not None,\n        list(map(lambda x: 2**x - 1 if prime(2**x - 1) else None, range(1, n + 1))),\n    )\n)\n\nmersenne(31)\n# [1, 3, 7, 31, 127, 8191, 131071, 524287, 2147483647]\n</code></pre></p>"},{"location":"blogs/snippets/#sophie-germain-prime-wip","title":"Sophie Germain Prime (WIP)","text":"<pre><code># Black Format\nprime = (\n    lambda n: False\n    if n / 2 % 1 == 0\n    else any([i for i in range(2, int(n / 2) + 1) if n % i == 0]) != True\n)\n\ngermain = lambda n: list(\n    filter(\n        lambda i: i is not None,\n        list(map(lambda x: x if all([prime(x), prime(2**x - 1)]) else None, range(1, n + 1))),\n    )\n)\n</code></pre>"},{"location":"blogs/snippets/#regex-and-text-handling","title":"Regex and Text Handling","text":""},{"location":"blogs/snippets/#regex-for-zipcodes","title":"Regex for Zipcodes","text":"<p>Stackoverflow <pre><code>re.search(r\"^\\d{5}(?:[-\\s]\\d{4})?$\", some_str)\n# ^ = Start of the string.\n# \\d{5} = Match 5 digits (for condition 1, 2, 3)\n# (?:\u2026) = Grouping\n# [-\\s] = Match a space (for condition 3) or a hyphen (for condition 2)\n# \\d{4} = Match 4 digits (for condition 2, 3)\n# \u2026? = The pattern before it is optional (for condition 1)\n# $ = End of the string.\n</code></pre></p>"},{"location":"blogs/snippets/#regex-for-phone-numbers","title":"Regex for Phone Numbers","text":"<p>Stackoverflow <pre><code>re.findall(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]', some_str)\n</code></pre></p>"},{"location":"blogs/snippets/#regex-for-emails","title":"Regex for emails","text":"<pre><code>re.findall(r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-.]+\\.[a-zA-Z0-9-]+)', some_str)\n</code></pre>"},{"location":"blogs/snippets/#regex-for-urls","title":"Regex for urls","text":"<pre><code>re.findall(r'(http|ftp|https):\\/\\/([\\w\\-_]+(?:(?:\\.[\\w\\-_]+)+))([\\w\\-\\.,@?^=%&amp;:/~\\+#]*[\\w\\-\\@?^=%&amp;/~\\+#])?', some_str)\n</code></pre>"},{"location":"blogs/snippets/#regex-for-numbers","title":"Regex for numbers","text":"<pre><code>re.findall(r'[1-9](?:\\d{0,2})(?:,\\d{3})*(?:\\.\\d*[1-9])?|0?\\.\\d*[1-9]|0', some_str)\n</code></pre>"},{"location":"blogs/snippets/#regex-scanner","title":"Regex Scanner","text":"<pre><code>import re\nscanner=re.Scanner([\n  (r\"[0-9]+\",    lambda scanner,token:(\"INTEGER\", token)),\n  (r\"[a-z_]+\",   lambda scanner,token:(\"IDENTIFIER\", token)),\n  (r\"[,.]+\",     lambda scanner,token:(\"PUNCTUATION\", token)),\n  (r\"\\s+\", None), # None == skip token.\n])\n\nresults, remainder=scanner.scan(\"45 pigeons, 23 cows, 11 spiders.\")\n</code></pre>"},{"location":"blogs/snippets/#quick-ratio-rough-without-dependencies","title":"Quick Ratio (rough) without dependencies","text":"<pre><code>quick = lambda a, b: len(list(filter(lambda x: x[0] == x[1], zip(a,b)))) / (len(a + b) / 2)\n\nquick('dog', 'dog')\n# 1.0\nquick('dog', 'fog')\n# 0.6666666666666666\nquick('dog', 'cat')\n# 0.0\nquick('dog', 'dogmatic')\n# 0.5454545454545454\n</code></pre>"},{"location":"blogs/snippets/#gestalt-pattern-matching","title":"Gestalt Pattern Matching","text":"<pre><code>gestalt = lambda word, compared: (2 * (len([i[0] for i in zip(word, compared) if i[0] == i[1]])) / (len(word) + len(compared))) if isinstance(compared, str) else [{target: (2 * (len([i[0] for i in zip(word, target) if i[0] == i[1]])) / (len(word) + len(target)))} for target in compared]\n</code></pre>"},{"location":"blogs/snippets/#frequency-map-of-character-counts-in-a-string","title":"Frequency map of character counts in a string","text":"<pre><code>freq_map = lambda some_str: dict(zip(some_str, map(lambda x: some_str.count(x), some_str)))\n\nfreq_map('Hello World')\n# {'H': 1, 'e': 1, 'l': 3, 'o': 2, ' ': 1, 'W': 1, 'r': 1, 'd': 1}\n</code></pre>"},{"location":"blogs/snippets/#strip-non-alphabet-characters-from-strings","title":"Strip non alphabet characters from strings","text":"<pre><code>remove_chars = lambda some_str: \"\".join([x for x in some_str if x.isalpha() or x == \" \"])\n# or\nremove_chars = lambda some_str: \"\".join((filter(lambda x: x.isalpha() or x == \" \", some_str)))\nremove_chars(\")#$Hello3 World@$)\")\n# Hello World\n</code></pre>"},{"location":"blogs/snippets/#frequency-map-of-word-counts-in-a-string-rough","title":"Frequency map of word counts in a string (rough)","text":"<pre><code># Flat\nword_count = lambda some_text: dict(zip(\"\".join([x.lower() for x in some_text if x.isalpha() or x == \" \"]).split(), map(lambda x: some_text.count(x), some_text.split())))\n\n# Black Format\nword_count = lambda some_text: dict(\n    zip(\n        \"\".join([x.lower() for x in some_text if x.isalpha() or x == \" \"]).split(),\n        map(lambda x: some_text.count(x), some_text.split()),\n    )\n)\n\nword_count(\"the quick brown fox jumps over the lazy dog\")\n# {'the': 2, 'quick': 1, 'brown': 1, 'fox': 1, 'jumps': 1, 'over': 1, 'lazy': 1, 'dog': 1}\n</code></pre>"},{"location":"blogs/snippets/#spongebob-case-text","title":"Spongebob Case text","text":"<p>convert text into SpOnGeBoB cAsE <pre><code># Flat\nspongebob_case = lambda word: \"\".join(map(lambda i: i[1].upper() if i[0] % 2 != 0 else i[1].lower(), enumerate(word)))\n\nspongebob_case = lambda word: \"\".join(s.upper() if i &amp; 1 else s.lower() for i, s in enumerate(word))\n\n# Black Format\nspongebob_case = lambda word: \"\".join(\n        map(lambda i: i[1].upper() if i[0] % 2 != 0 else i[1].lower(), enumerate(word))\n)\n\nspongebob_case('hello world')\n# hElLo wOrLd\n</code></pre></p>"},{"location":"blogs/snippets/#proper-nouns-naive","title":"Proper Nouns (naive)","text":"<pre><code>proper_nouns = re.findall(r'\\b(?:[A-Z][a-z]+)\\b', text)\n</code></pre>"},{"location":"blogs/snippets/#verbs-naive","title":"Verbs (naive)","text":"<pre><code>verbs = re.findall(r'\\b(?:[A-z]+ing|[A-z]+ed|[A-z]+en|[A-z]+s)\\b', text)\n</code></pre>"},{"location":"blogs/snippets/#palindrome","title":"Palindrome","text":"<pre><code>word = \"racecar\"\npalindrome = lambda word: word.lower() == \"\".join(reversed(word.lower()))\n# or\npalindrome = lambda word: word.lower() == word.lower()[::-1]\n\npalindrome(word)\n# True\n</code></pre>"},{"location":"blogs/snippets/#random-social-security-number","title":"Random Social Security Number","text":"<pre><code>ssn = lambda: f\"{str(random.randint(0,999)).zfill(3)}-{str(random.randint(0,99)).zfill(2)}-{str(random.randint(0,999)).zfill(4)}\"\n\nssn()\n# 885-92-0191\n</code></pre>"},{"location":"blogs/snippets/#random-phone-number","title":"Random Phone number","text":"<pre><code>phone = lambda: f\"+{random.randint(1,99)}-{str(random.randint(0,999)).zfill(3)}-{str(random.randint(0,999)).zfill(2)}-{str(random.randint(0,999)).zfill(4)}\"\n\nphone()\n# +4-267-62-0151\n</code></pre>"},{"location":"blogs/snippets/#ciphers-encryption","title":"Ciphers / Encryption","text":""},{"location":"blogs/snippets/#rot13-cipher-in-a-one-line-lambda","title":"ROT13 Cipher in a one line lambda","text":"<pre><code>rot13 = lambda word: \"\".join(map(lambda l: chr(((ord(l)-84) % 26) + 97), word))\nrot13('weattackatdawn')\n# jrnggnpxngqnja\nrot13('jrnggnpxngqnja')\n# weattackatdawn\n</code></pre>"},{"location":"blogs/snippets/#xor-cipher","title":"XOR Cipher","text":"<pre><code>xor_cipher = lambda message, key: \"\".join([chr(ord(m) ^ ord(k)) for (m,k) in zip(message, key*len(message))])\n\ncipher = xor_cipher('Hello world!', 's3cR3tP@$sW0rD')\n\nxor_cipher(cipher, 's3cR3tP@$sW0rD')\n</code></pre>"},{"location":"blogs/snippets/#one-time-pad-in-one-line-lambda-kinda","title":"One-Time Pad in one line lambda (kinda)","text":"<p>TODO: add support for mod operation <pre><code># Flat\notp = lambda message: list(zip(*list(map(lambda l: (chr(((ord(l[0])+l[1]))), l[1]), list(map(lambda x: (x, random.randint(1,10)),message))))))\n\n# Black Format\notp = lambda message: list(\n    zip(\n        *list(\n            map(\n                lambda l: (chr(((ord(l[0]) + l[1]))), l[1]),\n                list(map(lambda x: (x, random.randint(1, 10)), message)),\n            )\n        )\n    )\n)\n\n\n# returns a list of two tuples, first is characters, second is keys\nresponse = otp('Hello World!')\nkey = response[1]\n# key = (3, 5, 1, 10, 10, 7, 2, 1, 10, 10, 5, 2)\ncipher = \"\".join(response[0])\n# cipher = Kjmvy'Yp|vi#\n\n# decrypt\n\"\".join(chr(ord(letter) - key) for letter, key in zip(cipher, key))\n# 'Hello World!'\n</code></pre></p> <p></p>"},{"location":"blogs/snippets/#data-conversions","title":"Data Conversions","text":""},{"location":"blogs/snippets/#convert-lists-to-tuple-recursive","title":"Convert lists to tuple (Recursive)","text":"<p>Converting tuples or nested tuples into JSON changes tuples into lists, so loading back in would change the data types. This recursively goes through a list and converts inner lists into tuples. <pre><code># Flat\ntuple_it = (lambda x: tuple(map(lambda i: tuple_it(i) if isinstance(i, list) else i, x)) if isinstance(x, list) else x)\n\n# Black format\ntuple_it = (\n    lambda x: tuple(\n        map(lambda i: tuple_it(i) if isinstance(i, list) else i, x)\n    ) \n    if isinstance(x, list)\n    else x\n)\n\ndata = ('apple', 'banana', 'cherry', ('this', 'that'), (('inner', 'deep'), 'stuff'))\ndata = json.dumps(data)\n# '[\"apple\", \"banana\", \"cherry\", [\"this\", \"that\"], [[\"inner\", \"deep\"], \"stuff\"]]'\n\n# Later on...\ndata = json.loads(data)\n# ['apple', 'banana', 'cherry', ['this', 'that'], [['inner', 'deep'], 'stuff']]\n\ndata = tuple_it(data)\n# ('apple', 'banana', 'cherry', ('this', 'that'), (('inner', 'deep'), 'stuff'))\n</code></pre></p>"},{"location":"blogs/snippets/#combinations","title":"Combinations","text":"<pre><code>A = [1,2,3]\n\ncombination = lambda A: [[a] + b for i, a in enumerate(A) for b in (combination(A[i+1:]) or [[]])]\n\ncombination(A)\n# [[1, 2, 3], [1, 3], [2, 3], [3]]\n</code></pre>"},{"location":"blogs/snippets/#misc-math","title":"Misc Math","text":""},{"location":"blogs/snippets/#factoral-recursive","title":"Factoral (Recursive)","text":"<p>blog.finxter <pre><code>factoral = lambda n: 1 if n &lt;= 1 else n * factoral(n - 1)\nprint(factoral(10))\n# 3628800\n</code></pre></p>"},{"location":"blogs/snippets/#mean","title":"Mean","text":"<pre><code>A = [99,86,87,88,111,86,103,87,94,78,77,85,86]\n\nmean = lambda A: sum(A)/len(A)\n\nmean(A)\n# 89.76923076923077\n</code></pre>"},{"location":"blogs/snippets/#median","title":"Median","text":"<pre><code>A = [99,86,87,88,111,86,103,87,94,78,77,85,86]\n\nmedian = lambda A: sorted(A)[int(len(A)/2)]\n\nmedian(A)\n# 87\n</code></pre>"},{"location":"blogs/snippets/#mode","title":"Mode","text":"<pre><code>A = [99,86,87,88,111,86,103,87,94,78,77,85,86]\n\nmode = max(A, key=A.count)\n\nmode(A)\n# 86\n</code></pre>"},{"location":"blogs/snippets/#least-common-item-in-list","title":"Least Common item in List","text":"<pre><code>A = [99,86,87,88,111,86,103,87,94,78,77,85,86]\n\nleast_common_item = min(A, key=A.count)\n\nleast_common_item(A)\n# 99\n</code></pre>"},{"location":"blogs/snippets/#check-if-n-is-divisible-by-x-and-y","title":"check if N is divisible by X AND Y","text":"<pre><code>n_divisible = lambda n, x, y: all([divmod(n, x)[1] == 0, divmod(n, y)[1] == 0])\nn_divisible(12, 2, 6)\n# True\n</code></pre>"},{"location":"blogs/snippets/#get-number-of-odd-numbers-below-n","title":"Get number of odd numbers below N","text":"<pre><code># Faster\nodd_below = lambda n: len([x for x in range(n) if x % 2 != 0])\n# or a bit slower\nodd_below = lambda n: len(list(filter(lambda x: x % 2 != 0, range(n))))\nodd_below(15)\n# 7\n</code></pre>"},{"location":"blogs/snippets/#get-number-of-even-numbers-below-n","title":"Get number of even numbers below N","text":"<pre><code># Faster\neven_below = lambda n: len([x for x in range(n) if x % 2 == 0])\n# or a bit slower\neven_below = lambda n: len(list(filter(lambda x: x % 2 == 0, range(n))))\neven_below(15)\n# 7\n</code></pre>"},{"location":"blogs/snippets/#greatest-common-denominator","title":"Greatest Common Denominator","text":"<pre><code>a, b = 24503, 321\ngcd = lambda a, b: a if b == 0 else gcd(b, a % b)\n\ngcd(a, b)\n# 107\n</code></pre>"},{"location":"blogs/snippets/#euler-totient","title":"Euler Totient","text":"<pre><code>p, q = 7, 13\neuler_totient = lambda p, q: (p-1) * (q-1)\n\neuler_totient(p, q)\n# 72\n</code></pre>"},{"location":"blogs/snippets/#remove-outliers","title":"Remove Outliers","text":"<p>Filter list down by those within N standard deviations of the mean <pre><code>A = [10, 8, 10, 8, 2, 7, 9, 3, 34, 9, 5, 9, 25]\n# Mean is 10.692307692307692\n\nremove_outliers = lambda A, n: list(filter(lambda x: (sum(A)/len(A) - n * (sum(map(lambda x: (x-sum(A)/len(A))**2, A))/len(A))**0.5) &lt;= x &lt;= (sum(A)/len(A) + n * (sum(map(lambda x: (x-sum(A)/len(A))**2, A))/len(A))**0.5), A))\n\nremove_outliers(A, 2)\n# [10, 8, 10, 8, 2, 7, 9, 3, 9, 5, 9, 25]\nremove_outliers(A, 1)\n# [10, 8, 10, 8, 7, 9, 3, 9, 5, 9]\nremove_outliers(A, 0.25)\n# [10, 10, 9, 9, 9]\n</code></pre></p>"},{"location":"blogs/snippets/#least-squares-regression","title":"Least Squares Regression","text":"<p>Calculate the least squares regression line given two lists</p> <pre><code>A = [2, 3, 5, 7, 9]\nB = [4, 5, 7, 10, 15]\n\n# Flat\nlsr = lambda A, B: [(sum_ab * len(A) - sum_a * sum_b) / (sum_aa * len(A) - sum_a**2) * i + (sum_b - (sum_ab * len(A) - sum_a * sum_b) / (sum_aa * len(A) - sum_a**2) * sum_a) / len(A) for i, sum_a, sum_b, sum_aa, sum_ab in [(ai, sum(A), sum(B), sum([ai**2 for ai in A]), sum([ai * bi for ai, bi in zip(A, B)]),) for ai in A]]\n\n\n# Black format\nlsr = lambda A, B: [\n    (sum_ab * len(A) - sum_a * sum_b) / (sum_aa * len(A) - sum_a**2) * i\n    + (\n        sum_b\n        - (sum_ab * len(A) - sum_a * sum_b) / (sum_aa * len(A) - sum_a**2) * sum_a\n    )\n    / len(A)\n    for i, sum_a, sum_b, sum_aa, sum_ab in [\n        (\n            ai,\n            sum(A),\n            sum(B),\n            sum([ai**2 for ai in A]),\n            sum([ai * bi for ai, bi in zip(A, B)]),\n        )\n        for ai in A\n    ]\n]\n\n\nlsr(A, B)\n# [3.341463414634146, 4.859756097560975, 7.896341463414634, 10.932926829268293, 13.96951219512195]\n</code></pre>"},{"location":"blogs/snippets/#fisher-jenks-natural-breaks-work-in-progress","title":"Fisher Jenks Natural Breaks (Work in Progress)","text":"<pre><code>A = [1500, 2100, 50, 20, 75, 1100, 950, 1300, 1400]\n\n# Flat\njenks = lambda A: min(map(lambda x: {'split': x, 'value': sum([(sum([(ia - sum(i)/len(i))**2 for ia in i])) for i in x])}, [[sorted(A)[i:i+N] for i in range(0, len(A), N)] for N in range(2, len(A)+1)]), key = lambda k: k.get('value', None)).get('split')\n\n# Black\njenks = lambda A: min(\n    map(\n        lambda x: {\n            \"split\": x,\n            \"value\": sum([(sum([(ia - sum(i) / len(i)) ** 2 for ia in i])) for i in x]),\n        },\n        [\n            [sorted(A)[i : i + N] for i in range(0, len(A), N)]\n            for N in range(2, len(A) + 1)\n        ],\n    ),\n    key=lambda k: k.get(\"value\", None),\n).get(\"split\")\n\njenks(A)\n# [[20, 50, 75], [950, 1100, 1300], [1400, 1500, 2100]]\njenks(A )\n</code></pre>"},{"location":"blogs/snippets/#distance-between-points-pythagorean-2d-and-3d-space","title":"Distance Between Points (Pythagorean 2D and 3D space)","text":"<pre><code>x = (9, 7)\ny = (3, 2)\n\npythagorean = lambda x,y: sum(map(lambda i: (i[0] - i[1])**2, list(zip(x,y))))**0.5\n\npythagorean(x,y)\n# 7.810249675906654\n\nx = [8, 2, 6]\ny = [3, 5, 7]\n\npythagorean(x, y)\n# 5.916079783099616\n</code></pre>"},{"location":"blogs/snippets/#perceptron-wip","title":"Perceptron (WIP)","text":"<pre><code>A = []\nB = []\n\nlsr = lambda A, B: [\n    (sum_ab * len(A) - sum_a * sum_b) / (sum_aa * len(A) - sum_a**2) * i\n    + (\n        sum_b\n        - (sum_ab * len(A) - sum_a * sum_b) / (sum_aa * len(A) - sum_a**2) * sum_a\n    )\n    / len(A)\n    for i, sum_a, sum_b, sum_aa, sum_ab in [\n        (\n            ai,\n            sum(A),\n            sum(B),\n            sum([ai**2 for ai in A]),\n            sum([ai * bi for ai, bi in zip(A, B)]),\n        )\n        for ai in A\n    ]\n]\n</code></pre>"},{"location":"blogs/snippets/#probability","title":"Probability","text":"<pre><code>A = [9, 2, 5, 4, 12, 7, 8, 11, 9, 3, 7, 4, 12, 5, 4, 10, 9, 6, 9, 4]\n\nprobability = lambda n, A: A.count(n)/len(A)\n\nprobability(4, A)\n# 0.2\n</code></pre>"},{"location":"blogs/snippets/#cli-commands","title":"CLI commands","text":""},{"location":"blogs/snippets/#host-a-web-server-at-current-directory","title":"host a web server at current directory","text":"<pre><code>python -m http.server\n</code></pre>"},{"location":"blogs/snippets/#display-documentation-for-a-python-module","title":"display documentation for a python module","text":"<pre><code>python -m pydoc module\n</code></pre>"},{"location":"blogs/snippets/#precompile-python-code-to-bytecode","title":"precompile python code to bytecode","text":"<pre><code>python -m py_compile minimath.py\n</code></pre>"},{"location":"crypto/encryption/","title":"Encryption","text":"<p>Encryption is a way of scrambling data so that only authorized parties can understand the information. In technical terms, it is the process of converting human-readable plaintext to incomprehensible text, also known as ciphertext. In simpler terms, encryption takes readable data and alters it so that it appears random. Encryption requires the use of a cryptographic key: a set of mathematical values that both the sender and the recipient of an encrypted message agree on.</p> <p>Definition</p>"},{"location":"crypto/encryption/#symmetric-encryption","title":"Symmetric Encryption","text":"<p>In symmetric-key encryption, the data is encoded and decoded with the same key. This is the easiest way of encryption, but also less secure. The receiver needs the key for decryption, so a safe way need for transferring keys. Anyone with the key can read the data in the middle.</p>"},{"location":"crypto/encryption/#fernet","title":"Fernet","text":"<p>Steps</p> <ol> <li>Import Fernet</li> <li>Then generate an encryption key, that can be used for encryption and decryption.</li> <li>Convert the string to byte string, so that it can be encrypted.</li> <li>Instance the Fernet class with the encryption key.</li> <li> <p>Then encrypt the string with Fernet instance.</p> </li> <li> <p>Then it can be decrypted with Fernet class instance and it should be instanced with the same key used for encryption.</p> </li> </ol> <p><pre><code>from cryptography.fernet import Fernet\n\nmessage = \"Encrypt this message\"\n\nkey = Fernet.generate_key()\nfernet = Fernet(key)\n# string must must be encoded to byte string before encryption\nencMessage = fernet.encrypt(message.encode())\n\n# Decrypting\ndecMessage = fernet.decrypt(encMessage).decode()\n</code></pre> More Info</p>"},{"location":"crypto/encryption/#asymmetric-encryption","title":"Asymmetric Encryption","text":"<p>In Asymmetric-key Encryption, we use two keys a public key and private key. The public key is used to encrypt the data and the private key is used to decrypt the data. By the name, the public key can be public (can be sent to anyone who needs to send data). No one has your private key, so no one the middle can read your data.</p>"},{"location":"crypto/encryption/#rsa","title":"RSA","text":"<p>Steps:</p> <ol> <li>Import rsa library</li> <li>Generate public and private keys with rsa.newkeys() method.</li> <li>Encode the string to byte string.</li> <li>Then encrypt the byte string with the public key.</li> <li>Then the encrypted string can be decrypted with the private key.</li> <li>The public key can only be used for encryption and the private can only be used for decryption.</li> </ol> <p><pre><code>import rsa\n\npublicKey, privateKey = rsa.newkeys(512)\nmessage = \"Encrypt this message\"\n# string must must be encoded to byte string before encryption\nencMessage = rsa.encrypt(message.encode(), publicKey)\n# Decrypting\ndecMessage = rsa.decrypt(encMessage, privateKey).decode()\n</code></pre> More Info</p>"},{"location":"crypto/encryption/#homomorphic-encryption","title":"Homomorphic Encryption","text":""},{"location":"crypto/encryption/#python-paillier","title":"Python-Paillier","text":"<p>Documentation</p> <p>A Python 3 library implementing the Paillier Partially Homomorphic Encryption.</p> <p>The homomorphic properties of the paillier crypto system are:</p> <ul> <li>Encrypted numbers can be multiplied by a non encrypted scalar.</li> <li>Encrypted numbers can be added together.</li> <li>Encrypted numbers can be added to non encrypted scalars.</li> </ul>"},{"location":"crypto/encryption/#private-information-retrieval","title":"Private Information Retrieval","text":""},{"location":"crypto/encryption/#using-homomorphic-encryption","title":"using homomorphic encryption","text":"<p>Documentation</p> <ol> <li>The client encodes its desired index i=3i=3 in a one-hot encoding. That is, it builds a vector of n=4n=4 bits, where the ii-th bit is 11 and the rest are 00.</li> <li>The client generates a homomorphic encryption secret key, and uses it to encrypt each bit, producing nn ciphertexts, where the ii-th ciphertext is an encrypted 11, and the rest are encrypted 00's.</li> <li>The client sends this vector of encrypted bits to the server. To the server, this vector of encrypted bits is completely random noise; it cannot learn anything about the encrypted bits.</li> <li>The server takes the nn ciphertexts (each encrypting a bit), and homomorphically multiplies them by the corresponding plaintext database item. This produces a total of nn ciphertexts, each encrypting either 0 or the desired database item.</li> <li>The server homomorphically adds all of these ciphertexts, resulting in a single ciphertext encrypting the desired database item.</li> <li>The server sends this single ciphertext to the client.</li> <li>The client decrypts this ciphertext and obtains its desired plaintext item.</li> </ol> <p></p> <pre><code>n = 512\nq = 3329\nnoise_distribution = [-3, 3]\n\nnum_items_in_db = 50\ndesired_idx = 24\ndb = [random_bit() for i in range(num_items_in_db)]\n\ndef generate_query(desired_idx):\n    v = []\n    for i in range(num_items_in_db):\n        bit = 1 if i == desired_idx else 0\n        ct = encrypt(s, bit)\n        v.append(ct)\n    return v\n\ndef answer_query(query, db):\n    summed_A = zero_matrix(n, n)\n    summed_c = zero_vector(n)\n    for i in range(num_items_in_db):\n        if db[i] == 1:\n            (A, c) = query[i]\n            summed_A += A\n            summed_c += c\n    return (summed_A, summed_c)\n\ns = keygen()\nquery = generate_query(desired_idx)\n\nprint(\"Sending the query to the server...\")\n\nanswer = answer_query(query, db)\n\nprint(\"Got the answer back from the server...\")\n\nresult = decrypt(s, answer)\n\nprint(\"The item at index %d of the database is %d\", desired_idx, result)\n\nassert result == db[desired_idx]\nprint(\"PIR was correct!\")\n</code></pre>"},{"location":"crypto/encryption/#elliptic-curve-cryptography","title":"Elliptic Curve Cryptography","text":""},{"location":"crypto/encryption/#_1","title":"Encryption","text":"<p>Documentation</p>"},{"location":"crypto/hashing/","title":"Hashing","text":"<p>Cryptographic hashes are used in day-day life like in digital signatures, message authentication codes, manipulation detection, fingerprints, checksums (message integrity check), hash tables, password storage and much more. They are also used in sending messages over network for security or storing messages in databases.</p> <p>Definition</p> <p>Many encryption and hashing algorithms use the follow functions:</p> <ul> <li><code>encode()</code> : Converts the string into bytes to be acceptable by hash function.</li> <li><code>digest()</code> : Returns the encoded data in byte format.</li> <li><code>hexdigest()</code> : Returns the encoded data in hexadecimal format.</li> </ul>"},{"location":"crypto/hashing/#hashing-algorithms","title":"Hashing Algorithms","text":""},{"location":"crypto/hashing/#python-hash","title":"Python hash()","text":"<p><pre><code>int_val = 4\nstr_val = 'GeeksforGeeks'\nflt_val = 24.56\n\n# Printing the hash values.\n# Notice Integer value doesn't change\n# You'l have answer later in article.\nprint(\"The integer hash value is : \" + str(hash(int_val)))\nprint(\"The string hash value is : \" + str(hash(str_val)))\nprint(\"The float hash value is : \" + str(hash(flt_val)))\n</code></pre> More Info</p>"},{"location":"crypto/hashing/#md5-hash","title":"MD5 Hash","text":"<pre><code>import hashlib\n# note the string is in byte format\nresult = hashlib.md5(b'information to hash')\n\n# alternatively...\ninfo_to_hash = \"information to hash\"\nresult = hashlib.md5(info_to_hash.encode())\n\n# printing the equivalent byte value.\nprint(result.digest())\n# b'\\xf1\\xe0ix~\\xcetS\\x1d\\x11%Y\\x94\\\\hq'\n\n# printing the equivalent hexadecimal value.\nprint(result.hexdigest())\n# f1e069787ece74531d112559945c6871\n</code></pre>"},{"location":"crypto/hashing/#sha1-hash","title":"SHA1 Hash","text":""},{"location":"crypto/hashing/#sha256-hash","title":"SHA256 Hash","text":"<pre><code>import hashlib\n\n# initializing string\nstr = \"GeeksforGeeks\"\n\n# then sending to SHA1()\nresult = hashlib.sha1(str.encode())\n# printing the equivalent hexadecimal value.\nprint(result.hexdigest())\n\n# then sending to SHA256()\nresult = hashlib.sha256(str.encode())\nprint(result.hexdigest())\n\n# then sending to SHA512()\nresult = hashlib.sha512(str.encode())\nprint(result.hexdigest())\n\n# Of the same inputs, each algorithm produces these outputs\n# SHA1: 4175a37afd561152fb60c305d4fa6026b7e79856\n# SHA256: f6071725e7ddeb434fb6b32b8ec4a2b14dd7db0d785347b2fb48f9975126178f\n# SHA512: 0d8fb9370a5bf7b892be4865cdf8b658a82209624e33ed71cae353b0df254a75db63d1baa35ad99f26f1b399c31f3c666a7fc67ecef3bdcdb7d60e8ada90b722\n</code></pre> <p>More Info</p>"},{"location":"crypto/steganography/","title":"Steganography","text":"<p>Definition</p>"},{"location":"crypto/steganography/#bi-tools","title":"BI Tools","text":""},{"location":"crypto/steganography/#rawgraphs","title":"RawGraphs","text":"<p>RAW Graphs is an open source data visualization framework built with the goal of making the visual representation of complex data easy for everyone.</p> <p>Primarily conceived as a tool for designers and vis geeks, RAW Graphs aims at providing a missing link between spreadsheet applications (e.g. Microsoft Excel, Apple Numbers, OpenRefine) and vector graphics editors (e.g. Adobe Illustrator, Inkscape, Sketch).</p>"},{"location":"data/binning/","title":"Binning Data","text":"<p>When dealing with continuous numeric data, it is often helpful to bin the data into multiple buckets for further analysis. There are several different terms for binning including bucketing, discrete binning, discretization or quantization.</p> <p>One of the most common instances of binning is done behind the scenes for you when creating a histogram. The histogram below of customer sales data, shows how a continuous set of sales numbers can be divided into discrete bins (for example: $60,000 - $70,000) and then used to group and count account instances.</p>"},{"location":"data/binning/#binning-data_1","title":"Binning Data","text":""},{"location":"data/binning/#pandas-cut","title":"Pandas cut","text":"<p>Pandas <code>cut</code> and <code>qcut</code> info is taken from pbpython</p> <p>Documentation</p> <pre><code>cut_labels_4 = ['silver', 'gold', 'platinum', 'diamond']\ncut_bins = [0, 70000, 100000, 130000, 200000]\ndf['cut_ex1'] = pd.cut(df['ext price'], bins=cut_bins, labels=cut_labels_4)\n</code></pre> account number name ext price cut_ex1 141962 Herman LLC 63626.03 silver 146832 Kiehn-Spinka 99608.77 gold 163416 Purdy-Kunde 77898.21 gold 218895 Kulas Inc 137351.96 diamond 239344 Stokes LLC 91535.92 gold"},{"location":"data/binning/#pandas-qcut","title":"Pandas qcut","text":"<p>Documentation</p> <p>The pandas documentation describes <code>qcut</code> as a \u201cQuantile-based discretization function.\u201d This basically means that <code>qcut</code> tries to divide up the underlying data into equal sized bins. The function defines the bins using percentiles based on the distribution of the data, not the actual numeric edges of the bins.</p> <p>The simplest use of qcut is to define the number of quantiles and let pandas figure out how to divide up the data. In the example below, we tell pandas to create 4 equal sized groupings of the data. <pre><code>df['quantile_ex_1'] = pd.qcut(df['ext price'], q=4)\ndf['quantile_ex_2'] = pd.qcut(df['ext price'], q=10, precision=0)\n\ndf.head()\n</code></pre></p> account number name ext price cut_ex1 quantile_ex_2 141962 Herman LLC 63626.03 (55733.049000000006, 89137.708] (55732.0, 76471.0] 146832 Kiehn-Spinka 99608.77 (89137.708, 100271.535] (95908.0, 100272.0] 163416 Purdy-Kunde 77898.21 (55733.049000000006, 89137.708] (76471.0, 87168.0] 218895 Kulas Inc 137351.96 (110132.552, 184793.7] (124778.0, 184794.0] 239344 Stokes LLC 91535.92 (89137.708, 100271.535] (90686.0, 95908.0]"},{"location":"data/binning/#fisher-jenks-algorithm","title":"Fisher-Jenks Algorithm","text":"<p>Documentation</p> <p>What we are trying to do is identify natural groupings of numbers that are \u201cclose\u201d together while also maximizing the distance between the other groupings. Fisher developed a clustering algorithm that does this with 1 dimensional data (essentially a single list of numbers). In many ways it is similar to k-means clustering but is ultimately a simpler and faster algorithm because it only works on 1 dimensional data. Like k-means, you do need to specify the number of clusters. Therefore domain knowledge and understanding of the data are still essential to using this effectively.</p> <p>The algorithm uses an iterative approach to find the best groupings of numbers based on how close they are together (based on variance from the group\u2019s mean) while also trying to ensure the different groupings are as distinct as possible (by maximizing the group\u2019s variance between groups).</p> <p><pre><code>import pandas as pd\nimport jenkspy\n\nsales = {\n    'account': [\n        'Jones Inc', 'Alpha Co', 'Blue Inc', 'Super Star Inc', 'Wamo',\n        'Next Gen', 'Giga Co', 'IniTech', 'Beta LLC'\n    ],\n    'Total': [1500, 2100, 50, 20, 75, 1100, 950, 1300, 1400]\n}\ndf = pd.DataFrame(sales)\ndf.sort_values(by='Total')\n</code></pre> which yields this dataframe</p> <p></p> <p>In order to illustrate how natural breaks are found, we can start by contrasting it with how quantiles are determined. For example, what happens if we try to use <code>pd.qcut</code> with 2 quantiles? Will that give us a similar result? <pre><code>df['quantile'] = pd.qcut(df['Total'], q=2, labels=['bucket_1', 'bucket_2'])\n</code></pre> </p> <p>Just to get one more example, we can see what 4 buckets would look like with natural breaks and with a quantile cut approach: <pre><code>df['quantilev2'] = pd.qcut(\n    df['Total'], q=4, labels=['bucket_1', 'bucket_2', 'bucket_3', 'bucket_4'])\n\ndf['cut_jenksv3'] = pd.cut(\n    df['Total'],\n    bins=jenkspy.jenks_breaks(df['Total'], nb_class=4),\n    labels=['bucket_1', 'bucket_2', 'bucket_3', 'bucket_4'],\n    include_lowest=True)\n\ndf.sort_values(by='Total')\n</code></pre> </p>"},{"location":"data/data_scale/","title":"Data Scaling","text":"<p>Small-Medium Data - 0mb - 100mb</p> <p>Large Data - 100mb - 1TB</p> <p>Big Data - 1TB+</p>"},{"location":"data/data_scale/#tools","title":"Tools","text":""},{"location":"data/data_scale/#pandas","title":"Pandas","text":"<p>Documentation</p> <ol> <li> <p>Read CSV file data in chunk size: The parameter essentially means the number of rows to be read into a dataframe at any single time in order to fit into the local memory. <pre><code>import pandas as pd\n# read the large csv file with specified chunksize \ndf_chunk = pd.read_csv(r'../input/data.csv', chunksize=1_000_000)\n</code></pre> The operation above resulted in a TextFileReader object for iteration. Strictly speaking, df_chunk is not a dataframe but an object for further operation in the next step. <pre><code>chunk_list = []  # append each chunk df here \n\n# Each chunk is in df format\nfor chunk in df_chunk:  \n    # perform data filtering \n    chunk_filter = chunk_preprocessing(chunk)\n\n    # Once the data filtering is done, append the chunk to list\n    chunk_list.append(chunk_filter)\n\n# concat the list into dataframe \ndf_concat = pd.concat(chunk_list)\n</code></pre></p> </li> <li> <p>Filter out unimportant columns to save memory <pre><code># Filter out unimportant columns\ndf = df[['col_1','col_2', 'col_3', 'col_4', 'col_5', 'col_6','col_7', 'col_8', 'col_9', 'col_10']]\n</code></pre></p> </li> <li>Change dtypes for columns</li> </ol> <p>The simplest way to convert a pandas column of data to a different type is to use <code>astype()</code>. <pre><code># Change the dtypes (int64 -&gt; int32)\ndf[['col_1','col_2', \n    'col_3', 'col_4', 'col_5']] = df[['col_1','col_2', \n                                      'col_3', 'col_4', 'col_5']].astype('int32')\n\n# Change the dtypes (float64 -&gt; float32)\ndf[['col_6', 'col_7',\n    'col_8', 'col_9', 'col_10']] = df[['col_6', 'col_7',\n                                       'col_8', 'col_9', 'col_10']].astype('float32')\n</code></pre></p>"},{"location":"data/data_scale/#modin","title":"Modin","text":"<p>Documentation</p> <p>Modin is a DataFrame for datasets from 1MB to 1TB+</p> <p>Modin uses Ray or Dask to provide an effortless way to speed up your pandas notebooks, scripts, and libraries. Pandas  traditionally loads data into a single CPU core, Modin will spread that dataset over multiple cores, this makes it easy to scale and also to build out multiple clusters.</p> <p>To use Modin, you do not need to know how many cores your system has and you do not need to specify how to distribute the data. In fact, you can continue using your previous pandas notebooks while experiencing a considerable speedup from Modin, even on a single machine. Once you\u2019ve changed your import statement, you\u2019re ready to use Modin just like you would pandas.</p> <p>If you don\u2019t have Ray or Dask installed, you will need to install Modin with one of the targets: <pre><code>pip install \"modin[ray]\" # Install Modin dependencies and Ray to run on Ray\npip install \"modin[dask]\" # Install Modin dependencies and Dask to run on Dask\npip install \"modin[all]\" # Install all of the above\n</code></pre> If you want to choose a specific compute engine to run on, you can set the environment variable MODIN_ENGINE and Modin will do computation with that engine: <pre><code>export MODIN_ENGINE=ray  # Modin will use Ray\nexport MODIN_ENGINE=dask  # Modin will use Dask\n</code></pre> In pandas, you are only able to use one core at a time when you are doing computation of any kind. With Modin, you are able to use all of the CPU cores on your machine. Even in <code>read_csv</code>, we see large gains by efficiently distributing the work across your entire machine. <pre><code>import modin.pandas as pd\n\ndf = pd.read_csv(\"my_dataset.csv\")\n# 31.4 Seconds with Pandas\n# 7.73 Seconds with Modin\n</code></pre></p>"},{"location":"data/data_scale/#dask","title":"Dask","text":"<p>Documentation</p> <p>Dask is composed of two parts: - Dynamic task scheduling optimized for computation. This is similar to Airflow, Luigi, Celery, or Make, but optimized for interactive computational workloads. - \u201cBig Data\u201d collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of dynamic task schedulers.</p> <pre><code>python -m pip install dask                # Install only core parts of dask\n\npython -m pip install \"dask[complete]\"    # Install everything\npython -m pip install \"dask[array]\"       # Install requirements for dask array\npython -m pip install \"dask[dataframe]\"   # Install requirements for dask dataframe\npython -m pip install \"dask[diagnostics]\" # Install requirements for dask diagnostics\npython -m pip install \"dask[distributed]\" # Install requirements for distributed dask\n</code></pre>"},{"location":"data/data_scale/#polars","title":"Pola.rs","text":"<p>Documentation</p> <p>Polars is written in Rust but has Python Pandas API</p> <p>Polars is a lightning fast DataFrame library/in-memory query engine. Its embarrassingly parallel execution, cache efficient algorithms and expressive API makes it perfect for efficient data wrangling, data pipelines, snappy APIs and so much more.</p> <pre><code>import polars as pl\n\nq = (\n    pl.scan_csv(\"iris.csv\")\n    .filter(pl.col(\"sepal_length\") &gt; 5)\n    .groupby(\"species\")\n    .agg(pl.all().sum())\n)\n\ndf = q.collect()\n</code></pre>"},{"location":"data/eda/","title":"EDA Exploratory Data Analysis","text":"<p>Exploratory Data Analysis (EDA) is used on the one hand to answer questions, test business assumptions, generate hypotheses for further analysis. On the other hand, you can also use it to prepare the data for modeling. The thing that these two probably have in common is a good knowledge of your data to either get the answers that you need or to develop an intuition for interpreting the results of future modeling.</p> <p>There are a lot of ways to reach these goals: you can get a basic description of the data, visualize it, identify patterns in it, identify challenges of using the data, etc.</p>"},{"location":"data/eda/#data-exploration-tools","title":"Data Exploration Tools","text":""},{"location":"data/eda/#pandas-profiling","title":"Pandas Profiling","text":"<p>Documentation</p> <p>Generates profile reports from a pandas <code>DataFrame</code>. The pandas <code>df.describe()</code> function is great but a little basic for serious exploratory data analysis. <code>pandas_profiling</code> extends the pandas DataFrame with <code>df.profile_report()</code> for quick data analysis.</p> <p>For each column the following statistics - if relevant for the column type - are presented in an interactive HTML report:</p> <ul> <li>Type inference: detect the types of columns in a dataframe.</li> <li>Essentials: type, unique values, missing values</li> <li>Quantile statistics: like minimum value, Q1, median, Q3, maximum, range, interquartile range</li> <li>Descriptive statistics: like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness</li> <li>Most frequent values:</li> <li>Histograms:</li> <li>Correlations: highlighting of highly correlated variables, Spearman, Pearson and Kendall matrices</li> <li>Missing values: matrix, count, heatmap and dendrogram of missing values</li> <li>Duplicate rows: Lists the most occurring duplicate rows</li> <li>Text analysis: learn about categories (Uppercase, Space), scripts (Latin, Cyrillic) and blocks (ASCII) of text data</li> </ul>"},{"location":"data/eda/#basic-usage","title":"Basic Usage","text":"<p><pre><code>import numpy as np\nimport pandas as pd\nfrom pandas_profiling import ProfileReport\n\n# build dataframe\ndf = pd.DataFrame(np.random.rand(100, 5), columns=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n\n# generate a report\nprofile = ProfileReport(df, title=\"Pandas Profiling Report\", explorative=True)\n\n# Save report\nprofile.to_file(\"your_report.html\")\n\n# As a string\njson_data = profile.to_json()\n\n# As a file\nprofile.to_file(\"your_report.json\")\n</code></pre> </p>"},{"location":"data/eda/#working-with-larger-datasets","title":"Working with Larger datasets","text":"<p>Pandas Profiling by default comprehensively summarizes the input dataset in a way that gives the most insights for data analysis. For small datasets these computations can be performed in real-time. For larger datasets, we have to decide upfront which calculations to make. Whether a computation scales to big data not only depends on it\u2019s complexity, but also on fast implementations that are available.</p> <p><code>pandas-profiling</code> includes a minimal configuration file, with more expensive computations turned off by default. This is a great starting point for larger datasets. <pre><code># minimal flag to True\n# Sample 10.000 rows\nsample = large_dataset.sample(10000)\n\nprofile = ProfileReport(sample, minimal=True)\nprofile.to_file(\"output.html\")\n</code></pre></p>"},{"location":"data/eda/#privacy-features","title":"Privacy Features","text":"<p>When dealing with sensitive data, such as private health records, sharing a report that includes a sample would violate patient\u2019s privacy. The following shorthand groups together various options so that only aggregate information is provided in the report.</p> <pre><code>report = df.profile_report(sensitive=True)\n</code></pre>"},{"location":"data/eda/#sweetviz","title":"SweetViz","text":"<p>Documentation</p> <p>Sweetviz is an open-source python auto-visualization library that generates a report, exploring the data with the help of high-density plots. It not only automates the EDA but is also used for comparing datasets and drawing inferences from it. A comparison of two datasets can be done by treating one as training and the other as testing.</p> <p>The Sweetviz library generates a report having: - An overview of the dataset - Variable properties - Categorical associations - Numerical associations - Most frequent, smallest, largest values for numerical features</p> <p>See Example Titanic Dataset</p> <p><pre><code>import sweetviz\nimport pandas as pd\ntrain = pd.read_csv(\"train.csv\")\ntest = pd.read_csv(\"test.csv\")\n\nmy_report = sweetviz.compare([train, \"Train\"], [test, \"Test\"], \"Survived\")\n</code></pre> Running this command will perform the analysis and create the report object. To get the output, simply use the <code>show_html()</code> command: <pre><code># Not providing a filename will default to SWEETVIZ_REPORT.html\nmy_report.show_html(\"Report.html\")\n</code></pre></p> <p></p> <p>More Details Here</p>"},{"location":"data/eda/#dtale","title":"Dtale","text":"<p>Documentation</p> <p>D-Tale is the combination of a Flask back-end and a React front-end to bring you an easy way to view &amp; analyze Pandas data structures. It integrates seamlessly with ipython notebooks &amp; python/ipython terminals. Currently this tool supports such Pandas objects as DataFrame, Series, MultiIndex, DatetimeIndex &amp; RangeIndex.</p> <p>To start without any data right away you can run <code>dtale.show()</code> and open the browser to input a <code>CSV</code> or <code>TSV</code> file.</p> <p></p> <pre><code>import dtale\nimport pandas as pd\n\ndf = pd.DataFrame([dict(a=1,b=2,c=3)])\n# Assigning a reference to a running D-Tale process\nd = dtale.show(df)\n</code></pre> <p></p>"},{"location":"data/eda/#bulk","title":"Bulk","text":"<p>Documentation</p> <p>Bulk is a quick developer tool to apply some bulk labels. Given a prepared dataset with 2d embeddings it can generate an interface that allows you to quickly add some bulk, albeit less precice, annotations.</p> <p></p> <p>To use bulk for text, you'll first need to prepare a csv file first.</p> <p>The example below uses embetter to generate the embeddings and umap to reduce the dimensions. But you're totally free to use what-ever text embedding tool that you like. You will need to install these tools seperately. Note that embetter uses sentence-transformers under the hood.</p> <p>You can also use Bulk to explore semantic text data.</p> <pre><code>import pandas as pd\nfrom umap import UMAP\nfrom sklearn.pipeline import make_pipeline \nfrom sklearn.linear_model import LogisticRegression\n\n\n# pip install \"embetter[text]\"\nfrom embetter.text import SentenceEncoder\n\n# Build a sentence encoder pipeline with UMAP at the end.\ntext_emb_pipeline = make_pipeline(\n  SentenceEncoder('all-MiniLM-L6-v2'),\n  UMAP()\n)\n\n# Load sentences\nsentences = list(pd.read_csv(\"original.csv\")['sentences'])\n\n# Calculate embeddings \nX_tfm = text_emb_pipeline.fit_transform(sentences)\n\n# Write to disk. Note! Text column must be named \"text\"\ndf = pd.DataFrame({\"text\": sentences})\ndf['x'] = X_tfm[:, 0]\ndf['y'] = X_tfm[:, 1]\ndf.to_csv(\"ready.csv\")\n</code></pre> <p><code>python -m bulk text ready.csv</code></p> <p><code>python -m bulk text ready.csv --keywords \"foo,bar,other\"</code></p>"},{"location":"data/eda/#data-exploration-manual","title":"Data Exploration Manual","text":""},{"location":"data/eda/#python-pandas","title":"Python Pandas","text":"<pre><code>import pandas as pd\ndf = pd.read_csv(\"path/to/file.csv\")\ndf.describe()\n</code></pre> <p>More to come...</p> <p>More to come...</p>"},{"location":"data/fileformats/","title":"File Formats","text":"<p>Data has to be stored somehow, here are some useful resources for popular data formats for the web as well as storage.</p>"},{"location":"data/fileformats/#csv","title":"CSV","text":"<p>Documentation</p> <p>Files with .csv (Comma Separated Values) extension represent plain text files that contain records of data with comma separated values. Each line in a CSV file is a new record from the set of records contained in the file.</p> <p>Pretty straightforward and very common. CSV doesn't retain data types so storage space isn't optimal. </p> <p>Each record is located on a separate line, delimited by a line break (CRLF).  For example: <pre><code>aaa,bbb,ccc CRLF\nzzz,yyy,xxx CRLF\n</code></pre></p>"},{"location":"data/fileformats/#log","title":"LOG","text":"<p>Documentation</p> <p>A file with <code>.log</code> extension contains the list of plain text with timestamp. Usually, certain activity detail is logged by the softwares or operating systems to help the developers or users to track what was happening at a certain time period. Users can edit these files very easily by using any text editors. Usually the error reports or login activities are logged by the operating systems, but other softwares or web servers also generate log files to track visitors and to monitor bandwidth usage.</p>"},{"location":"data/fileformats/#sqlite","title":"SQLite","text":"<p>Documentation</p> <p>A file with <code>.sqlite</code> extension is a lightweight SQL database file created with the SQLite software. It is a database in a file itself and implements a self-contained, full-featured, highly-reliable SQL database engine. SQLite database files can be used to share rich contents between systems by simple exchanging these files over the network. Almost all mobiles and computers use SQLite for storing and sharing of data, and is the choice of file format for cross-platform applications. Due to its compact use and easy usability, it comes bundled inside other applications. SQLite bindings exist for programming languages such as C, C#, C++, Java, PHP, and many others.</p> <p>SQLite in reality is a C-Language library that implements the SQLite RDBMS using the SQLite file format. With the evolution of new devices every single day, its file format has been kept backwards compatible to accommodate older devices. SQLite file format is seen as long-term archival format for the data.</p>"},{"location":"data/fileformats/#sql","title":"SQL","text":"<p>Documentation</p> <p>A file with <code>.sql</code> extension is a Structured Query Language (SQL) file that contains code to work with relational databases. It is used to write SQL statements for CRUD (Create, Read, Update, and Delete) operations on databases. SQL files are common while working with desktop as well as web-based databases. There are several alternatives to SQL such as Java Persistence Query Language (JPQL), LINQ, HTSQL, 4D QL, and several others. SQL files can be opened by query editors of Microsoft SQL Server, MySQL and other plain text editors such as Notepad on Windows OS.</p> <p>SQL files are in plain text format and can comprise of several language elements. Multiple statements can be added to a single SQL file if their execution is possible without depending on each other. These SQL commands can be executed by query editors for carrying out CRUD operations.</p>"},{"location":"data/fileformats/#feather","title":"Feather","text":"<p>Documentation</p> <p>Feather is a portable file format for storing Arrow tables or data frames (from languages like Python or R) that utilizes the Arrow IPC format internally. Feather was created early in the Arrow project as a proof of concept for fast, language-agnostic data frame storage for Python (pandas) and R.</p> <p>Additional Info</p>"},{"location":"data/fileformats/#parquet","title":"Parquet","text":"<p>Documentation</p> <p>Apache Parquet has the following characteristics:</p> <ul> <li>Self-describing</li> <li>Columnar format</li> <li>Language-independent </li> </ul> <p>Self-describing data embeds the schema or structure with the data itself. Hadoop use cases drive the growth of self-describing data formats, such as Parquet and JSON, and of NoSQL databases, such as HBase. These formats and databases are well suited for the agile and iterative development cycle of Hadoop applications and BI/analytics. </p> <p>Optimized for working with large files, Parquet arranges data in columns, putting related values in close proximity to each other to optimize query performance, minimize I/O, and facilitate compression. Parquet detects and encodes the same or similar data using a technique that conserves resources.</p> <p>Additional Info</p>"},{"location":"data/fileformats/#avro","title":"Avro","text":"<p>Documentation</p> <p>Apache Avro is a language-neutral data serialization system. Avro has a schema-based system. A language-independent schema is associated with its read and write operations. Avro serializes the data which has a built-in schema. Avro serializes the data into a compact binary format, which can be deserialized by any application.</p> <p>Avro uses JSON format to declare the data structures. Presently, it supports languages such as Java, C, C++, C#, Python, and Ruby.</p>"},{"location":"data/fileformats/#jsonl","title":"JSONL","text":"<p>Documentation</p> <p>JSON Lines is a convenient format for storing structured data that may be processed one record at a time. It works well with unix-style text processing tools and shell pipelines. It's a great format for log files. It's also a flexible format for passing messages between cooperating processes.</p> <p>The JSON Lines format has three requirements: - UTF-8 Encoding   - JSON allows encoding Unicode strings with only ASCII escape sequences, however those escapes will be hard to read when viewed in a text editor. - Each Line is a Valid JSON Value   - The most common values will be objects or arrays, but any JSON value is permitted. - Line Separator is <code>\\n</code>   - This means '\\r\\n' is also supported because surrounding white space is implicitly ignored when parsing JSON values. The last character in the file may be a line separator, and it will be treated the same as if there was no line separator present.</p> <p>Data as JSONL: <pre><code>{\"name\": \"Gilbert\", \"wins\": [[\"straight\", \"7\u2663\"], [\"one pair\", \"10\u2665\"]]}\n{\"name\": \"Alexa\", \"wins\": [[\"two pair\", \"4\u2660\"], [\"two pair\", \"9\u2660\"]]}\n{\"name\": \"May\", \"wins\": []}\n{\"name\": \"Deloise\", \"wins\": [[\"three of a kind\", \"5\u2663\"]]} </code></pre></p>"},{"location":"data/sorting/","title":"Sorting Algorithms","text":"<p>Sorting is a basic building block that many other algorithms are built upon. It\u2019s related to several exciting ideas that you\u2019ll see throughout your programming career. Understanding how sorting algorithms in Python work behind the scenes is a fundamental step toward implementing correct and efficient algorithms that solve real-world problems.</p>"},{"location":"data/sorting/#pythons-built-in-sorting-algorithms","title":"Python\u2019s Built-In Sorting Algorithms","text":""},{"location":"data/sorting/#sorted","title":"sorted()","text":"<p>Documentation</p>"},{"location":"data/sorting/#basic-usage","title":"Basic Usage","text":"<pre><code>numbers = [6, 9, 3, 1]\nnumbers_tuple = (6, 9, 3, 1)\nnumbers_set = {5, 5, 10, 1, 0}\nstring_number_value = '34521'\nstring_value = 'I like to sort'\n\nnumbers_sorted = sorted(numbers)\n# numbers_sorted\n# [1, 3, 6, 9]\n\nnumbers_tuple_sorted = sorted(numbers_tuple)\n# numbers_tuple_sorted\n# [1, 3, 6, 9]\n\nnumbers_set_sorted = sorted(numbers_set)\n# numbers_set_sorted\n# [0, 1, 5, 10]\n\nsorted_string_number = sorted(string_number_value)\n# sorted_string_number\n# ['1', '2', '3', '4', '5']\n\nsorted_string = sorted(string_value)\n# sorted_string\n# [' ', ' ', ' ', 'I', 'e', 'i', 'k', 'l', 'o', 'o', 'r', 's', 't', 't']\n</code></pre>"},{"location":"data/sorting/#bubble-sort","title":"Bubble Sort","text":"<p>Since this implementation sorts the array in ascending order, each step \u201cbubbles\u201d the largest element to the end of the array. This means that each iteration takes fewer steps than the previous iteration because a continuously larger portion of the array is sorted. <pre><code>def bubble_sort(array):\n    n = len(array)\n\n    for i in range(n):\n        # Create a flag that will allow the function to\n        # terminate early if there's nothing left to sort\n        already_sorted = True\n\n        # Start looking at each item of the list one by one,\n        # comparing it with its adjacent value. With each\n        # iteration, the portion of the array that you look at\n        # shrinks because the remaining items have already been\n        # sorted.\n        for j in range(n - i - 1):\n            if array[j] &gt; array[j + 1]:\n                # If the item you're looking at is greater than its\n                # adjacent value, then swap them\n                array[j], array[j + 1] = array[j + 1], array[j]\n\n                # Since you had to swap two elements,\n                # set the `already_sorted` flag to `False` so the\n                # algorithm doesn't finish prematurely\n                already_sorted = False\n\n        # If there were no swaps during the last iteration,\n        # the array is already sorted, and you can terminate\n        if already_sorted:\n            break\n\n    return array\n</code></pre></p>"},{"location":"data/sorting/#insertion-sort","title":"Insertion Sort","text":"<p>Like bubble sort, the insertion sort algorithm is straightforward to implement and understand. But unlike bubble sort, it builds the sorted list one element at a time by comparing each item with the rest of the list and inserting it into its correct position. This \u201cinsertion\u201d procedure gives the algorithm its name.</p> <p>An excellent analogy to explain insertion sort is the way you would sort a deck of cards. Imagine that you\u2019re holding a group of cards in your hands, and you want to arrange them in order. You\u2019d start by comparing a single card step by step with the rest of the cards until you find its correct position. At that point, you\u2019d insert the card in the correct location and start over with a new card, repeating until all the cards in your hand were sorted.</p> <pre><code>def insertion_sort(array):\n    # Loop from the second element of the array until\n    # the last element\n    for i in range(1, len(array)):\n        # This is the element we want to position in its\n        # correct place\n        key_item = array[i]\n\n        # Initialize the variable that will be used to\n        # find the correct position of the element referenced\n        # by `key_item`\n        j = i - 1\n\n        # Run through the list of items (the left\n        # portion of the array) and find the correct position\n        # of the element referenced by `key_item`. Do this only\n        # if `key_item` is smaller than its adjacent values.\n        while j &gt;= 0 and array[j] &gt; key_item:\n            # Shift the value one position to the left\n            # and reposition j to point to the next element\n            # (from right to left)\n            array[j + 1] = array[j]\n            j -= 1\n\n        # When you finish shifting the elements, you can position\n        # `key_item` in its correct location\n        array[j + 1] = key_item\n\n    return array\n</code></pre>"},{"location":"data/sorting/#merge-sort","title":"Merge Sort","text":"<p>Merge sort is a very efficient sorting algorithm. It\u2019s based on the divide-and-conquer approach, a powerful algorithmic technique used to solve complex problems.</p> <p>To properly understand divide and conquer, you should first understand the concept of recursion. Recursion involves breaking a problem down into smaller subproblems until they\u2019re small enough to manage. In programming, recursion is usually expressed by a function calling itself.</p> <pre><code>def merge(left, right):\n    # If the first array is empty, then nothing needs\n    # to be merged, and you can return the second array as the result\n    if len(left) == 0:\n        return right\n\n    # If the second array is empty, then nothing needs\n    # to be merged, and you can return the first array as the result\n    if len(right) == 0:\n        return left\n\n    result = []\n    index_left = index_right = 0\n\n    # Now go through both arrays until all the elements\n    # make it into the resultant array\n    while len(result) &lt; len(left) + len(right):\n        # The elements need to be sorted to add them to the\n        # resultant array, so you need to decide whether to get\n        # the next element from the first or the second array\n        if left[index_left] &lt;= right[index_right]:\n            result.append(left[index_left])\n            index_left += 1\n        else:\n            result.append(right[index_right])\n            index_right += 1\n\n        # If you reach the end of either array, then you can\n        # add the remaining elements from the other array to\n        # the result and break the loop\n        if index_right == len(right):\n            result += left[index_left:]\n            break\n\n        if index_left == len(left):\n            result += right[index_right:]\n            break\n\n    return result\n</code></pre>"},{"location":"data/sorting/#quicksort","title":"QuickSort","text":"<p>Just like merge sort, the Quicksort algorithm applies the divide-and-conquer principle to divide the input array into two lists, the first with small items and the second with large items. The algorithm then sorts both lists recursively until the resultant list is completely sorted.</p> <p>Dividing the input list is referred to as partitioning the list. Quicksort first selects a pivot element and partitions the list around the pivot, putting every smaller element into a low array and every larger element into a high array.</p> <p>Putting every element from the low list to the left of the pivot and every element from the high list to the right positions the pivot precisely where it needs to be in the final sorted list. This means that the function can now recursively apply the same procedure to low and then high until the entire list is sorted.</p> <pre><code>from random import randint\n\ndef quicksort(array):\n    # If the input array contains fewer than two elements,\n    # then return it as the result of the function\n    if len(array) &lt; 2:\n        return array\n\n    low, same, high = [], [], []\n\n    # Select your `pivot` element randomly\n    pivot = array[randint(0, len(array) - 1)]\n\n    for item in array:\n        # Elements that are smaller than the `pivot` go to\n        # the `low` list. Elements that are larger than\n        # `pivot` go to the `high` list. Elements that are\n        # equal to `pivot` go to the `same` list.\n        if item &lt; pivot:\n            low.append(item)\n        elif item == pivot:\n            same.append(item)\n        elif item &gt; pivot:\n            high.append(item)\n\n    # The final result combines the sorted `low` list\n    # with the `same` list and the sorted `high` list\n    return quicksort(low) + same + quicksort(high)\n</code></pre>"},{"location":"data/sorting/#timsort","title":"Timsort","text":"<p>The Timsort algorithm is considered a hybrid sorting algorithm because it employs a best-of-both-worlds combination of insertion sort and merge sort. Timsort is near and dear to the Python community because it was created by Tim Peters in 2002 to be used as the standard sorting algorithm of the Python language.</p> <p>The main characteristic of Timsort is that it takes advantage of already-sorted elements that exist in most real-world datasets. These are called natural runs. The algorithm then iterates over the list, collecting the elements into runs and merging them into a single sorted list.</p> <pre><code>def insertion_sort(array, left=0, right=None):\n    if right is None:\n        right = len(array) - 1\n\n    # Loop from the element indicated by\n    # `left` until the element indicated by `right`\n    for i in range(left + 1, right + 1):\n        # This is the element we want to position in its\n        # correct place\n        key_item = array[i]\n\n        # Initialize the variable that will be used to\n        # find the correct position of the element referenced\n        # by `key_item`\n        j = i - 1\n\n        # Run through the list of items (the left\n        # portion of the array) and find the correct position\n        # of the element referenced by `key_item`. Do this only\n        # if the `key_item` is smaller than its adjacent values.\n        while j &gt;= left and array[j] &gt; key_item:\n            # Shift the value one position to the left\n            # and reposition `j` to point to the next element\n            # (from right to left)\n            array[j + 1] = array[j]\n            j -= 1\n\n        # When you finish shifting the elements, position\n        # the `key_item` in its correct location\n        array[j + 1] = key_item\n\n    return array\n</code></pre>"},{"location":"datastorage/datastorage/","title":"Data Storage Index","text":"<p>Empty for now...</p>"},{"location":"etl/extract/","title":"ETL (Extract)","text":"<p>Extract Transform Load is the process whereby some data is obtained, (extracted) cleaned, wrangled (transformed), and placed into a user-friendly data structure like a data frame (loaded).</p> <p>Extraction involves using some tool to pull data from a source, most commonly with API and webpages will involve using the Requests package</p>"},{"location":"etl/extract/#webpages-html","title":"Webpages (HTML)","text":""},{"location":"etl/extract/#beautifulsoup","title":"BeautifulSoup","text":"<p>Documentation</p> <p>Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work. <pre><code>from bs4 import BeautifulSoup as bs\n\nhtml_doc = \"\"\"\n&lt;html&gt;&lt;head&gt;&lt;title&gt;BeautifulSoup Demo&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;p class=\"title\"&gt;&lt;b&gt;BS4 Demo title&lt;/b&gt;&lt;/p&gt;\n&lt;p class=\"story\"&gt;...&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\"\"\"\nsoup = bs(html_doc, 'html.parser')\n</code></pre> When pulling from <code>Requests</code> you can use: <pre><code>from bs4 import BeautifulSoup as bs\nimport requests\n\nresponse = requests.get(\"http://some-url.com\")\nsoup = bs(response.content, 'html.parser')\n</code></pre></p>"},{"location":"etl/extract/#mechanicalsoup","title":"MechanicalSoup","text":"<p>Documentation</p> <p>A Python library for automating interaction with websites. MechanicalSoup automatically stores and sends cookies, follows redirects, and can follow links and submit forms. It doesn\u2019t do Javascript.</p>"},{"location":"etl/extract/#example-usage-of-mechanicalsoup-to-get-the-results-from-duckduckgo-import-mechanicalsoup-connect-to-duckduckgo-browser-mechanicalsoupstatefulbrowseruser_agentmechanicalsoup-need-to-use-the-non-js-version-of-ddg-since-python-cant-render-js-browseropenhttpshtmlduckduckgocomhtml-methods-available-to-browser-browserabsolute_url-browserget_cookiejar-browserlinks-browserput-browserset_user_agent-browseradd_soup-browserget_current_form-browserlist_links-browserraise_on_404-browserset_verbose-browserclose-browserget_current_page-browsernew_control-browserrefresh-browsersoup_config-browserdownload_link-browserget_debug-browseropen-browserrequest-browsersubmit-browserfind_link-browserget_request_kwargs-browseropen_fake_page-browserselect_form-browsersubmit_selected-browserfollow_link-browserget_url-browseropen_relative-browsersession-browserurl-browserform-browserget_verbose-browserpage-browserset_cookiejar-browserget-browserlaunch_browser-browserpost-browserset_debug-fill-in-the-search-form-browserselect_formsearch_form_homepage-this-will-open-a-browser-to-show-the-html-currently-selected-in-the-browser-object-browserlaunch_browser-browserq-mechanicalsoup-browsersubmit_selected-display-the-results-for-link-in-browserpageselectaresult__a-printlinktext-linkattrshref","title":"<pre><code>\"\"\"\nExample usage of MechanicalSoup to get the results from DuckDuckGo.\n\"\"\"\nimport mechanicalsoup\n\n\n# Connect to duckduckgo\nbrowser = mechanicalsoup.StatefulBrowser(user_agent=\"MechanicalSoup\")\n# Need to use the non JS version of DDG since Python can't render JS\nbrowser.open(\"https://html.duckduckgo.com/html/\")\n\n# methods available to browser\n# browser.absolute_url(        browser.get_cookiejar(       browser.links(               browser.put(                 browser.set_user_agent(\n# browser.add_soup(            browser.get_current_form(    browser.list_links(          browser.raise_on_404         browser.set_verbose(\n# browser.close(               browser.get_current_page(    browser.new_control(         browser.refresh(             browser.soup_config\n# browser.download_link(       browser.get_debug(           browser.open(                browser.request(             browser.submit(\n# browser.find_link(           browser.get_request_kwargs(  browser.open_fake_page(      browser.select_form(         browser.submit_selected(\n# browser.follow_link(         browser.get_url(             browser.open_relative(       browser.session              browser.url\n# browser.form                 browser.get_verbose(         browser.page(                browser.set_cookiejar(       \n# browser.get(                 browser.launch_browser(      browser.post(                browser.set_debug(\n\n# Fill-in the search form\nbrowser.select_form('#search_form_homepage')\n\n# this will open a browser to show the HTML currently selected in the browser object\nbrowser.launch_browser()\n\nbrowser[\"q\"] = \"MechanicalSoup\"\nbrowser.submit_selected()\n\n# Display the results\nfor link in browser.page.select('a.result__a'):\n    print(link.text, '-&gt;', link.attrs['href'])\n</code></pre>","text":""},{"location":"etl/extract/#splinter","title":"Splinter","text":"<p>Documentation</p> <p>Splinter is a Python framework that provides a simple and consistent interface for web application automation. Splinter can use Selenium-based Drivers, chrome, fireFox, edge, or remote, the python bindings for Selenium 3 or Selenium 4 must be installed.  It can also use Django and Flask based drivers.</p> <p><pre><code>from splinter import Browser\n\n\nbrowser = Browser('firefox')\nbrowser.visit('http://google.com')\nbrowser.find_by_name('q').fill('splinter - python acceptance testing for web applications')\nbrowser.find_by_name('btnK').click()\n\nif browser.is_text_present('splinter.readthedocs.io'):\n    print(\"Yes, the official website was found!\")\nelse:\n    print(\"No, it wasn't found... We need to improve our SEO techniques\")\n\nbrowser.quit()\n</code></pre> You can easily execute JavaScript, in drivers which support it: <pre><code>browser.execute_script(\"$('body').empty()\")\n</code></pre> You can return the result of the script: <pre><code>browser.evaluate_script(\"4+4\") == 8\n</code></pre> Some text input actions cannot be \u201ctyped\u201d thru browser.fill(), like new lines and tab characters. Below is en example how to work around this using browser.execute_script(). This is also much faster than browser.fill() as there is no simulated key typing delay, making it suitable for longer texts. <pre><code>def fast_fill_by_javascript(browser: DriverAPI, elem_id: str, text: str):\n\"\"\"Fill text field with copy-paste, not by typing key by key.\n    Otherwise you cannot type enter or tab.\n    :param id: CSS id of the textarea element to fill\n    \"\"\"\n    text = text.replace(\"\\t\", \"\\\\t\")\n    text = text.replace(\"\\n\", \"\\\\n\")\n\n    # Construct a JavaScript snippet that is executed on the browser sdie\n    snippet = f\"\"\"document.querySelector(\"#{elem_id}\").value = \"{text}\";\"\"\"\n    browser.execute_script(snippet)\n</code></pre></p>"},{"location":"etl/extract/#selenium","title":"Selenium","text":"<p>Documentation</p> <p>Selenium Python bindings provides a simple API to write functional/acceptance tests using Selenium WebDriver. Through Selenium Python API you can access all functionalities of Selenium WebDriver in an intuitive way.</p> <p><pre><code>from selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\n# The instance of Firefox WebDriver is created\ndriver = webdriver.Firefox()\n\n# The driver.get method will navigate to a page given by the URL. \n# WebDriver will wait until the page has fully loaded (the \u201conload\u201d event has fired)\n# before returning control to your test or script\ndriver.get(\"http://www.python.org\")\n\n# assertion to confirm that title has \u201cPython\u201d word in it\nassert \"Python\" in driver.title\n\n# WebDriver offers a number of ways to find elements using one of the find_element_by_* methods.\nelem = driver.find_element_by_name(\"q\")\n\n# we\u2019ll first clear any pre-populated text in the input field\nelem.clear()\n\n# we are sending keys, this is similar to entering keys using your keyboard\nelem.send_keys(\"pycon\")\nelem.send_keys(Keys.RETURN)\n\n# ensure that some results are found, make an assertion\nassert \"No results found.\" not in driver.page_source\n\n# The driver.quit() will exit entire browser whereas drive.close() will close one tab\ndriver.close()\n</code></pre> Some additional WebDrivers for Selenium include the following: <pre><code>webdriver.Firefox\nwebdriver.FirefoxProfile\nwebdriver.Chrome\nwebdriver.ChromeOptions\nwebdriver.Ie\nwebdriver.Opera\nwebdriver.PhantomJS\nwebdriver.Remote\nwebdriver.DesiredCapabilities\nwebdriver.ActionChains\nwebdriver.TouchActions\nwebdriver.Proxy\n</code></pre> To use without actually opening a window and save CPU use Chrome driver in <code>--headless</code> mode: <pre><code>from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\n\n# instantiate a chrome options object so you can set the size and headless preference\nchrome_options = Options()\nchrome_options.add_argument(\"--headless\")\n</code></pre></p>"},{"location":"etl/extract/#pandas","title":"Pandas","text":"<p>Pandas can be used to extract HTML tables from webpages</p> <pre><code>import pandas as pd\n\nurl = \"https://example.com\"\ntable_list = pd.read_html(url)\n\n# table_list will contain a list of pandas df's for each table on the page. \n# if the page uses HTML tables for layout purposes and not necessarily for storing data\n# it likely won't look good or be useful as is.\n\n# it's also faster to grab the page with requests and parse with pandas afterwards\nresponse = requests.get(url)\nif response.status_code == 200:\n    table_list = pd.read_html(response.content)\n</code></pre>"},{"location":"etl/extract/#apis","title":"APIs","text":"<p>Most APIs accessible with an HTTP request can be accessed with Requests <pre><code>url = \"https://api.example.com/somevalue\"\n\nresponse = requests.get(url)\n# returns a response object\n# The content from an API is most likely going to be JSON, check status code first\nif response.status_code == 200:\n    json_data = response.json()\n\n# if you are pulling from a private API you can try using a unique useragent.\nuseragent = {'User-Agent': 'Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 1.1.4322; .NET CLR 2.0.50727)'}\nresponse = requests.get(url, headers=useragent)\n</code></pre></p>"},{"location":"etl/extract/#databases","title":"Databases","text":""},{"location":"etl/extract/#sqlite3","title":"SQLite3","text":"<p>Documentation</p> <pre><code>import sqlite3\n\nconn = sqlite3.connect('test_database') \nc = conn.cursor()\n\nc.execute('''\n          CREATE TABLE IF NOT EXISTS products\n          ([product_id] INTEGER PRIMARY KEY, [product_name] TEXT, [price] INTEGER)\n          ''')\n\nc.execute('''\n          INSERT INTO products (product_id, product_name, price)\n\n                VALUES\n                (1,'Computer',800),\n                (2,'Printer',200),\n                (3,'Tablet',300),\n                (4,'Desk',450),\n                (5,'Chair',150)\n          ''')                     \n\nconn.commit()\n</code></pre>"},{"location":"etl/extract/#sqlalchemy","title":"SQLAlchemy","text":"<p>Documentation</p>"},{"location":"etl/extract/#viewing-tables","title":"Viewing Tables","text":"<pre><code>import sqlalchemy as db\nengine = db.create_engine('sqlite:///census.sqlite')\n\nconnection = engine.connect()\nmetadata = db.MetaData()\ncensus = db.Table('census', metadata, autoload=True, autoload_with=engine)\n</code></pre>"},{"location":"etl/extract/#querying-tables","title":"Querying Tables","text":"<p><pre><code>#Equivalent to 'SELECT * FROM census'\nquery = db.select([census])\n\n# ResultProxy: The object returned by the .execute() method. \n# It can be used in a variety of ways to get the data returned by the query.\nResultProxy = connection.execute(query)\n\n# ResultSet: The actual data asked for in the query when using\n# a fetch method such as .fetchall() on a ResultProxy.\nResultSet = ResultProxy.fetchall()\n</code></pre> Additional Querying tutorial SQLAlchemy \u2014 Python Tutorial</p>"},{"location":"etl/extract/#pandas_1","title":"Pandas","text":"<p>Documentation</p> <pre><code># import the modules\nimport pandas as pd \nfrom sqlalchemy import create_engine\n\n# SQLAlchemy connectable\ncnx = create_engine('sqlite:///contacts.db').connect()\n\n# table named 'contacts' will be returned as a dataframe.\ndf = pd.read_sql_table('contacts', cnx)\nprint(df)\n</code></pre>"},{"location":"etl/extract/#pyspark","title":"PySpark","text":"<p>Documentation</p> <pre><code># import the modules\n</code></pre>"},{"location":"etl/extract/#flat-files","title":"Flat Files","text":""},{"location":"etl/extract/#python","title":"Python","text":"<p>Python has the ability to read many files when given a path. Using <code>with open</code> will close the file when the with statement closes. More parameters on reading files can be found here Real Python Tutorial <pre><code>with open('dog_breeds.txt', 'r') as reader:\n    # Further file processing goes here\n    pass\n</code></pre> additional parameters include:</p> Character Meaning 'r' Open for reading (default) 'w' Open for writing, truncating (overwriting) the file first 'rb' or wb' Open in binary mode (read/write using byte data)"},{"location":"etl/extract/#pandas_2","title":"Pandas","text":"<p>Documentation</p> <p>Pandas can read many types of flat files including <code>csv</code>, <code>parquet</code>, <code>xlsx</code>, <code>txt</code>, <code>pickle</code>, <code>clipboard</code>, <code>xml</code>, <code>html</code>, <code>json</code> and others.</p> <pre><code>import pandas as pd\n\n# Reading CSV\ndf = pd.read_csv(\"path/to/file.csv\")\n# Reading CSV Faster with Pyarrow in Pandas 1.4\ndf = pd.read_csv(\"large.csv\", engine=\"pyarrow\")\n\n# Reading Parquet file\ndf = pd.read_parquet(\"large.parquet\")\n# Reading Parquet file with faster parquet engine\ndf = pd.read_parquet(\"large.parquet\", engine=\"fastparquet\")\n\n# Reading Excel\ndf = pd.read_excel(\"path/to/file.xlsx\")\n# Reading Excel with multiple sheets\nxls = pd.ExcelFile('path_to_file.xls')\ndf1 = pd.read_excel(xls, 'Sheet1')\ndf2 = pd.read_excel(xls, 'Sheet2')\n\n# Reading JSON\ndf = pd.read_json(\"path/to/file.json\")\n# nested JSON often leaves the df in an undesirable state\ndf = pd.json_normalize(df['data'])\n# this will flatten lists into columns\ndf.explode('col_of_lists')\n\n# Reading JSONL\ndf = pd.read_json(\"path/to/file.jsonl\", lines=True)\n</code></pre>"},{"location":"etl/load/","title":"ETL (Load)","text":""},{"location":"etl/load/#flat-files","title":"Flat Files","text":""},{"location":"etl/load/#python-print-statement","title":"Python Print Statement","text":"<p>Documentation</p> <p>Give print a file keyword argument, where the value of the argument is a file stream. We can create a file stream using the open function <pre><code># use file=open() to direct the output to the file location\n# using \"a\" appends or creates the file\n\nprint(\"Hello World!\", file=open(\"output.txt\", \"a\"))\n\nsome_text = \"This is a string of something I want to save\"\n\nprint(some_text, file=open(\"output.txt\", \"a\"))\n</code></pre> Using <code>a</code> with python's <code>open()</code> command will append the data to a new line separated by a line break this actually is exactly how <code>JSONL</code> documents are formated, where each new line is valid JSON. So running the  following will build valid <code>JSONL</code> documents.</p> <pre><code>some_dict = {'Name':\"Alice\"}\n\nprint(json.dumps(some_dict), file=open(\"logs.jsonl\", \"a\"))\n</code></pre>"},{"location":"etl/pipelines/","title":"ETL Pipelines","text":"<p>An ETL pipeline is the set of processes used to move data from a source or multiple sources into a database such as a data warehouse. ETL stands for \u201cextract, transform, load,\u201d the three interdependent processes of data integration used to pull data from one database and move it to another. Once loaded, data can be used for reporting, analysis, and deriving actionable business insights. </p>"},{"location":"etl/pipelines/#luigi","title":"Luigi","text":"<p>Documentation Luigi is a Python (2.7, 3.6, 3.7 tested) package that helps you build complex pipelines of batch jobs. It handles  dependency resolution, workflow management, visualization, handling failures, command line integration, and much more.</p> <pre><code>\"\"\"\nYou can run this example like this:\n    .. code:: console\n            $ luigi --module examples.hello_world examples.HelloWorldTask --local-scheduler\nIf that does not work, see :ref:`CommandLine`.\n\"\"\"\nimport luigi\n\n\nclass HelloWorldTask(luigi.Task):\n    task_namespace = 'examples'\n\n    def run(self):\n        print(\"{task} says: Hello world!\".format(task=self.__class__.__name__))\n\n\nif __name__ == '__main__':\n    luigi.run(['examples.HelloWorldTask', '--workers', '1', '--local-scheduler'])\n</code></pre>"},{"location":"etl/pipelines/#bonobo","title":"Bonobo","text":"<p>Documentation</p> <p>Bonobo is a lightweight Extract-Transform-Load (ETL) framework for Python 3.5+. It provides tools for building data transformation pipelines, using plain python primitives, and executing them in parallel. Bonobo is the swiss army knife for everyday's data.</p> <pre><code>import datetime\nimport time\n\nimport bonobo\n\n\ndef extract():\n\"\"\"Placeholder, change, rename, remove... \"\"\"\n    for x in range(60):\n        if x:\n            time.sleep(1)\n        yield datetime.datetime.now()\n\n\ndef get_graph():\n    graph = bonobo.Graph()\n    graph.add_chain(extract, print)\n\n    return graph\n\n\nif __name__ == \"__main__\":\n    parser = bonobo.get_argument_parser()\n    with bonobo.parse_args(parser):\n        bonobo.run(get_graph())\n</code></pre>"},{"location":"etl/transform/","title":"ETL (Transform)","text":"<p>Extract Transform Load is the process whereby some data is obtained, (extracted) cleaned, wrangled (transformed), and placed into a user-friendly data structure like a data frame (loaded).</p> <p>Transforming is </p>"},{"location":"etl/transform/#pandas","title":"Pandas","text":""},{"location":"etl/transform/#json","title":"JSON","text":"<p>Documentation</p> <p>Often JSON files directly translate to <code>pd.DataFrame</code> but nested JSON usually leave the JSON array in a single column </p> <pre><code>import json \nimport pandas as pd \nfrom pandas.io.json import json_normalize #package for flattening json in pd\n\ndf = pd.DataFrame(json_file)\nworks_data = json_normalize(data = df['programs'],\n                            record_path ='works', \n                            meta =['id', 'orchestra', 'programID', 'season'])\n\n# record_path is the column name you want to \"flatten\"\n# by that it makes JSON keys as a new\n# column name and places the value as the row value.\n# Also pass the parent metadata we wanted to append\n</code></pre>"},{"location":"etl/transform/#jsonl","title":"JSONL","text":"<p>Documentation</p> <p>Pandas can work with <code>JSONL</code> as well </p> <p><pre><code>import json \nimport pandas as pd \n\ndf = pd.read_json(jsonl_file, lines=True)\n\n# using lines=True lets Pandas read each line as a valid JSON file\n</code></pre> Additional Info</p>"},{"location":"ml/cluster/","title":"Clustering","text":"<p>Grouping related examples, particularly during unsupervised learning. Once all the examples are grouped, a human can optionally supply meaning to each cluster.</p> <p>Definition</p>"},{"location":"ml/cluster/#clustering-methods","title":"Clustering Methods","text":""},{"location":"ml/cluster/#k-means","title":"k-means","text":"<p>Documentation</p> <pre><code>from sklearn.datasets import make_blobs\nfrom numpy import quantile, random, where\nfrom sklearn.ensemble import IsolationForest\nimport matplotlib.pyplot as plt\n</code></pre> <p>More Info</p>"},{"location":"ml/cluster/#finding-number-of-clusters","title":"Finding number of Clusters","text":""},{"location":"ml/cluster/#elbow-method","title":"Elbow Method","text":""},{"location":"ml/cluster/#bayesian-information-criterion-bic","title":"Bayesian Information Criterion (BIC)","text":"<p>Documentation</p> <pre><code>def bic_score(X, labels):\n\"\"\"\n  BIC score for the goodness of fit of clusters.\n  This Python function is directly translated from the GoLang code made by the author of the paper. \n  The original code is available here: https://github.com/bobhancock/goxmeans/blob/a78e909e374c6f97ddd04a239658c7c5b7365e5c/km.go#L778\n  \"\"\"\n\n  n_points = len(labels)\n  n_clusters = len(set(labels))\n  n_dimensions = X.shape[1]\n\n  n_parameters = (n_clusters - 1) + (n_dimensions * n_clusters) + 1\n\n  loglikelihood = 0\n  for label_name in set(labels):\n    X_cluster = X[labels == label_name]\n    n_points_cluster = len(X_cluster)\n    centroid = np.mean(X_cluster, axis=0)\n    variance = np.sum((X_cluster - centroid) ** 2) / (len(X_cluster) - 1)\n    loglikelihood += \\\n      n_points_cluster * np.log(n_points_cluster) \\\n      - n_points_cluster * np.log(n_points) \\\n      - n_points_cluster * n_dimensions / 2 * np.log(2 * math.pi * variance) \\\n      - (n_points_cluster - 1) / 2\n\n  bic = loglikelihood - (n_parameters / 2) * np.log(n_points)\n\n  return bic\n</code></pre>"},{"location":"ml/ml/","title":"Machine Learning","text":"<p>Definition</p>"},{"location":"ml/ml/#anomoly-detection","title":"Anomoly Detection","text":""},{"location":"ml/ml/#isolation-forests","title":"Isolation Forests","text":"<p>Documentation</p> <p>Recall that decision trees are built using information criteria such as Gini index or entropy. The obviously different groups are separated at the root of the tree and deeper into the branches, the subtler distinctions are identified. </p> <p>Based on randomly picked characteristics, an isolation forest processes the randomly subsampled data in a tree structure. Samples that reach further into the tree and require more cuts to separate them have a very little probability that they are anomalies. Likewise, samples that are found on the shorter branches of the tree are more likely to be anomalies, since the tree found it simpler to distinguish them from the other data.</p> <pre><code>from sklearn.datasets import make_blobs\nfrom numpy import quantile, random, where\nfrom sklearn.ensemble import IsolationForest\nimport matplotlib.pyplot as plt\n</code></pre> <pre><code>IF = IsolationForest(n_estimators=100, contamination=.03)\npredictions = IF.fit_predict(X)\n\noutlier_index = where(predictions==-1)\nvalues = X[outlier_index]\n\nplt.scatter(X[:,0], X[:,1])\nplt.scatter(values[:,0], values[:,1], color='y')\nplt.show()\n</code></pre> <p></p> <p>More Info</p>"},{"location":"ml/nlp/","title":"Natural Language Processing","text":""},{"location":"ml/nlp/#summarization","title":"Summarization","text":""},{"location":"ml/nlp/#extractive-summary","title":"Extractive Summary","text":"<p>Documentation</p> <p>Extractive Summary: This method summarizes the text by selecting the most important subset of sentences from the original text. As the name suggests, it extracts the most important information from the text. This method does not have the capability of text generation by itself and hence the output will always have some part of the original text.</p> <p><pre><code>from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom collections import Counter\n\nstopWords = set(stopwords.words(\"english\"))\n\ndef Summary(text: str, sensitivity: float=1.2):\n    words = [word.lower() for word in word_tokenize(text) if word.lower() not in stopWords]\n    freqTable = dict(Counter(words))\n    sentences = [sentence.lower() for sentence in sent_tokenize(text)]\n    sentenceValues = {}\n    for sentence in sentences:\n        sentenceValues[sentence] = 0\n        for word in sentence.split():\n            sentenceValues[sentence] += freqTable.get(word, 0)\n    average = int(sum(sentenceValues.values()) / len(sentenceValues))\n    summary = \" \".join(sentence for sentence in sentences if sentenceValues[sentence] &gt; sensitivity * average)\n    return summary\n</code></pre> The above code removes stopwords from the text (the, and, or, at, etc.) then counts how frequently each word is used. This sort of becomes the score for a word. Then it loops through each word of each sentence and tallies up the score for the sentence. Finally it takes an average score for the sentences and builds the summary from the highest  scoring sentences (as a multiple of the sensitivity param).</p>"},{"location":"ml/nlp/#abstractive-summary","title":"Abstractive Summary","text":"<p>Abstractive Summary: The idea behind this method is to understand the core context of the original text and produce new text based on this understanding. It can be compared to the way humans read and summarize text in their own way. The output of abstractive summary can have elements not present in the original text.</p>"},{"location":"nlp/nlp/","title":"Natural Language Processing","text":"<p>Natural language processing (NLP) is a branch of artificial intelligence that helps computers understand, interpret and manipulate human language. NLP draws from many disciplines, including computer science and computational linguistics, in its pursuit to fill the gap between human communication and computer understanding. </p>"},{"location":"nlp/nlp/#text-similarity","title":"Text Similarity","text":"<p>Text similarity can be broken down into two components, semantic similarity and lexical similarity. Given a pair of text, the semantic similarity of the pair refers to how close the documents are in meaning. Whereas, lexical similarity is a measure  of overlap in vocabulary. If both documents in the pairs have the same vocabularies, then they would have a lexical similarity of 1 and vice versa of 0 if there was no overlap in vocabularies.</p>"},{"location":"nlp/nlp/#gestalt-pattern-matching","title":"Gestalt pattern matching","text":"<p>Documentation</p> <p>Gestalt pattern matching, also Ratcliff/Obershelp pattern recognition, is a string-matching algorithm for determining the similarity of two strings.</p> String letter letter letter letter letter letter letter letter letter S1 W I K I M E D I A S1 W I K I M A N I A <p>The longest common substring is WIKIM (grey) with 5 characters. There is no further substring on the left. The non-matching substrings on the right side are EDIA and ANIA. They again have a longest common substring IA (dark gray) with length 2. The similarity metric is determined by:</p> <p></p> <pre><code># Drqr Implementation in Python\ndef real_quick_ratio(s1: str, s2: str) -&gt; float:\n\"\"\"Return an upper bound on ratio() very quickly.\"\"\"\n    l1, l2 = len(s1), len(s2)\n    length = l1 + l2\n\n    if not length:\n        return 1.0\n\n    return 2.0 * min(l1, l2) / length\n</code></pre> <pre><code># Dqr Implementation in Python\ndef quick_ratio(s1: str, s2: str) -&gt; float:\n\"\"\"Return an upper bound on ratio() relatively quickly.\"\"\"\n    length = len(s1) + len(s2)\n\n    if not length:\n        return 1.0\n\n    intersect = collections.Counter(s1) &amp; collections.Counter(s2)\n    matches = sum(intersect.values())\n    return 2.0 * matches / length\n</code></pre>"},{"location":"nlp/nlp/#levenshtein-distance","title":"Levenshtein Distance","text":"<p>Documentation</p> <p>Levenshtein distance is very impactful because it does not require two strings to be of equal length for them to be compared. Intuitively speaking, Levenshtein distance is quite easy to understand.</p> <p>Essentially implying that the output distance between the two is the cumulative sum of the single-character edits.  The larger the output distance implies that more changes were necessary to make the two words equal to each other, and  the lower the output distance implies that fewer changes were necessary. For example, given a pair of words <code>dream</code> and <code>dream</code> the resulting Levenshtein distance would be 0 because the two words are the same. However,  if the words were <code>dream</code> and <code>steam</code> the Levenshtein distance would be 2 as you would need to make 2 edits to change <code>dr</code> to <code>st</code>.</p> <pre><code>def LevenshteinRatio(string1: str, string2: str):\n\"\"\" levenshtein_ratio_and_distance:\n        Calculates levenshtein distance between two strings.\n        If ratio_calc = True, the function computes the\n        levenshtein distance ratio of similarity between two strings\n        For all i and j, distance[i,j] will contain the Levenshtein\n        distance between the first i characters of s and the\n        first j characters of t\n    \"\"\"\n    rows = len(string1) + 1\n    cols = len(string2) + 1\n    # Initialize matrix of zeros\n    distance = np.zeros((rows, cols), dtype = int)\n    # Iterate over the matrix to compute the cost of deletions,insertions and/or substitutions\n    for i in range(1, rows):\n        for k in range(1, cols):\n            distance[i][0] = i\n            distance[0][k] = k\n    for col in range(1, cols):\n        for row in range(1, rows):\n            # the cost of a substitution is 2. If we calculate just distance, then the cost of a substitution is 1\n            cost = 0 if string1[row-1] == string2[col-1] else 2\n            distance[row][col] = min(\n                distance[row-1][col] + 1, # Cost of deletions\n                distance[row][col-1] + 1, # Cost of insertions\n                distance[row-1][col-1] + cost # Cost of substitutions\n            )\n    # Computation of the Levenshtein Distance Ratio\n    Ratio = ((len(string1) + len(string2)) - distance[row][col]) / (len(string1) + len(string2))\n    return Ratio\n\nprint(LevenshteinRatio(\"apple\", \"apple\"))\n# 1.0\nprint(LevenshteinRatio(\"apple\", \"apples\"))\n# 0.9090909090909091\nprint(LevenshteinRatio(\"applez\", \"apples\"))\n# 0.8333333333333334\nprint(LevenshteinRatio(\"apples\", \"oranges\"))\n# 0.46153846153846156\n</code></pre>"},{"location":"nlp/nlp/#extractive-summary","title":"Extractive Summary","text":""},{"location":"nlp/nlp/#naive-extractive-summary","title":"Naive Extractive Summary","text":"<p>Documentation</p> <p>Summarizing is based on ranks of text sentences using a variation of the TextRank algorithm</p> <p>Extractive summarization based on TextRank involves extracting all non-stopwords (e.g. The, if, and etc)  and giving words a score based on the number of occurrences. You then extract sentences from the text and loop through  each sentence and tallying a score based on the words used.</p> <p>A naive implementation is below:  <pre><code>from collections import Counter\n\nclass Summarizer:\n    def __init__(self):\n        self.stopWords = stopWords = ['i', 'me', 'my', 'myself', 'add more stopwords']\n    def summary(self, text: str, summary_sentences: int=3, sensitivity: float=1.2) -&gt; dict:\n        words = [\n            word\n            for word in text.split()\n            if word.lower() not in self.stopWords\n        ]\n        freqTable = dict(Counter(words))\n        sentences = [\n            sentence\n            for sentence in text.split('. ')\n        ]\n        sentenceValues = {}\n        for sentence in sentences:\n            sentenceValues[sentence] = 0\n            for word in sentence.split():\n                sentenceValues[sentence] += freqTable.get(word, 0)\n        average = int(sum(sentenceValues.values()) / len(sentenceValues))\n        summary = \" \".join(\n            sentence\n            for sentence in sentences\n            if sentenceValues[sentence] &gt; sensitivity * average\n        )\n        n_sentence_summary = \" \".join(\n            dict(\n                sorted(sentenceValues.items(), key=lambda x: x[1], reverse=True)[:summary_sentences]\n            )\n        )\n        top_ten_words = {key: val for key, val in sorted(freqTable.items(), key = lambda ele: ele[1], reverse = True)}\n        top_ten_words = [word for word in top_ten_words if word.isalpha()]\n        data = {\n            \"keywords\":list(set(words)),\n            \"frequency_table\":freqTable,\n            \"top_ten_words\": top_ten_words,\n            \"average_score\":average,\n            \"sensitivity_summary\":summary,\n            \"top_sentence\":max(sentenceValues.items(), key=lambda x: x[1])[0],\n            \"n_sentence_summary\":n_sentence_summary,\n            \"sentence_values\":sentenceValues,\n            \"summary_count\":len(n_sentence_summary.split())\n        }\n        return data\n</code></pre></p>"},{"location":"schedule/scheduling/","title":"Scheduling Tasks","text":"<p>On how to schedule tasks </p>"},{"location":"schedule/scheduling/#dagster","title":"Dagster","text":"<p>Documentation</p> <p>Dagster is a data orchestrator. It lets you define jobs in terms of the data flow between logical components called ops. These jobs can be developed locally and run anywhere. Dagster uses decorators like <code>@op</code> to decorate functions and <code>@job</code> to group <code>@op</code> into a single flow.</p> <pre><code>from dagster import job, op\n\n@op\ndef get_name():\n    return \"dagster\"\n\n@op\ndef hello(name: str):\n    print(f\"Hello, {name}!\")\n\n@job\ndef hello_dagster():\n    hello(get_name())\n</code></pre> <p>Then run with the follow command <pre><code>dagit -f hello_world.py\n</code></pre> Which will serve up a web UI</p> <p></p>"},{"location":"schedule/scheduling/#prefect","title":"Prefect","text":"<p>Documentation</p> <p>Prefect Orion is the second-generation workflow orchestration engine from Prefect, now available as a technical preview. Orion has been designed from the ground up to handle the dynamic, scalable workloads that the modern data stack demands. Powered by a brand-new, asynchronous rules engine, it represents an enormous amount of research, development, and dedication to a simple idea: You should love your workflows again.</p> <pre><code>from prefect import flow, task\nfrom typing import List\nimport httpx\n\n\n@task(retries=3)\ndef get_stars(repo: str):\n    url = f\"https://api.github.com/repos/{repo}\"\n    count = httpx.get(url).json()[\"stargazers_count\"]\n    print(f\"{repo} has {count} stars!\")\n\n\n@flow(name=\"Github Stars\")\ndef github_stars(repos: List[str]):\n    for repo in repos:\n        get_stars(repo)\n\n\n# run the flow!\ngithub_stars([\"PrefectHQ/Prefect\", \"PrefectHQ/miter-design\"])\n</code></pre> <p>Then run with the follow command <pre><code>prefect orion start\n</code></pre> Which will serve up a web UI</p> <p></p>"},{"location":"schedule/scheduling/#celery","title":"Celery","text":"<p>Documentation</p> <pre><code>from celery import Celery\n\napp = Celery('tasks', broker='pyamqp://guest@localhost//')\n\n@app.task\ndef add(x, y):\n    return x + y\n</code></pre>"},{"location":"schedule/scheduling/#schedule","title":"Schedule","text":"<p>Documentation</p> <p>Python job scheduling for humans. Run Python functions (or any other callable) periodically using a friendly syntax.</p> <ul> <li>A simple to use API for scheduling jobs, made for humans.</li> <li>In-process scheduler for periodic jobs. No extra processes needed!</li> <li>Very lightweight and no external dependencies.</li> <li>Excellent test coverage.</li> <li>Tested on Python 3.6, 3.7, 3.8 and 3.9</li> </ul> <p>Schedule is not a \u2018one size fits all\u2019 scheduling library. This library is designed to be a simple solution for simple scheduling problems. You should probably look somewhere else if you need:</p> <ul> <li>Job persistence (remember schedule between restarts)</li> <li>Exact timing (sub-second precision execution)</li> <li>Concurrent execution (multiple threads)</li> <li>Localization (time zones, workdays or holidays)</li> </ul> <p>Schedule does not account for the time it takes for the job function to execute. To guarantee a stable execution schedule you need to move long-running jobs off the main-thread (where the scheduler runs). See Parallel execution for a sample implementation.</p> <p>Simple Single threaded execution: <pre><code>import schedule\nimport time\n\ndef job():\n    print(\"I'm working...\")\n\nschedule.every(10).minutes.do(job)\nschedule.every().hour.do(job)\nschedule.every().day.at(\"10:30\").do(job)\nschedule.every().monday.do(job)\nschedule.every().wednesday.at(\"13:15\").do(job)\nschedule.every().minute.at(\":17\").do(job)\n\nwhile True:\n    schedule.run_pending()\n    time.sleep(1)\n</code></pre></p> <p>Multi threaded execution:</p> <p>More Info</p> <pre><code>import threading\nimport time\nimport schedule\n\ndef job():\n    print(\"I'm running on thread %s\" % threading.current_thread())\n\ndef run_threaded(job_func):\n    job_thread = threading.Thread(target=job_func)\n    job_thread.start()\n\nschedule.every(10).seconds.do(run_threaded, job)\nschedule.every(10).seconds.do(run_threaded, job)\nschedule.every(10).seconds.do(run_threaded, job)\nschedule.every(10).seconds.do(run_threaded, job)\nschedule.every(10).seconds.do(run_threaded, job)\n\n\nwhile 1:\n    schedule.run_pending()\n    time.sleep(1)\n</code></pre>"},{"location":"viz/viz/","title":"Viz","text":""},{"location":"viz/viz/#visualization-credit","title":"Visualization Credit","text":"<p>All these plots, code, and explanations come from this website</p> <p>www.machinelearningplus.com</p>"},{"location":"viz/viz/#correlation","title":"Correlation","text":"<p>The plots under correlation is used to visualize the relationship between 2 or more variables. That is, how does one variable change with respect to another.</p>"},{"location":"viz/viz/#scatter-plot","title":"Scatter plot","text":"<p>Scatterplot is a classic and fundamental plot used to study the relationship between two variables. If you have multiple groups in your data you may want to visualise each group in a different color. In <code>matplotlib</code>, you can conveniently do this using <code>plt.scatterplot()</code>.</p> Show Code <pre><code>    # Import dataset \n    midwest = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/midwest_filter.csv\")\n\n    # Prepare Data \n    # Create as many colors as there are unique midwest['category']\n    categories = np.unique(midwest['category'])\n    colors = [plt.cm.tab10(i/float(len(categories)-1)) for i in range(len(categories))]\n\n    # Draw Plot for Each Category\n    plt.figure(figsize=(16, 10), dpi= 80, facecolor='w', edgecolor='k')\n\n    for i, category in enumerate(categories):\n        plt.scatter('area', 'poptotal', \n                    data=midwest.loc[midwest.category==category, :], \n                    s=20, c=colors[i], label=str(category))\n\n    # Decorations\n    plt.gca().set(xlim=(0.0, 0.1), ylim=(0, 90000),\n                  xlabel='Area', ylabel='Population')\n\n    plt.xticks(fontsize=12); plt.yticks(fontsize=12)\n    plt.title(\"Scatterplot of Midwest Area vs Population\", fontsize=22)\n    plt.legend(fontsize=12)    \n    plt.show()    \n</code></pre> <p></p>"},{"location":"viz/viz/#bubble-plot-with-encircling","title":"Bubble plot with Encircling","text":"<p>Sometimes you want to show a group of points within a boundary to emphasize their importance. In this example, you get the records from the dataframe that should be encircled and pass it to the <code>encircle()</code> described in the code below.</p> Show Code <pre><code>    from matplotlib import patches\n    from scipy.spatial import ConvexHull\n    import warnings; warnings.simplefilter('ignore')\n    sns.set_style(\"white\")\n\n    # Step 1: Prepare Data\n    midwest = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/midwest_filter.csv\")\n\n    # As many colors as there are unique midwest['category']\n    categories = np.unique(midwest['category'])\n    colors = [plt.cm.tab10(i/float(len(categories)-1)) for i in range(len(categories))]\n\n    # Step 2: Draw Scatterplot with unique color for each category\n    fig = plt.figure(figsize=(16, 10), dpi= 80, facecolor='w', edgecolor='k')    \n\n    for i, category in enumerate(categories):\n        plt.scatter('area', 'poptotal', data=midwest.loc[midwest.category==category, :], s='dot_size', c=colors[i], label=str(category), edgecolors='black', linewidths=.5)\n\n    # Step 3: Encircling\n    # https://stackoverflow.com/questions/44575681/how-do-i-encircle-different-data-sets-in-scatter-plot\n    def encircle(x,y, ax=None, **kw):\n        if not ax: ax=plt.gca()\n        p = np.c_[x,y]\n        hull = ConvexHull(p)\n        poly = plt.Polygon(p[hull.vertices,:], **kw)\n        ax.add_patch(poly)\n\n    # Select data to be encircled\n    midwest_encircle_data = midwest.loc[midwest.state=='IN', :]                         \n\n    # Draw polygon surrounding vertices    \n    encircle(midwest_encircle_data.area, midwest_encircle_data.poptotal, ec=\"k\", fc=\"gold\", alpha=0.1)\n    encircle(midwest_encircle_data.area, midwest_encircle_data.poptotal, ec=\"firebrick\", fc=\"none\", linewidth=1.5)\n\n    # Step 4: Decorations\n    plt.gca().set(xlim=(0.0, 0.1), ylim=(0, 90000),\n                  xlabel='Area', ylabel='Population')\n\n    plt.xticks(fontsize=12); plt.yticks(fontsize=12)\n    plt.title(\"Bubble Plot with Encircling\", fontsize=22)\n    plt.legend(fontsize=12)    \n    plt.show()    \n</code></pre> <p></p>"},{"location":"viz/viz/#scatter-plot-with-linear-regression-line-of-best-fit","title":"Scatter plot with linear regression line of best fit","text":"<p>If you want to understand how two variables change with respect to each other, the line of best fit is the way to go. The below plot shows how the line of best fit differs amongst various groups in the data. To disable the groupings and to just draw one line-of-best-fit for the entire dataset, remove the <code>hue='cyl'</code> parameter from the <code>sns.lmplot()</code> call below.</p> Show Code <pre><code>    # Import Data\n    df = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\")\n    df_select = df.loc[df.cyl.isin([4,8]), :]\n\n    # Plot\n    sns.set_style(\"white\")\n    gridobj = sns.lmplot(x=\"displ\", y=\"hwy\", hue=\"cyl\", data=df_select, \n                         height=7, aspect=1.6, robust=True, palette='tab10', \n                         scatter_kws=dict(s=60, linewidths=.7, edgecolors='black'))\n\n    # Decorations\n    gridobj.set(xlim=(0.5, 7.5), ylim=(0, 50))\n    plt.title(\"Scatterplot with line of best fit grouped by number of cylinders\", fontsize=20)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#each-regression-line-in-its-own-column","title":"Each regression line in its own column","text":"<p>Alternately, you can show the best fit line for each group in its own column. You cando this by setting the <code>col=groupingcolumn</code> parameter inside the <code>sns.lmplot()</code>.</p> Show Code <pre><code>    # Import Data\n    df = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\")\n    df_select = df.loc[df.cyl.isin([4,8]), :]\n\n    # Each line in its own column\n    sns.set_style(\"white\")\n    gridobj = sns.lmplot(x=\"displ\", y=\"hwy\", \n                         data=df_select, \n                         height=7, \n                         robust=True, \n                         palette='Set1', \n                         col=\"cyl\",\n                         scatter_kws=dict(s=60, linewidths=.7, edgecolors='black'))\n\n    # Decorations\n    gridobj.set(xlim=(0.5, 7.5), ylim=(0, 50))\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#jittering-with-stripplot","title":"Jittering with stripplot","text":"<p>Often multiple datapoints have exactly the same X and Y values. As a result, multiple points get plotted over each other and hide. To avoid this, jitter the points slightly so you can visually see them. This is convenient to do using seaborn\u2019s <code>stripplot()</code>.</p> Show Code <pre><code>    # Import Data\n    df = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\")\n\n    # Draw Stripplot\n    fig, ax = plt.subplots(figsize=(16,10), dpi= 80)    \n    sns.stripplot(df.cty, df.hwy, jitter=0.25, size=8, ax=ax, linewidth=.5)\n\n    # Decorations\n    plt.title('Use jittered plots to avoid overlapping of points', fontsize=22)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#counts-plot","title":"Counts Plot","text":"<p>Another option to avoid the problem of points overlap is the increase the size of the dot depending on how many points lie in that spot. So, larger the size of the point more is the concentration of points around that.</p> Show Code <pre><code>    # Import Data\n    df = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\")\n    df_counts = df.groupby(['hwy', 'cty']).size().reset_index(name='counts')\n\n    # Draw Stripplot\n    fig, ax = plt.subplots(figsize=(16,10), dpi= 80)    \n    sns.stripplot(df_counts.cty, df_counts.hwy, size=df_counts.counts*2, ax=ax)\n\n    # Decorations\n    plt.title('Counts Plot - Size of circle is bigger as more points overlap', fontsize=22)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#marginal-histogram","title":"Marginal Histogram","text":"<p>Marginal histograms have a histogram along the X and Y axis variables. This is used to visualize the relationship between the X and Y along with the univariate distribution of the X and the Y individually. This plot if often used in exploratory data analysis (EDA).</p> Show Code <pre><code>    # Import Data\n    df = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\")\n\n    # Create Fig and gridspec\n    fig = plt.figure(figsize=(16, 10), dpi= 80)\n    grid = plt.GridSpec(4, 4, hspace=0.5, wspace=0.2)\n\n    # Define the axes\n    ax_main = fig.add_subplot(grid[:-1, :-1])\n    ax_right = fig.add_subplot(grid[:-1, -1], xticklabels=[], yticklabels=[])\n    ax_bottom = fig.add_subplot(grid[-1, 0:-1], xticklabels=[], yticklabels=[])\n\n    # Scatterplot on main ax\n    ax_main.scatter('displ', 'hwy', s=df.cty*4, c=df.manufacturer.astype('category').cat.codes, alpha=.9, data=df, cmap=\"tab10\", edgecolors='gray', linewidths=.5)\n\n    # histogram on the right\n    ax_bottom.hist(df.displ, 40, histtype='stepfilled', orientation='vertical', color='deeppink')\n    ax_bottom.invert_yaxis()\n\n    # histogram in the bottom\n    ax_right.hist(df.hwy, 40, histtype='stepfilled', orientation='horizontal', color='deeppink')\n\n    # Decorations\n    ax_main.set(title='Scatterplot with Histograms \\n displ vs hwy', xlabel='displ', ylabel='hwy')\n    ax_main.title.set_fontsize(20)\n    for item in ([ax_main.xaxis.label, ax_main.yaxis.label] + ax_main.get_xticklabels() + ax_main.get_yticklabels()):\n        item.set_fontsize(14)\n\n    xlabels = ax_main.get_xticks().tolist()\n    ax_main.set_xticklabels(xlabels)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#marginal-boxplot","title":"Marginal Boxplot","text":"<p>Marginal boxplot serves a similar purpose as marginal histogram. However, the boxplot helps to pinpoint the median, 25th and 75th percentiles of the X and the Y.</p> Show Code <pre><code>    # Import Data\n    df = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\")\n\n    # Create Fig and gridspec\n    fig = plt.figure(figsize=(16, 10), dpi= 80)\n    grid = plt.GridSpec(4, 4, hspace=0.5, wspace=0.2)\n\n    # Define the axes\n    ax_main = fig.add_subplot(grid[:-1, :-1])\n    ax_right = fig.add_subplot(grid[:-1, -1], xticklabels=[], yticklabels=[])\n    ax_bottom = fig.add_subplot(grid[-1, 0:-1], xticklabels=[], yticklabels=[])\n\n    # Scatterplot on main ax\n    ax_main.scatter('displ', 'hwy', s=df.cty*5, c=df.manufacturer.astype('category').cat.codes, alpha=.9, data=df, cmap=\"Set1\", edgecolors='black', linewidths=.5)\n\n    # Add a graph in each part\n    sns.boxplot(df.hwy, ax=ax_right, orient=\"v\")\n    sns.boxplot(df.displ, ax=ax_bottom, orient=\"h\")\n\n    # Decorations ------------------\n    # Remove x axis name for the boxplot\n    ax_bottom.set(xlabel='')\n    ax_right.set(ylabel='')\n\n    # Main Title, Xlabel and YLabel\n    ax_main.set(title='Scatterplot with Histograms \\n displ vs hwy', xlabel='displ', ylabel='hwy')\n\n    # Set font size of different components\n    ax_main.title.set_fontsize(20)\n    for item in ([ax_main.xaxis.label, ax_main.yaxis.label] + ax_main.get_xticklabels() + ax_main.get_yticklabels()):\n        item.set_fontsize(14)\n\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#correllogram","title":"Correllogram","text":"<p>Correlogram is used to visually see the correlation metric between all possible pairs of numeric variables in a given dataframe (or 2D array).</p> Show Code <pre><code>    # Import Dataset\n    df = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mtcars.csv\")\n\n    # Plot\n    plt.figure(figsize=(12,10), dpi= 80)\n    sns.heatmap(df.corr(), xticklabels=df.corr().columns, yticklabels=df.corr().columns, cmap='RdYlGn', center=0, annot=True)\n\n    # Decorations\n    plt.title('Correlogram of mtcars', fontsize=22)\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#pairwise-plot","title":"Pairwise Plot","text":"<p>Pairwise plot is a favorite in exploratory analysis to understand the relationship between all possible pairs of numeric variables. It is a must have tool for bivariate analysis.</p> Show Code <pre><code>    # Load Dataset\n    df = sns.load_dataset('iris')\n\n    # Plot\n    plt.figure(figsize=(10,8), dpi= 80)\n    sns.pairplot(df, kind=\"scatter\", hue=\"species\", plot_kws=dict(s=80, edgecolor=\"white\", linewidth=2.5))\n    plt.show()\n</code></pre> <p></p> Show Code <pre><code>    # Load Dataset\n    df = sns.load_dataset('iris')\n\n    # Plot\n    plt.figure(figsize=(10,8), dpi= 80)\n    sns.pairplot(df, kind=\"reg\", hue=\"species\")\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#deviation","title":"Deviation","text":""},{"location":"viz/viz/#diverging-bars","title":"Diverging Bars","text":"<p>If you want to see how the items are varying based on a single metric and visualize the order and amount of this variance, the diverging bars is a great tool. It helps to quickly differentiate the performance of groups in your data and is quite intuitive and instantly conveys the point.</p> Show Code <pre><code>    # Prepare Data\n    df = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mtcars.csv\")\n    x = df.loc[:, ['mpg']]\n    df['mpg_z'] = (x - x.mean())/x.std()\n    df['colors'] = ['red' if x &lt; 0 else 'green' for x in df['mpg_z']]\n    df.sort_values('mpg_z', inplace=True)\n    df.reset_index(inplace=True)\n\n    # Draw plot\n    plt.figure(figsize=(14,10), dpi= 80)\n    plt.hlines(y=df.index, xmin=0, xmax=df.mpg_z, color=df.colors, alpha=0.4, linewidth=5)\n\n    # Decorations\n    plt.gca().set(ylabel='$Model$', xlabel='$Mileage$')\n    plt.yticks(df.index, df.cars, fontsize=12)\n    plt.title('Diverging Bars of Car Mileage', fontdict={'size':20})\n    plt.grid(linestyle='--', alpha=0.5)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#diverging-texts","title":"Diverging Texts","text":"<p>Diverging texts is similar to diverging bars and it preferred if you want to show the value of each items within the chart in a nice and presentable way.</p> Show Code <pre><code>    # Prepare Data\n    df = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mtcars.csv\")\n    x = df.loc[:, ['mpg']]\n    df['mpg_z'] = (x - x.mean())/x.std()\n    df['colors'] = ['red' if x &lt; 0 else 'green' for x in df['mpg_z']]\n    df.sort_values('mpg_z', inplace=True)\n    df.reset_index(inplace=True)\n\n    # Draw plot\n    plt.figure(figsize=(14,14), dpi= 80)\n    plt.hlines(y=df.index, xmin=0, xmax=df.mpg_z)\n    for x, y, tex in zip(df.mpg_z, df.index, df.mpg_z):\n        t = plt.text(x, y, round(tex, 2), horizontalalignment='right' if x &lt; 0 else 'left', \n                     verticalalignment='center', fontdict={'color':'red' if x &lt; 0 else 'green', 'size':14})\n\n    # Decorations    \n    plt.yticks(df.index, df.cars, fontsize=12)\n    plt.title('Diverging Text Bars of Car Mileage', fontdict={'size':20})\n    plt.grid(linestyle='--', alpha=0.5)\n    plt.xlim(-2.5, 2.5)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#diverging-dot-plot","title":"Diverging Dot Plot","text":"<p>Divering dot plot is also similar to the diverging bars. However compared to diverging bars, the absence of bars reduces the amount of contrast and disparity between the groups.</p> Show Code <pre><code>    # Prepare Data\n    df = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mtcars.csv\")\n    x = df.loc[:, ['mpg']]\n    df['mpg_z'] = (x - x.mean())/x.std()\n    df['colors'] = ['red' if x &lt; 0 else 'darkgreen' for x in df['mpg_z']]\n    df.sort_values('mpg_z', inplace=True)\n    df.reset_index(inplace=True)\n\n    # Draw plot\n    plt.figure(figsize=(14,16), dpi= 80)\n    plt.scatter(df.mpg_z, df.index, s=450, alpha=.6, color=df.colors)\n    for x, y, tex in zip(df.mpg_z, df.index, df.mpg_z):\n        t = plt.text(x, y, round(tex, 1), horizontalalignment='center', \n                     verticalalignment='center', fontdict={'color':'white'})\n\n    # Decorations\n    # Lighten borders\n    plt.gca().spines[\"top\"].set_alpha(.3)\n    plt.gca().spines[\"bottom\"].set_alpha(.3)\n    plt.gca().spines[\"right\"].set_alpha(.3)\n    plt.gca().spines[\"left\"].set_alpha(.3)\n\n    plt.yticks(df.index, df.cars)\n    plt.title('Diverging Dotplot of Car Mileage', fontdict={'size':20})\n    plt.xlabel('$Mileage$')\n    plt.grid(linestyle='--', alpha=0.5)\n    plt.xlim(-2.5, 2.5)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#diverging-lollipop-chart-with-markers","title":"Diverging Lollipop Chart with Markers","text":"<p>Lollipop with markers provides a flexible way of visualizing the divergence by laying emphasis on any significant datapoints you want to bring attention to and give reasoning within the chart appropriately.</p> Show Code <pre><code>    # Prepare Data\n    df = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mtcars.csv\")\n    x = df.loc[:, ['mpg']]\n    df['mpg_z'] = (x - x.mean())/x.std()\n    df['colors'] = 'black'\n\n    # color fiat differently\n    df.loc[df.cars == 'Fiat X1-9', 'colors'] = 'darkorange'\n    df.sort_values('mpg_z', inplace=True)\n    df.reset_index(inplace=True)\n\n\n    # Draw plot\n    import matplotlib.patches as patches\n\n    plt.figure(figsize=(14,16), dpi= 80)\n    plt.hlines(y=df.index, xmin=0, xmax=df.mpg_z, color=df.colors, alpha=0.4, linewidth=1)\n    plt.scatter(df.mpg_z, df.index, color=df.colors, s=[600 if x == 'Fiat X1-9' else 300 for x in df.cars], alpha=0.6)\n    plt.yticks(df.index, df.cars)\n    plt.xticks(fontsize=12)\n\n    # Annotate\n    plt.annotate('Mercedes Models', xy=(0.0, 11.0), xytext=(1.0, 11), xycoords='data', \n                fontsize=15, ha='center', va='center',\n                bbox=dict(boxstyle='square', fc='firebrick'),\n                arrowprops=dict(arrowstyle='-[, widthB=2.0, lengthB=1.5', lw=2.0, color='steelblue'), color='white')\n\n    # Add Patches\n    p1 = patches.Rectangle((-2.0, -1), width=.3, height=3, alpha=.2, facecolor='red')\n    p2 = patches.Rectangle((1.5, 27), width=.8, height=5, alpha=.2, facecolor='green')\n    plt.gca().add_patch(p1)\n    plt.gca().add_patch(p2)\n\n    # Decorate\n    plt.title('Diverging Bars of Car Mileage', fontdict={'size':20})\n    plt.grid(linestyle='--', alpha=0.5)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#area-chart","title":"Area Chart","text":"<p>By coloring the area between the axis and the lines, the area chart throws more emphasis not just on the peaks and troughs but also the duration of the highs and lows. The longer the duration of the highs, the larger is the area under the line.</p> Show Code <pre><code>    import numpy as np\n    import pandas as pd\n\n    # Prepare Data\n    df = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/economics.csv\", parse_dates=['date']).head(100)\n    x = np.arange(df.shape[0])\n    y_returns = (df.psavert.diff().fillna(0)/df.psavert.shift(1)).fillna(0) * 100\n\n    # Plot\n    plt.figure(figsize=(16,10), dpi= 80)\n    plt.fill_between(x[1:], y_returns[1:], 0, where=y_returns[1:] &gt;= 0, facecolor='green', interpolate=True, alpha=0.7)\n    plt.fill_between(x[1:], y_returns[1:], 0, where=y_returns[1:] &lt;= 0, facecolor='red', interpolate=True, alpha=0.7)\n\n    # Annotate\n    plt.annotate('Peak \\n1975', xy=(94.0, 21.0), xytext=(88.0, 28),\n                 bbox=dict(boxstyle='square', fc='firebrick'),\n                 arrowprops=dict(facecolor='steelblue', shrink=0.05), fontsize=15, color='white')\n\n\n    # Decorations\n    xtickvals = [str(m)[:3].upper()+\"-\"+str(y) for y,m in zip(df.date.dt.year, df.date.dt.month_name())]\n    plt.gca().set_xticks(x[::6])\n    plt.gca().set_xticklabels(xtickvals[::6], rotation=90, fontdict={'horizontalalignment': 'center', 'verticalalignment': 'center_baseline'})\n    plt.ylim(-35,35)\n    plt.xlim(1,100)\n    plt.title(\"Month Economics Return %\", fontsize=22)\n    plt.ylabel('Monthly returns %')\n    plt.grid(alpha=0.5)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#ranking","title":"Ranking","text":""},{"location":"viz/viz/#ordered-bar-chart","title":"Ordered Bar Chart","text":"<p>Ordered bar chart conveys the rank order of the items effectively. But adding the value of the metric above the chart, the user gets the precise information from the chart itself.</p> Show Code <pre><code>    # Prepare Data\n    df_raw = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n    df = df_raw[['cty', 'manufacturer']].groupby('manufacturer').apply(lambda x: x.mean())\n    df.sort_values('cty', inplace=True)\n    df.reset_index(inplace=True)\n\n    # Draw plot\n    import matplotlib.patches as patches\n\n    fig, ax = plt.subplots(figsize=(16,10), facecolor='white', dpi= 80)\n    ax.vlines(x=df.index, ymin=0, ymax=df.cty, color='firebrick', alpha=0.7, linewidth=20)\n\n    # Annotate Text\n    for i, cty in enumerate(df.cty):\n        ax.text(i, cty+0.5, round(cty, 1), horizontalalignment='center')\n\n\n    # Title, Label, Ticks and Ylim\n    ax.set_title('Bar Chart for Highway Mileage', fontdict={'size':22})\n    ax.set(ylabel='Miles Per Gallon', ylim=(0, 30))\n    plt.xticks(df.index, df.manufacturer.str.upper(), rotation=60, horizontalalignment='right', fontsize=12)\n\n    # Add patches to color the X axis labels\n    p1 = patches.Rectangle((.57, -0.005), width=.33, height=.13, alpha=.1, facecolor='green', transform=fig.transFigure)\n    p2 = patches.Rectangle((.124, -0.005), width=.446, height=.13, alpha=.1, facecolor='red', transform=fig.transFigure)\n    fig.add_artist(p1)\n    fig.add_artist(p2)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#lollipop-chart","title":"Lollipop Chart","text":"<p>Lollipop chart serves a similar purpose as a ordered bar chart in a visually pleasing way.</p> Show Code <pre><code>    # Prepare Data\n    df_raw = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n    df = df_raw[['cty', 'manufacturer']].groupby('manufacturer').apply(lambda x: x.mean())\n    df.sort_values('cty', inplace=True)\n    df.reset_index(inplace=True)\n\n    # Draw plot\n    fig, ax = plt.subplots(figsize=(16,10), dpi= 80)\n    ax.vlines(x=df.index, ymin=0, ymax=df.cty, color='firebrick', alpha=0.7, linewidth=2)\n    ax.scatter(x=df.index, y=df.cty, s=75, color='firebrick', alpha=0.7)\n\n    # Title, Label, Ticks and Ylim\n    ax.set_title('Lollipop Chart for Highway Mileage', fontdict={'size':22})\n    ax.set_ylabel('Miles Per Gallon')\n    ax.set_xticks(df.index)\n    ax.set_xticklabels(df.manufacturer.str.upper(), rotation=60, fontdict={'horizontalalignment': 'right', 'size':12})\n    ax.set_ylim(0, 30)\n\n    # Annotate\n    for row in df.itertuples():\n        ax.text(row.Index, row.cty+.5, s=round(row.cty, 2), horizontalalignment= 'center', verticalalignment='bottom', fontsize=14)\n\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#dot-plot","title":"Dot Plot","text":"<p>The dot plot conveys the rank order of the items. And since it is aligned along the horizontal axis, you can visualize how far the points are from each other more easily.</p> Show Code <pre><code>    # Prepare Data\n    df_raw = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n    df = df_raw[['cty', 'manufacturer']].groupby('manufacturer').apply(lambda x: x.mean())\n    df.sort_values('cty', inplace=True)\n    df.reset_index(inplace=True)\n\n    # Draw plot\n    fig, ax = plt.subplots(figsize=(16,10), dpi= 80)\n    ax.hlines(y=df.index, xmin=11, xmax=26, color='gray', alpha=0.7, linewidth=1, linestyles='dashdot')\n    ax.scatter(y=df.index, x=df.cty, s=75, color='firebrick', alpha=0.7)\n\n    # Title, Label, Ticks and Ylim\n    ax.set_title('Dot Plot for Highway Mileage', fontdict={'size':22})\n    ax.set_xlabel('Miles Per Gallon')\n    ax.set_yticks(df.index)\n    ax.set_yticklabels(df.manufacturer.str.title(), fontdict={'horizontalalignment': 'right'})\n    ax.set_xlim(10, 27)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#slope-chart","title":"Slope Chart","text":"<p>Slope chart is most suitable for comparing the \u2018Before\u2019 and \u2018After\u2019 positions of a given person/item.</p> Show Code <pre><code>    import matplotlib.lines as mlines\n    # Import Data\n    df = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/gdppercap.csv\")\n\n    left_label = [str(c) + ', '+ str(round(y)) for c, y in zip(df.continent, df['1952'])]\n    right_label = [str(c) + ', '+ str(round(y)) for c, y in zip(df.continent, df['1957'])]\n    klass = ['red' if (y1-y2) &lt; 0 else 'green' for y1, y2 in zip(df['1952'], df['1957'])]\n\n    # draw line\n    # https://stackoverflow.com/questions/36470343/how-to-draw-a-line-with-matplotlib/36479941\n    def newline(p1, p2, color='black'):\n        ax = plt.gca()\n        l = mlines.Line2D([p1[0],p2[0]], [p1[1],p2[1]], color='red' if p1[1]-p2[1] &gt; 0 else 'green', marker='o', markersize=6)\n        ax.add_line(l)\n        return l\n\n    fig, ax = plt.subplots(1,1,figsize=(14,14), dpi= 80)\n\n    # Vertical Lines\n    ax.vlines(x=1, ymin=500, ymax=13000, color='black', alpha=0.7, linewidth=1, linestyles='dotted')\n    ax.vlines(x=3, ymin=500, ymax=13000, color='black', alpha=0.7, linewidth=1, linestyles='dotted')\n\n    # Points\n    ax.scatter(y=df['1952'], x=np.repeat(1, df.shape[0]), s=10, color='black', alpha=0.7)\n    ax.scatter(y=df['1957'], x=np.repeat(3, df.shape[0]), s=10, color='black', alpha=0.7)\n\n    # Line Segmentsand Annotation\n    for p1, p2, c in zip(df['1952'], df['1957'], df['continent']):\n        newline([1,p1], [3,p2])\n        ax.text(1-0.05, p1, c + ', ' + str(round(p1)), horizontalalignment='right', verticalalignment='center', fontdict={'size':14})\n        ax.text(3+0.05, p2, c + ', ' + str(round(p2)), horizontalalignment='left', verticalalignment='center', fontdict={'size':14})\n\n    # 'Before' and 'After' Annotations\n    ax.text(1-0.05, 13000, 'BEFORE', horizontalalignment='right', verticalalignment='center', fontdict={'size':18, 'weight':700})\n    ax.text(3+0.05, 13000, 'AFTER', horizontalalignment='left', verticalalignment='center', fontdict={'size':18, 'weight':700})\n\n    # Decoration\n    ax.set_title(\"Slopechart: Comparing GDP Per Capita between 1952 vs 1957\", fontdict={'size':22})\n    ax.set(xlim=(0,4), ylim=(0,14000), ylabel='Mean GDP Per Capita')\n    ax.set_xticks([1,3])\n    ax.set_xticklabels([\"1952\", \"1957\"])\n    plt.yticks(np.arange(500, 13000, 2000), fontsize=12)\n\n    # Lighten borders\n    plt.gca().spines[\"top\"].set_alpha(.0)\n    plt.gca().spines[\"bottom\"].set_alpha(.0)\n    plt.gca().spines[\"right\"].set_alpha(.0)\n    plt.gca().spines[\"left\"].set_alpha(.0)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#dumbbell-plot","title":"Dumbbell Plot","text":"<p>Dumbbell plot conveys the \u2018before\u2019 and \u2018after\u2019 positions of various items along with the rank ordering of the items. Its very useful if you want to visualize the effect of a particular project / initiative on different objects.</p> Show Code <pre><code>    import matplotlib.lines as mlines\n\n    # Import Data\n    df = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/health.csv\")\n    df.sort_values('pct_2014', inplace=True)\n    df.reset_index(inplace=True)\n\n    # Func to draw line segment\n    def newline(p1, p2, color='black'):\n        ax = plt.gca()\n        l = mlines.Line2D([p1[0],p2[0]], [p1[1],p2[1]], color='skyblue')\n        ax.add_line(l)\n        return l\n\n    # Figure and Axes\n    fig, ax = plt.subplots(1,1,figsize=(14,14), facecolor='#f7f7f7', dpi= 80)\n\n    # Vertical Lines\n    ax.vlines(x=.05, ymin=0, ymax=26, color='black', alpha=1, linewidth=1, linestyles='dotted')\n    ax.vlines(x=.10, ymin=0, ymax=26, color='black', alpha=1, linewidth=1, linestyles='dotted')\n    ax.vlines(x=.15, ymin=0, ymax=26, color='black', alpha=1, linewidth=1, linestyles='dotted')\n    ax.vlines(x=.20, ymin=0, ymax=26, color='black', alpha=1, linewidth=1, linestyles='dotted')\n\n    # Points\n    ax.scatter(y=df['index'], x=df['pct_2013'], s=50, color='#0e668b', alpha=0.7)\n    ax.scatter(y=df['index'], x=df['pct_2014'], s=50, color='#a3c4dc', alpha=0.7)\n\n    # Line Segments\n    for i, p1, p2 in zip(df['index'], df['pct_2013'], df['pct_2014']):\n        newline([p1, i], [p2, i])\n\n    # Decoration\n    ax.set_facecolor('#f7f7f7')\n    ax.set_title(\"Dumbell Chart: Pct Change - 2013 vs 2014\", fontdict={'size':22})\n    ax.set(xlim=(0,.25), ylim=(-1, 27), ylabel='Mean GDP Per Capita')\n    ax.set_xticks([.05, .1, .15, .20])\n    ax.set_xticklabels(['5%', '15%', '20%', '25%'])\n    ax.set_xticklabels(['5%', '15%', '20%', '25%'])    \n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#distribution","title":"Distribution","text":""},{"location":"viz/viz/#histogram-for-continuous-variable","title":"Histogram for Continuous Variable","text":"<p>Histogram shows the frequency distribution of a given variable. The below representation groups the frequency bars based on a categorical variable giving a greater insight about the continuous variable and the categorical variable in tandem.</p> Show Code <pre><code>    # Import Data\n    df = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n\n    # Prepare data\n    x_var = 'displ'\n    groupby_var = 'class'\n    df_agg = df.loc[:, [x_var, groupby_var]].groupby(groupby_var)\n    vals = [df[x_var].values.tolist() for i, df in df_agg]\n\n    # Draw\n    plt.figure(figsize=(16,9), dpi= 80)\n    colors = [plt.cm.Spectral(i/float(len(vals)-1)) for i in range(len(vals))]\n    n, bins, patches = plt.hist(vals, 30, stacked=True, density=False, color=colors[:len(vals)])\n\n    # Decoration\n    plt.legend({group:col for group, col in zip(np.unique(df[groupby_var]).tolist(), colors[:len(vals)])})\n    plt.title(f\"Stacked Histogram of ${x_var}$ colored by ${groupby_var}$\", fontsize=22)\n    plt.xlabel(x_var)\n    plt.ylabel(\"Frequency\")\n    plt.ylim(0, 25)\n    plt.xticks(ticks=bins[::3], labels=[round(b,1) for b in bins[::3]])\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#histogram-for-categorical-variable","title":"Histogram for Categorical Variable","text":"<p>The histogram of a categorical variable shows the frequency distribution of a that variable. By coloring the bars, you can visualize the distribution in connection with another categorical variable representing the colors.</p> Show Code <pre><code>    # Import Data\n    df = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n\n    # Prepare data\n    x_var = 'manufacturer'\n    groupby_var = 'class'\n    df_agg = df.loc[:, [x_var, groupby_var]].groupby(groupby_var)\n    vals = [df[x_var].values.tolist() for i, df in df_agg]\n\n    # Draw\n    plt.figure(figsize=(16,9), dpi= 80)\n    colors = [plt.cm.Spectral(i/float(len(vals)-1)) for i in range(len(vals))]\n    n, bins, patches = plt.hist(vals, df[x_var].unique().__len__(), stacked=True, density=False, color=colors[:len(vals)])\n\n    # Decoration\n    plt.legend({group:col for group, col in zip(np.unique(df[groupby_var]).tolist(), colors[:len(vals)])})\n    plt.title(f\"Stacked Histogram of ${x_var}$ colored by ${groupby_var}$\", fontsize=22)\n    plt.xlabel(x_var)\n    plt.ylabel(\"Frequency\")\n    plt.ylim(0, 40)\n    plt.xticks(ticks=bins, labels=np.unique(df[x_var]).tolist(), rotation=90, horizontalalignment='left')\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#density-plot","title":"Density Plot","text":"<p>Density plots are a commonly used tool visualise the distribution of a continuous variable. By grouping them by the \u2018response\u2019 variable, you can inspect the relationship between the X and the Y. The below case if for representational purpose to describe how the distribution of city mileage varies with respect the number of cylinders.</p> Show Code <pre><code>    # Import Data\n    df = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n\n    # Draw Plot\n    plt.figure(figsize=(16,10), dpi= 80)\n    sns.kdeplot(df.loc[df['cyl'] == 4, \"cty\"], shade=True, color=\"g\", label=\"Cyl=4\", alpha=.7)\n    sns.kdeplot(df.loc[df['cyl'] == 5, \"cty\"], shade=True, color=\"deeppink\", label=\"Cyl=5\", alpha=.7)\n    sns.kdeplot(df.loc[df['cyl'] == 6, \"cty\"], shade=True, color=\"dodgerblue\", label=\"Cyl=6\", alpha=.7)\n    sns.kdeplot(df.loc[df['cyl'] == 8, \"cty\"], shade=True, color=\"orange\", label=\"Cyl=8\", alpha=.7)\n\n    # Decoration\n    plt.title('Density Plot of City Mileage by n_Cylinders', fontsize=22)\n    plt.legend()\n    plt.show()\n\ns\n</code></pre> <p></p>"},{"location":"viz/viz/#density-curves-with-histogram","title":"Density Curves with Histogram","text":"<p>Density curve with histogram brings together the collective information conveyed by the two plots so you can have them both in a single figure instead of two.</p> Show Code <pre><code>    # Import Data\n    df = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n\n    # Draw Plot\n    plt.figure(figsize=(13,10), dpi= 80)\n    sns.distplot(df.loc[df['class'] == 'compact', \"cty\"], color=\"dodgerblue\", label=\"Compact\", hist_kws={'alpha':.7}, kde_kws={'linewidth':3})\n    sns.distplot(df.loc[df['class'] == 'suv', \"cty\"], color=\"orange\", label=\"SUV\", hist_kws={'alpha':.7}, kde_kws={'linewidth':3})\n    sns.distplot(df.loc[df['class'] == 'minivan', \"cty\"], color=\"g\", label=\"minivan\", hist_kws={'alpha':.7}, kde_kws={'linewidth':3})\n    plt.ylim(0, 0.35)\n\n    # Decoration\n    plt.title('Density Plot of City Mileage by Vehicle Type', fontsize=22)\n    plt.legend()\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#joy-plot","title":"Joy Plot","text":"<p>Joy Plot allows the density curves of different groups to overlap, it is a great way to visualize the distribution of a larger number of groups in relation to each other. It looks pleasing to the eye and conveys just the right information clearly. It can be easily built using the <code>joypy</code> package which is based on <code>matplotlib</code>.</p> Show Code <pre><code>    # !pip install joypy\n    # Import Data\n    mpg = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n\n    # Draw Plot\n    plt.figure(figsize=(16,10), dpi= 80)\n    fig, axes = joypy.joyplot(mpg, column=['hwy', 'cty'], by=\"class\", ylim='own', figsize=(14,10))\n\n    # Decoration\n    plt.title('Joy Plot of City and Highway Mileage by Class', fontsize=22)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#distributed-dot-plot","title":"Distributed Dot Plot","text":"<p>Distributed dot plot shows the univariate distribution of points segmented by groups. The darker the points, more is the concentration of data points in that region. By coloring the median differently, the real positioning of the groups becomes apparent instantly.</p> Show Code <pre><code>    import matplotlib.patches as mpatches\n\n    # Prepare Data\n    df_raw = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n    cyl_colors = {4:'tab:red', 5:'tab:green', 6:'tab:blue', 8:'tab:orange'}\n    df_raw['cyl_color'] = df_raw.cyl.map(cyl_colors)\n\n    # Mean and Median city mileage by make\n    df = df_raw[['cty', 'manufacturer']].groupby('manufacturer').apply(lambda x: x.mean())\n    df.sort_values('cty', ascending=False, inplace=True)\n    df.reset_index(inplace=True)\n    df_median = df_raw[['cty', 'manufacturer']].groupby('manufacturer').apply(lambda x: x.median())\n\n    # Draw horizontal lines\n    fig, ax = plt.subplots(figsize=(16,10), dpi= 80)\n    ax.hlines(y=df.index, xmin=0, xmax=40, color='gray', alpha=0.5, linewidth=.5, linestyles='dashdot')\n\n    # Draw the Dots\n    for i, make in enumerate(df.manufacturer):\n        df_make = df_raw.loc[df_raw.manufacturer==make, :]\n        ax.scatter(y=np.repeat(i, df_make.shape[0]), x='cty', data=df_make, s=75, edgecolors='gray', c='w', alpha=0.5)\n        ax.scatter(y=i, x='cty', data=df_median.loc[df_median.index==make, :], s=75, c='firebrick')\n\n    # Annotate    \n    ax.text(33, 13, \"$red \\; dots \\; are \\; the \\: median$\", fontdict={'size':12}, color='firebrick')\n\n    # Decorations\n    red_patch = plt.plot([],[], marker=\"o\", ms=10, ls=\"\", mec=None, color='firebrick', label=\"Median\")\n    plt.legend(handles=red_patch)\n    ax.set_title('Distribution of City Mileage by Make', fontdict={'size':22})\n    ax.set_xlabel('Miles Per Gallon (City)', alpha=0.7)\n    ax.set_yticks(df.index)\n    ax.set_yticklabels(df.manufacturer.str.title(), fontdict={'horizontalalignment': 'right'}, alpha=0.7)\n    ax.set_xlim(1, 40)\n    plt.xticks(alpha=0.7)\n    plt.gca().spines[\"top\"].set_visible(False)    \n    plt.gca().spines[\"bottom\"].set_visible(False)    \n    plt.gca().spines[\"right\"].set_visible(False)    \n    plt.gca().spines[\"left\"].set_visible(False)   \n    plt.grid(axis='both', alpha=.4, linewidth=.1)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#box-plot","title":"Box Plot","text":"<p>Box plots are a great way to visualize the distribution, keeping the median, 25th 75th quartiles and the outliers in mind. However, you need to be careful about interpreting the size the boxes which can potentially distort the number of points contained within that group. So, manually providing the number of observations in each box can help overcome this drawback.</p> <p>For example, the first two boxes on the left have boxes of the same size even though they have 5 and 47 obs respectively. So writing the number of observations in that group becomes necessary.</p> Show Code <pre><code>    # Import Data\n    df = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n\n    # Draw Plot\n    plt.figure(figsize=(13,10), dpi= 80)\n    sns.boxplot(x='class', y='hwy', data=df, notch=False)\n\n    # Add N Obs inside boxplot (optional)\n    def add_n_obs(df,group_col,y):\n        medians_dict = {grp[0]:grp[1][y].median() for grp in df.groupby(group_col)}\n        xticklabels = [x.get_text() for x in plt.gca().get_xticklabels()]\n        n_obs = df.groupby(group_col)[y].size().values\n        for (x, xticklabel), n_ob in zip(enumerate(xticklabels), n_obs):\n            plt.text(x, medians_dict[xticklabel]*1.01, \"#obs : \"+str(n_ob), horizontalalignment='center', fontdict={'size':14}, color='white')\n\n    add_n_obs(df,group_col='class',y='hwy')    \n\n    # Decoration\n    plt.title('Box Plot of Highway Mileage by Vehicle Class', fontsize=22)\n    plt.ylim(10, 40)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#dot-box-plot","title":"Dot + Box Plot","text":"<p>Dot + Box plot Conveys similar information as a boxplot split in groups. The dots, in addition, gives a sense of how many data points lie within each group.</p> Show Code <pre><code>    # Import Data\n    df = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n\n    # Draw Plot\n    plt.figure(figsize=(13,10), dpi= 80)\n    sns.boxplot(x='class', y='hwy', data=df, hue='cyl')\n    sns.stripplot(x='class', y='hwy', data=df, color='black', size=3, jitter=1)\n\n    for i in range(len(df['class'].unique())-1):\n        plt.vlines(i+.5, 10, 45, linestyles='solid', colors='gray', alpha=0.2)\n\n    # Decoration\n    plt.title('Box Plot of Highway Mileage by Vehicle Class', fontsize=22)\n    plt.legend(title='Cylinders')\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#violin-plot","title":"Violin Plot","text":"<p>Violin plot is a visually pleasing alternative to box plots. The shape or area of the violin depends on the number of observations it holds. However, the violin plots can be harder to read and it not commonly used in professional settings.</p> Show Code <pre><code>    # Import Data\n    df = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n\n    # Draw Plot\n    plt.figure(figsize=(13,10), dpi= 80)\n    sns.violinplot(x='class', y='hwy', data=df, scale='width', inner='quartile')\n\n    # Decoration\n    plt.title('Violin Plot of Highway Mileage by Vehicle Class', fontsize=22)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#population-pyramid","title":"Population Pyramid","text":"<p>Population pyramid can be used to show either the distribution of the groups ordered by the volumne. Or it can also be used to show the stage-by-stage filtering of the population as it is used below to show how many people pass through each stage of a marketing funnel.</p> Show Code <pre><code>    # Read data\n    df = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/email_campaign_funnel.csv\")\n\n    # Draw Plot\n    plt.figure(figsize=(13,10), dpi= 80)\n    group_col = 'Gender'\n    order_of_bars = df.Stage.unique()[::-1]\n    colors = [plt.cm.Spectral(i/float(len(df[group_col].unique())-1)) for i in range(len(df[group_col].unique()))]\n\n    for c, group in zip(colors, df[group_col].unique()):\n        sns.barplot(x='Users', y='Stage', data=df.loc[df[group_col]==group, :], order=order_of_bars, color=c, label=group)\n\n    # Decorations    \n    plt.xlabel(\"$Users$\")\n    plt.ylabel(\"Stage of Purchase\")\n    plt.yticks(fontsize=12)\n    plt.title(\"Population Pyramid of the Marketing Funnel\", fontsize=22)\n    plt.legend()\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#categorical-plots","title":"Categorical Plots","text":"<p>Categorical plots provided by the <code>seaborn</code> library can be used to visualize the counts distribution of 2 ore more categorical variables in relation to each other.</p> Show Code <pre><code>    # Load Dataset\n    titanic = sns.load_dataset(\"titanic\")\n\n    # Plot\n    g = sns.catplot(\"alive\", col=\"deck\", col_wrap=4,\n                    data=titanic[titanic.deck.notnull()],\n                    kind=\"count\", height=3.5, aspect=.8, \n                    palette='tab20')\n\n    fig.suptitle('sf')\n    plt.show()\n</code></pre> <p></p> Show Code <pre><code>    # Load Dataset\n    titanic = sns.load_dataset(\"titanic\")\n\n    # Plot\n    sns.catplot(x=\"age\", y=\"embark_town\",\n                hue=\"sex\", col=\"class\",\n                data=titanic[titanic.embark_town.notnull()],\n                orient=\"h\", height=5, aspect=1, palette=\"tab10\",\n                kind=\"violin\", dodge=True, cut=0, bw=.2)\n</code></pre> <p></p>"},{"location":"viz/viz/#composition","title":"Composition","text":""},{"location":"viz/viz/#waffle-chart","title":"Waffle Chart","text":"<p>The <code>waffle</code> chart can be created using the <code>pywaffle</code> package and is used to show the compositions of groups in a larger population.</p> Show Code <pre><code>    #! pip install pywaffle\n    # Reference: https://stackoverflow.com/questions/41400136/how-to-do-waffle-charts-in-python-square-piechart\n    from pywaffle import Waffle\n\n    # Import\n    df_raw = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n\n    # Prepare Data\n    df = df_raw.groupby('class').size().reset_index(name='counts')\n    n_categories = df.shape[0]\n    colors = [plt.cm.inferno_r(i/float(n_categories)) for i in range(n_categories)]\n\n    # Draw Plot and Decorate\n    fig = plt.figure(\n        FigureClass=Waffle,\n        plots={\n            '111': {\n                'values': df['counts'],\n                'labels': [\"{0} ({1})\".format(n[0], n[1]) for n in df[['class', 'counts']].itertuples()],\n                'legend': {'loc': 'upper left', 'bbox_to_anchor': (1.05, 1), 'fontsize': 12},\n                'title': {'label': '# Vehicles by Class', 'loc': 'center', 'fontsize':18}\n            },\n        },\n        rows=7,\n        colors=colors,\n        figsize=(16, 9)\n    )\n</code></pre> <p></p> Show Code <pre><code>    #! pip install pywaffle\n    from pywaffle import Waffle\n\n    # Import\n    # df_raw = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n\n    # Prepare Data\n    # By Class Data\n    df_class = df_raw.groupby('class').size().reset_index(name='counts_class')\n    n_categories = df_class.shape[0]\n    colors_class = [plt.cm.Set3(i/float(n_categories)) for i in range(n_categories)]\n\n    # By Cylinders Data\n    df_cyl = df_raw.groupby('cyl').size().reset_index(name='counts_cyl')\n    n_categories = df_cyl.shape[0]\n    colors_cyl = [plt.cm.Spectral(i/float(n_categories)) for i in range(n_categories)]\n\n    # By Make Data\n    df_make = df_raw.groupby('manufacturer').size().reset_index(name='counts_make')\n    n_categories = df_make.shape[0]\n    colors_make = [plt.cm.tab20b(i/float(n_categories)) for i in range(n_categories)]\n\n\n    # Draw Plot and Decorate\n    fig = plt.figure(\n        FigureClass=Waffle,\n        plots={\n            '311': {\n                'values': df_class['counts_class'],\n                'labels': [\"{1}\".format(n[0], n[1]) for n in df_class[['class', 'counts_class']].itertuples()],\n                'legend': {'loc': 'upper left', 'bbox_to_anchor': (1.05, 1), 'fontsize': 12, 'title':'Class'},\n                'title': {'label': '# Vehicles by Class', 'loc': 'center', 'fontsize':18},\n                'colors': colors_class\n            },\n            '312': {\n                'values': df_cyl['counts_cyl'],\n                'labels': [\"{1}\".format(n[0], n[1]) for n in df_cyl[['cyl', 'counts_cyl']].itertuples()],\n                'legend': {'loc': 'upper left', 'bbox_to_anchor': (1.05, 1), 'fontsize': 12, 'title':'Cyl'},\n                'title': {'label': '# Vehicles by Cyl', 'loc': 'center', 'fontsize':18},\n                'colors': colors_cyl\n            },\n            '313': {\n                'values': df_make['counts_make'],\n                'labels': [\"{1}\".format(n[0], n[1]) for n in df_make[['manufacturer', 'counts_make']].itertuples()],\n                'legend': {'loc': 'upper left', 'bbox_to_anchor': (1.05, 1), 'fontsize': 12, 'title':'Manufacturer'},\n                'title': {'label': '# Vehicles by Make', 'loc': 'center', 'fontsize':18},\n                'colors': colors_make\n            }\n        },\n        rows=9,\n        figsize=(16, 14)\n    )\n</code></pre> <p></p>"},{"location":"viz/viz/#pie-chart","title":"Pie Chart","text":"<p>Pie chart is a classic way to show the composition of groups. However, its not generally advisable to use nowadays because the area of the pie portions can sometimes become misleading. So, if you are to use pie chart, its highly recommended to explicitly write down the percentage or numbers for each portion of the pie.</p> Show Code <pre><code>    # Import\n    df_raw = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n\n    # Prepare Data\n    df = df_raw.groupby('class').size()\n\n    # Make the plot with pandas\n    df.plot(kind='pie', subplots=True, figsize=(8, 8), dpi= 80)\n    plt.title(\"Pie Chart of Vehicle Class - Bad\")\n    plt.ylabel(\"\")\n    plt.show()\n</code></pre> <p></p> Show Code <pre><code>    # Import\n    df_raw = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n\n    # Prepare Data\n    df = df_raw.groupby('class').size().reset_index(name='counts')\n\n    # Draw Plot\n    fig, ax = plt.subplots(figsize=(12, 7), subplot_kw=dict(aspect=\"equal\"), dpi= 80)\n\n    data = df['counts']\n    categories = df['class']\n    explode = [0,0,0,0,0,0.1,0]\n\n    def func(pct, allvals):\n        absolute = int(pct/100.*np.sum(allvals))\n        return \"{:.1f}% ({:d} )\".format(pct, absolute)\n\n    wedges, texts, autotexts = ax.pie(data, \n                                      autopct=lambda pct: func(pct, data),\n                                      textprops=dict(color=\"w\"), \n                                      colors=plt.cm.Dark2.colors,\n                                     startangle=140,\n                                     explode=explode)\n\n    # Decoration\n    ax.legend(wedges, categories, title=\"Vehicle Class\", loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n    plt.setp(autotexts, size=10, weight=700)\n    ax.set_title(\"Class of Vehicles: Pie Chart\")\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#treemap","title":"Treemap","text":"<p>Tree map is similar to a pie chart and it does a better work without misleading the contributions by each group.</p> Show Code <pre><code>    # pip install squarify\n    import squarify \n\n    # Import Data\n    df_raw = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n\n    # Prepare Data\n    df = df_raw.groupby('class').size().reset_index(name='counts')\n    labels = df.apply(lambda x: str(x[0]) + \"\\n (\" + str(x[1]) + \")\", axis=1)\n    sizes = df['counts'].values.tolist()\n    colors = [plt.cm.Spectral(i/float(len(labels))) for i in range(len(labels))]\n\n    # Draw Plot\n    plt.figure(figsize=(12,8), dpi= 80)\n    squarify.plot(sizes=sizes, label=labels, color=colors, alpha=.8)\n\n    # Decorate\n    plt.title('Treemap of Vechile Class')\n    plt.axis('off')\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#bar-chart","title":"Bar Chart","text":"<p>Bar chart is a classic way of visualizing items based on counts or any given metric. In below chart, I have used a different color for each item, but you might typically want to pick one color for all items unless you to color them by groups. The color names get stored inside <code>all_colors</code> in the code below. You can change the color of the bars by setting the <code>color</code> parameter in <code>plt.plot()</code>.</p> Show Code <pre><code>    import random\n\n    # Import Data\n    df_raw = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n\n    # Prepare Data\n    df = df_raw.groupby('manufacturer').size().reset_index(name='counts')\n    n = df['manufacturer'].unique().__len__()+1\n    all_colors = list(plt.cm.colors.cnames.keys())\n    random.seed(100)\n    c = random.choices(all_colors, k=n)\n\n    # Plot Bars\n    plt.figure(figsize=(16,10), dpi= 80)\n    plt.bar(df['manufacturer'], df['counts'], color=c, width=.5)\n    for i, val in enumerate(df['counts'].values):\n        plt.text(i, val, float(val), horizontalalignment='center', verticalalignment='bottom', fontdict={'fontweight':500, 'size':12})\n\n    # Decoration\n    plt.gca().set_xticklabels(df['manufacturer'], rotation=60, horizontalalignment= 'right')\n    plt.title(\"Number of Vehicles by Manaufacturers\", fontsize=22)\n    plt.ylabel('# Vehicles')\n    plt.ylim(0, 45)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#change","title":"Change","text":""},{"location":"viz/viz/#time-series-plot","title":"Time Series Plot","text":"<p>Time series plot is used to visualise how a given metric changes over time. Here you can see how the Air Passenger traffic changed between 1949 and 1969.</p> Show Code <pre><code>    # Import Data\n    df = pd.read_csv('https://github.com/selva86/datasets/raw/master/AirPassengers.csv')\n\n    # Draw Plot\n    plt.figure(figsize=(16,10), dpi= 80)\n    plt.plot('date', 'traffic', data=df, color='tab:red')\n\n    # Decoration\n    plt.ylim(50, 750)\n    xtick_location = df.index.tolist()[::12]\n    xtick_labels = [x[-4:] for x in df.date.tolist()[::12]]\n    plt.xticks(ticks=xtick_location, labels=xtick_labels, rotation=0, fontsize=12, horizontalalignment='center', alpha=.7)\n    plt.yticks(fontsize=12, alpha=.7)\n    plt.title(\"Air Passengers Traffic (1949 - 1969)\", fontsize=22)\n    plt.grid(axis='both', alpha=.3)\n\n    # Remove borders\n    plt.gca().spines[\"top\"].set_alpha(0.0)    \n    plt.gca().spines[\"bottom\"].set_alpha(0.3)\n    plt.gca().spines[\"right\"].set_alpha(0.0)    \n    plt.gca().spines[\"left\"].set_alpha(0.3)   \n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#time-series-with-peaks-and-troughs-annotated","title":"Time Series with Peaks and Troughs Annotated","text":"<p>The below time series plots all the the peaks and troughs and annotates the occurence of selected special events.</p> Show Code <pre><code>    # Import Data\n    df = pd.read_csv('https://github.com/selva86/datasets/raw/master/AirPassengers.csv')\n\n    # Get the Peaks and Troughs\n    data = df['traffic'].values\n    doublediff = np.diff(np.sign(np.diff(data)))\n    peak_locations = np.where(doublediff == -2)[0] + 1\n\n    doublediff2 = np.diff(np.sign(np.diff(-1*data)))\n    trough_locations = np.where(doublediff2 == -2)[0] + 1\n\n    # Draw Plot\n    plt.figure(figsize=(16,10), dpi= 80)\n    plt.plot('date', 'traffic', data=df, color='tab:blue', label='Air Traffic')\n    plt.scatter(df.date[peak_locations], df.traffic[peak_locations], marker=mpl.markers.CARETUPBASE, color='tab:green', s=100, label='Peaks')\n    plt.scatter(df.date[trough_locations], df.traffic[trough_locations], marker=mpl.markers.CARETDOWNBASE, color='tab:red', s=100, label='Troughs')\n\n    # Annotate\n    for t, p in zip(trough_locations[1::5], peak_locations[::3]):\n        plt.text(df.date[p], df.traffic[p]+15, df.date[p], horizontalalignment='center', color='darkgreen')\n        plt.text(df.date[t], df.traffic[t]-35, df.date[t], horizontalalignment='center', color='darkred')\n\n    # Decoration\n    plt.ylim(50,750)\n    xtick_location = df.index.tolist()[::6]\n    xtick_labels = df.date.tolist()[::6]\n    plt.xticks(ticks=xtick_location, labels=xtick_labels, rotation=90, fontsize=12, alpha=.7)\n    plt.title(\"Peak and Troughs of Air Passengers Traffic (1949 - 1969)\", fontsize=22)\n    plt.yticks(fontsize=12, alpha=.7)\n\n    # Lighten borders\n    plt.gca().spines[\"top\"].set_alpha(.0)\n    plt.gca().spines[\"bottom\"].set_alpha(.3)\n    plt.gca().spines[\"right\"].set_alpha(.0)\n    plt.gca().spines[\"left\"].set_alpha(.3)\n\n    plt.legend(loc='upper left')\n    plt.grid(axis='y', alpha=.3)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#autocorrelation-acf-and-partial-autocorrelation-pacf-plot","title":"Autocorrelation (ACF) and Partial Autocorrelation (PACF) Plot","text":"<p>The ACF plot shows the correlation of the time series with its own lags. Each vertical line (on the autocorrelation plot) represents the correlation between the series and its lag starting from lag 0. The blue shaded region in the plot is the significance level. Those lags that lie above the blue line are the significant lags.</p> <p>So how to interpret this?</p> <p>For AirPassengers, we see upto 14 lags have crossed the blue line and so are significant. This means, the Air Passengers traffic seen upto 14 years back has an influence on the traffic seen today.</p> <p>PACF on the other had shows the autocorrelation of any given lag (of time series) against the current series, but with the contributions of the lags-inbetween removed.</p> Show Code <pre><code>    from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n    # Import Data\n    df = pd.read_csv('https://github.com/selva86/datasets/raw/master/AirPassengers.csv')\n\n    # Draw Plot\n    fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(16,6), dpi= 80)\n    plot_acf(df.traffic.tolist(), ax=ax1, lags=50)\n    plot_pacf(df.traffic.tolist(), ax=ax2, lags=20)\n\n    # Decorate\n    # lighten the borders\n    ax1.spines[\"top\"].set_alpha(.3); ax2.spines[\"top\"].set_alpha(.3)\n    ax1.spines[\"bottom\"].set_alpha(.3); ax2.spines[\"bottom\"].set_alpha(.3)\n    ax1.spines[\"right\"].set_alpha(.3); ax2.spines[\"right\"].set_alpha(.3)\n    ax1.spines[\"left\"].set_alpha(.3); ax2.spines[\"left\"].set_alpha(.3)\n\n    # font size of tick labels\n    ax1.tick_params(axis='both', labelsize=12)\n    ax2.tick_params(axis='both', labelsize=12)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#cross-correlation-plot","title":"Cross Correlation plot","text":"<p>Cross correlation plot shows the lags of two time series with each other.</p> Show Code <pre><code>    import statsmodels.tsa.stattools as stattools\n\n    # Import Data\n    df = pd.read_csv('https://github.com/selva86/datasets/raw/master/mortality.csv')\n    x = df['mdeaths']\n    y = df['fdeaths']\n\n    # Compute Cross Correlations\n    ccs = stattools.ccf(x, y)[:100]\n    nlags = len(ccs)\n\n    # Compute the Significance level\n    # ref: https://stats.stackexchange.com/questions/3115/cross-correlation-significance-in-r/3128#3128\n    conf_level = 2 / np.sqrt(nlags)\n\n    # Draw Plot\n    plt.figure(figsize=(12,7), dpi= 80)\n\n    plt.hlines(0, xmin=0, xmax=100, color='gray')  # 0 axis\n    plt.hlines(conf_level, xmin=0, xmax=100, color='gray')\n    plt.hlines(-conf_level, xmin=0, xmax=100, color='gray')\n\n    plt.bar(x=np.arange(len(ccs)), height=ccs, width=.3)\n\n    # Decoration\n    plt.title('$Cross\\; Correlation\\; Plot:\\; mdeaths\\; vs\\; fdeaths$', fontsize=22)\n    plt.xlim(0,len(ccs))\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#time-series-decomposition-plot","title":"Time Series Decomposition Plot","text":"<p>Time series decomposition plot shows the break down of the time series into trend, seasonal and residual components.</p> Show Code <pre><code>    from statsmodels.tsa.seasonal import seasonal_decompose\n    from dateutil.parser import parse\n\n    # Import Data\n    df = pd.read_csv('https://github.com/selva86/datasets/raw/master/AirPassengers.csv')\n    dates = pd.DatetimeIndex([parse(d).strftime('%Y-%m-01') for d in df['date']])\n    df.set_index(dates, inplace=True)\n\n    # Decompose \n    result = seasonal_decompose(df['traffic'], model='multiplicative')\n\n    # Plot\n    plt.rcParams.update({'figure.figsize': (10,10)})\n    result.plot().suptitle('Time Series Decomposition of Air Passengers')\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#multiple-time-series","title":"Multiple Time Series","text":"<p>You can plot multiple time series that measures the same value on the same chart as shown below.</p> Show Code <pre><code>    # Import Data\n    df = pd.read_csv('https://github.com/selva86/datasets/raw/master/mortality.csv')\n\n    # Define the upper limit, lower limit, interval of Y axis and colors\n    y_LL = 100\n    y_UL = int(df.iloc[:, 1:].max().max()*1.1)\n    y_interval = 400\n    mycolors = ['tab:red', 'tab:blue', 'tab:green', 'tab:orange']    \n\n    # Draw Plot and Annotate\n    fig, ax = plt.subplots(1,1,figsize=(16, 9), dpi= 80)    \n\n    columns = df.columns[1:]  \n    for i, column in enumerate(columns):    \n        plt.plot(df.date.values, df[column].values, lw=1.5, color=mycolors[i])    \n        plt.text(df.shape[0]+1, df[column].values[-1], column, fontsize=14, color=mycolors[i])\n\n    # Draw Tick lines  \n    for y in range(y_LL, y_UL, y_interval):    \n        plt.hlines(y, xmin=0, xmax=71, colors='black', alpha=0.3, linestyles=\"--\", lw=0.5)\n\n    # Decorations    \n    plt.tick_params(axis=\"both\", which=\"both\", bottom=False, top=False,    \n                    labelbottom=True, left=False, right=False, labelleft=True)        \n\n    # Lighten borders\n    plt.gca().spines[\"top\"].set_alpha(.3)\n    plt.gca().spines[\"bottom\"].set_alpha(.3)\n    plt.gca().spines[\"right\"].set_alpha(.3)\n    plt.gca().spines[\"left\"].set_alpha(.3)\n\n    plt.title('Number of Deaths from Lung Diseases in the UK (1974-1979)', fontsize=22)\n    plt.yticks(range(y_LL, y_UL, y_interval), [str(y) for y in range(y_LL, y_UL, y_interval)], fontsize=12)    \n    plt.xticks(range(0, df.shape[0], 12), df.date.values[::12], horizontalalignment='left', fontsize=12)    \n    plt.ylim(y_LL, y_UL)    \n    plt.xlim(-2, 80)    \n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#plotting-with-different-scales-using-secondary-y-axis","title":"Plotting with different scales using secondary Y axis","text":"<p>If you want to show two time series that measures two different quantities at the same point in time, you can plot the second series againt the secondary Y axis on the right.</p> Show Code <pre><code>    # Import Data\n    df = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/economics.csv\")\n\n    x = df['date']\n    y1 = df['psavert']\n    y2 = df['unemploy']\n\n    # Plot Line1 (Left Y Axis)\n    fig, ax1 = plt.subplots(1,1,figsize=(16,9), dpi= 80)\n    ax1.plot(x, y1, color='tab:red')\n\n    # Plot Line2 (Right Y Axis)\n    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n    ax2.plot(x, y2, color='tab:blue')\n\n    # Decorations\n    # ax1 (left Y axis)\n    ax1.set_xlabel('Year', fontsize=20)\n    ax1.tick_params(axis='x', rotation=0, labelsize=12)\n    ax1.set_ylabel('Personal Savings Rate', color='tab:red', fontsize=20)\n    ax1.tick_params(axis='y', rotation=0, labelcolor='tab:red' )\n    ax1.grid(alpha=.4)\n\n    # ax2 (right Y axis)\n    ax2.set_ylabel(\"# Unemployed (1000's)\", color='tab:blue', fontsize=20)\n    ax2.tick_params(axis='y', labelcolor='tab:blue')\n    ax2.set_xticks(np.arange(0, len(x), 60))\n    ax2.set_xticklabels(x[::60], rotation=90, fontdict={'fontsize':10})\n    ax2.set_title(\"Personal Savings Rate vs Unemployed: Plotting in Secondary Y Axis\", fontsize=22)\n    fig.tight_layout()\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#time-series-with-error-bands","title":"Time Series with Error Bands","text":"<p>Time series with error bands can be constructed if you have a time series dataset with multiple observations for each time point (date / timestamp). Below you can see a couple of examples based on the orders coming in at various times of the day. And another example on the number of orders arriving over a duration of 45 days.</p> <p>In this approach, the mean of the number of orders is denoted by the white line. And a 95% confidence bands are computed and drawn around the mean.</p> Show Code <pre><code>    from scipy.stats import sem\n\n    # Import Data\n    df = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/user_orders_hourofday.csv\")\n    df_mean = df.groupby('order_hour_of_day').quantity.mean()\n    df_se = df.groupby('order_hour_of_day').quantity.apply(sem).mul(1.96)\n\n    # Plot\n    plt.figure(figsize=(16,10), dpi= 80)\n    plt.ylabel(\"# Orders\", fontsize=16)  \n    x = df_mean.index\n    plt.plot(x, df_mean, color=\"white\", lw=2) \n    plt.fill_between(x, df_mean - df_se, df_mean + df_se, color=\"#3F5D7D\")  \n\n    # Decorations\n    # Lighten borders\n    plt.gca().spines[\"top\"].set_alpha(0)\n    plt.gca().spines[\"bottom\"].set_alpha(1)\n    plt.gca().spines[\"right\"].set_alpha(0)\n    plt.gca().spines[\"left\"].set_alpha(1)\n    plt.xticks(x[::2], [str(d) for d in x[::2]] , fontsize=12)\n    plt.title(\"User Orders by Hour of Day (95% confidence)\", fontsize=22)\n    plt.xlabel(\"Hour of Day\")\n\n    s, e = plt.gca().get_xlim()\n    plt.xlim(s, e)\n\n    # Draw Horizontal Tick lines  \n    for y in range(8, 20, 2):    \n        plt.hlines(y, xmin=s, xmax=e, colors='black', alpha=0.5, linestyles=\"--\", lw=0.5)\n\n    plt.show()\n</code></pre> <p></p> Show Code <pre><code>    \"Data Source: https://www.kaggle.com/olistbr/brazilian-ecommerce#olist_orders_dataset.csv\"\n    from dateutil.parser import parse\n    from scipy.stats import sem\n\n    # Import Data\n    df_raw = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/orders_45d.csv', \n                         parse_dates=['purchase_time', 'purchase_date'])\n\n    # Prepare Data: Daily Mean and SE Bands\n    df_mean = df_raw.groupby('purchase_date').quantity.mean()\n    df_se = df_raw.groupby('purchase_date').quantity.apply(sem).mul(1.96)\n\n    # Plot\n    plt.figure(figsize=(16,10), dpi= 80)\n    plt.ylabel(\"# Daily Orders\", fontsize=16)  \n    x = [d.date().strftime('%Y-%m-%d') for d in df_mean.index]\n    plt.plot(x, df_mean, color=\"white\", lw=2) \n    plt.fill_between(x, df_mean - df_se, df_mean + df_se, color=\"#3F5D7D\")  \n\n    # Decorations\n    # Lighten borders\n    plt.gca().spines[\"top\"].set_alpha(0)\n    plt.gca().spines[\"bottom\"].set_alpha(1)\n    plt.gca().spines[\"right\"].set_alpha(0)\n    plt.gca().spines[\"left\"].set_alpha(1)\n    plt.xticks(x[::6], [str(d) for d in x[::6]] , fontsize=12)\n    plt.title(\"Daily Order Quantity of Brazilian Retail with Error Bands (95% confidence)\", fontsize=20)\n\n    # Axis limits\n    s, e = plt.gca().get_xlim()\n    plt.xlim(s, e-2)\n    plt.ylim(4, 10)\n\n    # Draw Horizontal Tick lines  \n    for y in range(5, 10, 1):    \n        plt.hlines(y, xmin=s, xmax=e, colors='black', alpha=0.5, linestyles=\"--\", lw=0.5)\n\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#stacked-area-chart","title":"Stacked Area Chart","text":"<p>Stacked area chart gives an visual representation of the extent of contribution from multiple time series so that it is easy to compare against each other.</p> Show Code <pre><code>    # Import Data\n    df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/nightvisitors.csv')\n\n    # Decide Colors \n    mycolors = ['tab:red', 'tab:blue', 'tab:green', 'tab:orange', 'tab:brown', 'tab:grey', 'tab:pink', 'tab:olive']      \n\n    # Draw Plot and Annotate\n    fig, ax = plt.subplots(1,1,figsize=(16, 9), dpi= 80)\n    columns = df.columns[1:]\n    labs = columns.values.tolist()\n\n    # Prepare data\n    x  = df['yearmon'].values.tolist()\n    y0 = df[columns[0]].values.tolist()\n    y1 = df[columns[1]].values.tolist()\n    y2 = df[columns[2]].values.tolist()\n    y3 = df[columns[3]].values.tolist()\n    y4 = df[columns[4]].values.tolist()\n    y5 = df[columns[5]].values.tolist()\n    y6 = df[columns[6]].values.tolist()\n    y7 = df[columns[7]].values.tolist()\n    y = np.vstack([y0, y2, y4, y6, y7, y5, y1, y3])\n\n    # Plot for each column\n    labs = columns.values.tolist()\n    ax = plt.gca()\n    ax.stackplot(x, y, labels=labs, colors=mycolors, alpha=0.8)\n\n    # Decorations\n    ax.set_title('Night Visitors in Australian Regions', fontsize=18)\n    ax.set(ylim=[0, 100000])\n    ax.legend(fontsize=10, ncol=4)\n    plt.xticks(x[::5], fontsize=10, horizontalalignment='center')\n    plt.yticks(np.arange(10000, 100000, 20000), fontsize=10)\n    plt.xlim(x[0], x[-1])\n\n    # Lighten borders\n    plt.gca().spines[\"top\"].set_alpha(0)\n    plt.gca().spines[\"bottom\"].set_alpha(.3)\n    plt.gca().spines[\"right\"].set_alpha(0)\n    plt.gca().spines[\"left\"].set_alpha(.3)\n\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#area-chart-unstacked","title":"Area Chart UnStacked","text":"<p>An unstacked area chart is used to visualize the progress (ups and downs) of two or more series with respect to each other. In the chart below, you can clearly see how the personal savings rate comes down as the median duration of unemployment increases. The unstacked area chart brings out this phenomenon nicely.</p> Show Code <pre><code>    # Import Data\n    df = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/economics.csv\")\n\n    # Prepare Data\n    x = df['date'].values.tolist()\n    y1 = df['psavert'].values.tolist()\n    y2 = df['uempmed'].values.tolist()\n    mycolors = ['tab:red', 'tab:blue', 'tab:green', 'tab:orange', 'tab:brown', 'tab:grey', 'tab:pink', 'tab:olive']      \n    columns = ['psavert', 'uempmed']\n\n    # Draw Plot \n    fig, ax = plt.subplots(1, 1, figsize=(16,9), dpi= 80)\n    ax.fill_between(x, y1=y1, y2=0, label=columns[1], alpha=0.5, color=mycolors[1], linewidth=2)\n    ax.fill_between(x, y1=y2, y2=0, label=columns[0], alpha=0.5, color=mycolors[0], linewidth=2)\n\n    # Decorations\n    ax.set_title('Personal Savings Rate vs Median Duration of Unemployment', fontsize=18)\n    ax.set(ylim=[0, 30])\n    ax.legend(loc='best', fontsize=12)\n    plt.xticks(x[::50], fontsize=10, horizontalalignment='center')\n    plt.yticks(np.arange(2.5, 30.0, 2.5), fontsize=10)\n    plt.xlim(-10, x[-1])\n\n    # Draw Tick lines  \n    for y in np.arange(2.5, 30.0, 2.5):    \n        plt.hlines(y, xmin=0, xmax=len(x), colors='black', alpha=0.3, linestyles=\"--\", lw=0.5)\n\n    # Lighten borders\n    plt.gca().spines[\"top\"].set_alpha(0)\n    plt.gca().spines[\"bottom\"].set_alpha(.3)\n    plt.gca().spines[\"right\"].set_alpha(0)\n    plt.gca().spines[\"left\"].set_alpha(.3)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#calendar-heat-map","title":"Calendar Heat Map","text":"<p>Calendar map is an alternate and a less preferred option to visualise time based data compared to a time series. Though can be visually appealing, the numeric values are not quite evident. It is however effective in picturising the extreme values and holiday effects nicely.</p> Show Code <pre><code>    import matplotlib as mpl\n    import calmap\n\n    # Import Data\n    df = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/yahoo.csv\", parse_dates=['date'])\n    df.set_index('date', inplace=True)\n\n    # Plot\n    plt.figure(figsize=(16,10), dpi= 80)\n    calmap.calendarplot(df['2014']['VIX.Close'], fig_kws={'figsize': (16,10)}, yearlabel_kws={'color':'black', 'fontsize':14}, subplot_kws={'title':'Yahoo Stock Prices'})\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#seasonal-plot","title":"Seasonal Plot","text":"<p>The seasonal plot can be used to compare how the time series performed at same day in the previous season (year / month / week etc).</p> Show Code <pre><code>    from dateutil.parser import parse \n\n    # Import Data\n    df = pd.read_csv('https://github.com/selva86/datasets/raw/master/AirPassengers.csv')\n\n    # Prepare data\n    df['year'] = [parse(d).year for d in df.date]\n    df['month'] = [parse(d).strftime('%b') for d in df.date]\n    years = df['year'].unique()\n\n    # Draw Plot\n    mycolors = ['tab:red', 'tab:blue', 'tab:green', 'tab:orange', 'tab:brown', 'tab:grey', 'tab:pink', 'tab:olive', 'deeppink', 'steelblue', 'firebrick', 'mediumseagreen']      \n    plt.figure(figsize=(16,10), dpi= 80)\n\n    for i, y in enumerate(years):\n        plt.plot('month', 'traffic', data=df.loc[df.year==y, :], color=mycolors[i], label=y)\n        plt.text(df.loc[df.year==y, :].shape[0]-.9, df.loc[df.year==y, 'traffic'][-1:].values[0], y, fontsize=12, color=mycolors[i])\n\n    # Decoration\n    plt.ylim(50,750)\n    plt.xlim(-0.3, 11)\n    plt.ylabel('$Air Traffic$')\n    plt.yticks(fontsize=12, alpha=.7)\n    plt.title(\"Monthly Seasonal Plot: Air Passengers Traffic (1949 - 1969)\", fontsize=22)\n    plt.grid(axis='y', alpha=.3)\n\n    # Remove borders\n    plt.gca().spines[\"top\"].set_alpha(0.0)    \n    plt.gca().spines[\"bottom\"].set_alpha(0.5)\n    plt.gca().spines[\"right\"].set_alpha(0.0)    \n    plt.gca().spines[\"left\"].set_alpha(0.5)   \n    # plt.legend(loc='upper right', ncol=2, fontsize=12)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#groups","title":"Groups","text":""},{"location":"viz/viz/#dendrogram","title":"Dendrogram","text":"<p>A Dendrogram groups similar points together based on a given distance metric and organizes them in tree like links based on the point\u2019s similarity.</p> Show Code <pre><code>    import scipy.cluster.hierarchy as shc\n\n    # Import Data\n    df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/USArrests.csv')\n\n    # Plot\n    plt.figure(figsize=(16, 10), dpi= 80)  \n    plt.title(\"USArrests Dendograms\", fontsize=22)  \n    dend = shc.dendrogram(shc.linkage(df[['Murder', 'Assault', 'UrbanPop', 'Rape']], method='ward'), labels=df.State.values, color_threshold=100)  \n    plt.xticks(fontsize=12)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#cluster-plot","title":"Cluster Plot","text":"<p>Cluster Plot canbe used to demarcate points that belong to the same cluster. Below is a representational example to group the US states into 5 groups based on the USArrests dataset. This cluster plot uses the \u2018murder\u2019 and \u2018assault\u2019 columns as X and Y axis. Alternately you can use the first to principal components as rthe X and Y axis.</p> Show Code <pre><code>    from sklearn.cluster import AgglomerativeClustering\n    from scipy.spatial import ConvexHull\n\n    # Import Data\n    df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/USArrests.csv')\n\n    # Agglomerative Clustering\n    cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')  \n    cluster.fit_predict(df[['Murder', 'Assault', 'UrbanPop', 'Rape']])  \n\n    # Plot\n    plt.figure(figsize=(14, 10), dpi= 80)  \n    plt.scatter(df.iloc[:,0], df.iloc[:,1], c=cluster.labels_, cmap='tab10')  \n\n    # Encircle\n    def encircle(x,y, ax=None, **kw):\n        if not ax: ax=plt.gca()\n        p = np.c_[x,y]\n        hull = ConvexHull(p)\n        poly = plt.Polygon(p[hull.vertices,:], **kw)\n        ax.add_patch(poly)\n\n    # Draw polygon surrounding vertices    \n    encircle(df.loc[cluster.labels_ == 0, 'Murder'], df.loc[cluster.labels_ == 0, 'Assault'], ec=\"k\", fc=\"gold\", alpha=0.2, linewidth=0)\n    encircle(df.loc[cluster.labels_ == 1, 'Murder'], df.loc[cluster.labels_ == 1, 'Assault'], ec=\"k\", fc=\"tab:blue\", alpha=0.2, linewidth=0)\n    encircle(df.loc[cluster.labels_ == 2, 'Murder'], df.loc[cluster.labels_ == 2, 'Assault'], ec=\"k\", fc=\"tab:red\", alpha=0.2, linewidth=0)\n    encircle(df.loc[cluster.labels_ == 3, 'Murder'], df.loc[cluster.labels_ == 3, 'Assault'], ec=\"k\", fc=\"tab:green\", alpha=0.2, linewidth=0)\n    encircle(df.loc[cluster.labels_ == 4, 'Murder'], df.loc[cluster.labels_ == 4, 'Assault'], ec=\"k\", fc=\"tab:orange\", alpha=0.2, linewidth=0)\n\n    # Decorations\n    plt.xlabel('Murder'); plt.xticks(fontsize=12)\n    plt.ylabel('Assault'); plt.yticks(fontsize=12)\n    plt.title('Agglomerative Clustering of USArrests (5 Groups)', fontsize=22)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#andrews-curve","title":"Andrews Curve","text":"<p>Andrews Curve helps visualize if there are inherent groupings of the numerical features based on a given grouping. If the features (columns in the dataset) doesn\u2019t help discriminate the group (<code>cyl)</code>, then the lines will not be well segregated as you see below.</p> Show Code <pre><code>    from pandas.plotting import andrews_curves\n\n    # Import\n    df = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mtcars.csv\")\n    df.drop(['cars', 'carname'], axis=1, inplace=True)\n\n    # Plot\n    plt.figure(figsize=(12,9), dpi= 80)\n    andrews_curves(df, 'cyl', colormap='Set1')\n\n    # Lighten borders\n    plt.gca().spines[\"top\"].set_alpha(0)\n    plt.gca().spines[\"bottom\"].set_alpha(.3)\n    plt.gca().spines[\"right\"].set_alpha(0)\n    plt.gca().spines[\"left\"].set_alpha(.3)\n\n    plt.title('Andrews Curves of mtcars', fontsize=22)\n    plt.xlim(-3,3)\n    plt.grid(alpha=0.3)\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    plt.show()\n</code></pre> <p></p>"},{"location":"viz/viz/#parallel-coordinates","title":"Parallel Coordinates","text":"<p>Parallel coordinates helps to visualize if a feature helps to segregate the groups effectively. If a segregation is effected, that feature is likely going to be very useful in predicting that group.</p> Show Code <pre><code>    from pandas.plotting import parallel_coordinates\n\n    # Import Data\n    df_final = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/diamonds_filter.csv\")\n\n    # Plot\n    plt.figure(figsize=(12,9), dpi= 80)\n    parallel_coordinates(df_final, 'cut', colormap='Dark2')\n\n    # Lighten borders\n    plt.gca().spines[\"top\"].set_alpha(0)\n    plt.gca().spines[\"bottom\"].set_alpha(.3)\n    plt.gca().spines[\"right\"].set_alpha(0)\n    plt.gca().spines[\"left\"].set_alpha(.3)\n\n    plt.title('Parallel Coordinated of Diamonds', fontsize=22)\n    plt.grid(alpha=0.3)\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    plt.show()\n</code></pre> <p></p>"},{"location":"web/webscraping/","title":"Web Scraping","text":"<p>More coming... </p> <p>This might just be Extract within ETL</p>"}]}