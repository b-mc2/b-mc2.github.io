{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to PyToolbox This is a collection of tools and resources that I have found helpful for various jobs. This is in no way complete and many sections have not been filled out yet. Go to ToolBox","title":"Home"},{"location":"#welcome-to-pytoolbox","text":"This is a collection of tools and resources that I have found helpful for various jobs. This is in no way complete and many sections have not been filled out yet. Go to ToolBox","title":"Welcome to PyToolbox"},{"location":"bi/bi/","text":"Business Intelligence Business intelligence (BI) refers to the procedural and technical infrastructure that collects, stores, and analyzes the data produced by a company\u2019s activities. BI is a broad term that encompasses data mining, process analysis, performance benchmarking, and descriptive analytics. BI parses all the data generated by a business and presents easy-to-digest reports, performance measures, and trends that inform management decisions. BI represents the technical infrastructure that collects, stores, and analyzes company data. BI parses data and produces reports and information that help managers to make better decisions. Software companies produce BI solutions for companies that wish to make better use of their data. BI tools and software come in a wide variety of forms such as spreadsheets, reporting/query software, data visualization software, data mining tools, and online analytical processing (OLAP). Self-service BI is an approach to analytics that allows individuals without a technical background to access and explore data. Definition BI Tools RawGraphs RAW Graphs is an open source data visualization framework built with the goal of making the visual representation of complex data easy for everyone. Primarily conceived as a tool for designers and vis geeks, RAW Graphs aims at providing a missing link between spreadsheet applications (e.g. Microsoft Excel, Apple Numbers, OpenRefine) and vector graphics editors (e.g. Adobe Illustrator, Inkscape, Sketch). Documentation Examples Use the Live App Sqliteviz Sqliteviz is a single-page offline-first PWA for fully client-side visualisation of SQLite databases or CSV files. With sqliteviz you can: - run SQL queries against a SQLite database and create Plotly charts and pivot tables based on the result sets - import a CSV file into a SQLite database and visualize imported data - export result set to CSV file - manage inquiries and run them against different databases - import/export inquiries from/to a JSON file - export a modified SQLite database - use it offline from your OS application menu like any other desktop app Documentation Examples Use the Live App Streamlit Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes you can build and deploy powerful data apps. Documentation Examples import streamlit as st import pandas as pd import numpy as np st . title ( 'Uber pickups in NYC' ) DATE_COLUMN = 'date/time' DATA_URL = ( 'https://s3-us-west-2.amazonaws.com/' 'streamlit-demo-data/uber-raw-data-sep14.csv.gz' ) def load_data ( nrows ): data = pd . read_csv ( DATA_URL , nrows = nrows ) data [ DATE_COLUMN ] = pd . to_datetime ( data [ DATE_COLUMN ]) return data data = load_data ( 10000 ) Create a bar chart hist_values = np . histogram ( data [ DATE_COLUMN ] . dt . hour , bins = 24 , range = ( 0 , 24 ) )[ 0 ] st . bar_chart ( hist_values ) Plot data on a map with sliding filter hour_to_filter = st . slider ( 'hour' , 0 , 23 , 17 ) # min: 0h, max: 23h, default: 17h filtered_data = data [ data [ DATE_COLUMN ] . dt . hour == hour_to_filter ] st . subheader ( f 'Map of all pickups at { hour_to_filter } :00' ) st . map ( filtered_data ) Use the Live App LightDash Connect Lightdash to your dbt project, add metrics directly in your data transformation layer, then create and share your insights with your team. Documentation Examples Use the Live Demo App Apache Superset Superset is fast, lightweight, intuitive, and loaded with options that make it easy for users of all skill sets to explore and visualize their data, from simple line charts to highly detailed geospatial charts. Documentation Examples","title":"BI"},{"location":"bi/bi/#business-intelligence","text":"Business intelligence (BI) refers to the procedural and technical infrastructure that collects, stores, and analyzes the data produced by a company\u2019s activities. BI is a broad term that encompasses data mining, process analysis, performance benchmarking, and descriptive analytics. BI parses all the data generated by a business and presents easy-to-digest reports, performance measures, and trends that inform management decisions. BI represents the technical infrastructure that collects, stores, and analyzes company data. BI parses data and produces reports and information that help managers to make better decisions. Software companies produce BI solutions for companies that wish to make better use of their data. BI tools and software come in a wide variety of forms such as spreadsheets, reporting/query software, data visualization software, data mining tools, and online analytical processing (OLAP). Self-service BI is an approach to analytics that allows individuals without a technical background to access and explore data. Definition","title":"Business Intelligence"},{"location":"bi/bi/#bi-tools","text":"","title":"BI Tools"},{"location":"bi/bi/#rawgraphs","text":"RAW Graphs is an open source data visualization framework built with the goal of making the visual representation of complex data easy for everyone. Primarily conceived as a tool for designers and vis geeks, RAW Graphs aims at providing a missing link between spreadsheet applications (e.g. Microsoft Excel, Apple Numbers, OpenRefine) and vector graphics editors (e.g. Adobe Illustrator, Inkscape, Sketch). Documentation Examples Use the Live App","title":"RawGraphs"},{"location":"bi/bi/#sqliteviz","text":"Sqliteviz is a single-page offline-first PWA for fully client-side visualisation of SQLite databases or CSV files. With sqliteviz you can: - run SQL queries against a SQLite database and create Plotly charts and pivot tables based on the result sets - import a CSV file into a SQLite database and visualize imported data - export result set to CSV file - manage inquiries and run them against different databases - import/export inquiries from/to a JSON file - export a modified SQLite database - use it offline from your OS application menu like any other desktop app Documentation Examples Use the Live App","title":"Sqliteviz"},{"location":"bi/bi/#streamlit","text":"Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes you can build and deploy powerful data apps. Documentation Examples import streamlit as st import pandas as pd import numpy as np st . title ( 'Uber pickups in NYC' ) DATE_COLUMN = 'date/time' DATA_URL = ( 'https://s3-us-west-2.amazonaws.com/' 'streamlit-demo-data/uber-raw-data-sep14.csv.gz' ) def load_data ( nrows ): data = pd . read_csv ( DATA_URL , nrows = nrows ) data [ DATE_COLUMN ] = pd . to_datetime ( data [ DATE_COLUMN ]) return data data = load_data ( 10000 ) Create a bar chart hist_values = np . histogram ( data [ DATE_COLUMN ] . dt . hour , bins = 24 , range = ( 0 , 24 ) )[ 0 ] st . bar_chart ( hist_values ) Plot data on a map with sliding filter hour_to_filter = st . slider ( 'hour' , 0 , 23 , 17 ) # min: 0h, max: 23h, default: 17h filtered_data = data [ data [ DATE_COLUMN ] . dt . hour == hour_to_filter ] st . subheader ( f 'Map of all pickups at { hour_to_filter } :00' ) st . map ( filtered_data ) Use the Live App","title":"Streamlit"},{"location":"bi/bi/#lightdash","text":"Connect Lightdash to your dbt project, add metrics directly in your data transformation layer, then create and share your insights with your team. Documentation Examples Use the Live Demo App","title":"LightDash"},{"location":"bi/bi/#apache-superset","text":"Superset is fast, lightweight, intuitive, and loaded with options that make it easy for users of all skill sets to explore and visualize their data, from simple line charts to highly detailed geospatial charts. Documentation Examples","title":"Apache Superset"},{"location":"blogs/blogs/","text":"Blogs and other External Resources Here's some resources I've found and like to reference, I'll try not to list extremely common sources here. Python https://www.fullstackpython.com/ https://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g https://docs.python-guide.org/ https://pbpython.com/ Data Engineering https://www.startdataengineering.com/ https://medium.com/@rchang/a-beginners-guide-to-data-engineering-part-i-4227c5c457d7 https://www.startdataengineering.com/ https://github.com/zappa/Zappa https://shancarter.github.io/mr-data-converter/ AI/ML https://aegeorge42.github.io/ https://www.eleuther.ai/ https://riverml.xyz/latest/ https://pypi.org/project/pybaobabdt/ https://becominghuman.ai/ https://explained.ai/decision-tree-viz/index.html https://medium.com/coders-camp/40-machine-learning-algorithms-with-python-3defd764b961 https://happytransformer.com/ https://pycaret.org/ https://developers.google.com/machine-learning/guides/rules-of-ml https://realpython.com/python-ai-neural-network/ Procedurally Generated Images https://github.com/enjeck/Blobby https://www.generativehut.com/post/robots-and-generative-art-and-python-oh-my https://github.com/nft-fun/generate-bitbirds https://cryptopunksnotdead.github.io/pixelart.js/editor/ https://robohash.org/ Visualizations/Colors https://www.python-graph-gallery.com/ https://rampgenerator.com/ https://imagecolorpicker.com/ https://matplotlib.org/ https://bokeh.org/ https://plotly.com/python/ https://seaborn.pydata.org/ https://www.machinelearningplus.com https://altair-viz.github.io/index.html https://www.thecolorapi.com/ https://feathericons.com/ https://diagrams.mingrammer.com https://github.com/scottrogowski/code2flow Cryptography https://blog.cloudflare.com/a-relatively-easy-to-understand-primer-on-elliptic-curve-cryptography/ JS/HTML/CSS https://mrcoles.com/bookmarklet/ https://htmx.org/ https://neutralino.js.org/ https://mermaid-js.github.io/mermaid/#/ Misc (for now) https://crontab.guru/ https://github.com/sindresorhus/awesome https://ivizri.com/ https://smirkygraphs.github.io/ http://bluoceans.co/ https://article-summary.herokuapp.com/","title":"Blogs"},{"location":"blogs/blogs/#blogs-and-other-external-resources","text":"Here's some resources I've found and like to reference, I'll try not to list extremely common sources here.","title":"Blogs and other External Resources"},{"location":"blogs/blogs/#python","text":"https://www.fullstackpython.com/ https://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g https://docs.python-guide.org/ https://pbpython.com/","title":"Python"},{"location":"blogs/blogs/#data-engineering","text":"https://www.startdataengineering.com/ https://medium.com/@rchang/a-beginners-guide-to-data-engineering-part-i-4227c5c457d7 https://www.startdataengineering.com/ https://github.com/zappa/Zappa https://shancarter.github.io/mr-data-converter/","title":"Data Engineering"},{"location":"blogs/blogs/#aiml","text":"https://aegeorge42.github.io/ https://www.eleuther.ai/ https://riverml.xyz/latest/ https://pypi.org/project/pybaobabdt/ https://becominghuman.ai/ https://explained.ai/decision-tree-viz/index.html https://medium.com/coders-camp/40-machine-learning-algorithms-with-python-3defd764b961 https://happytransformer.com/ https://pycaret.org/ https://developers.google.com/machine-learning/guides/rules-of-ml https://realpython.com/python-ai-neural-network/","title":"AI/ML"},{"location":"blogs/blogs/#procedurally-generated-images","text":"https://github.com/enjeck/Blobby https://www.generativehut.com/post/robots-and-generative-art-and-python-oh-my https://github.com/nft-fun/generate-bitbirds https://cryptopunksnotdead.github.io/pixelart.js/editor/ https://robohash.org/","title":"Procedurally Generated Images"},{"location":"blogs/blogs/#visualizationscolors","text":"https://www.python-graph-gallery.com/ https://rampgenerator.com/ https://imagecolorpicker.com/ https://matplotlib.org/ https://bokeh.org/ https://plotly.com/python/ https://seaborn.pydata.org/ https://www.machinelearningplus.com https://altair-viz.github.io/index.html https://www.thecolorapi.com/ https://feathericons.com/ https://diagrams.mingrammer.com https://github.com/scottrogowski/code2flow","title":"Visualizations/Colors"},{"location":"blogs/blogs/#cryptography","text":"https://blog.cloudflare.com/a-relatively-easy-to-understand-primer-on-elliptic-curve-cryptography/","title":"Cryptography"},{"location":"blogs/blogs/#jshtmlcss","text":"https://mrcoles.com/bookmarklet/ https://htmx.org/ https://neutralino.js.org/ https://mermaid-js.github.io/mermaid/#/","title":"JS/HTML/CSS"},{"location":"blogs/blogs/#misc-for-now","text":"https://crontab.guru/ https://github.com/sindresorhus/awesome https://ivizri.com/ https://smirkygraphs.github.io/ http://bluoceans.co/ https://article-summary.herokuapp.com/","title":"Misc (for now)"},{"location":"crypto/encryption/","text":"Encryption Encryption is a way of scrambling data so that only authorized parties can understand the information. In technical terms, it is the process of converting human-readable plaintext to incomprehensible text, also known as ciphertext. In simpler terms, encryption takes readable data and alters it so that it appears random. Encryption requires the use of a cryptographic key: a set of mathematical values that both the sender and the recipient of an encrypted message agree on. Definition Symmetric Encryption In symmetric-key encryption, the data is encoded and decoded with the same key. This is the easiest way of encryption, but also less secure. The receiver needs the key for decryption, so a safe way need for transferring keys. Anyone with the key can read the data in the middle. Fernet Steps Import Fernet Then generate an encryption key, that can be used for encryption and decryption. Convert the string to byte string, so that it can be encrypted. Instance the Fernet class with the encryption key. Then encrypt the string with Fernet instance. Then it can be decrypted with Fernet class instance and it should be instanced with the same key used for encryption. from cryptography.fernet import Fernet message = \"Encrypt this message\" key = Fernet . generate_key () fernet = Fernet ( key ) # string must must be encoded to byte string before encryption encMessage = fernet . encrypt ( message . encode ()) # Decrypting decMessage = fernet . decrypt ( encMessage ) . decode () More Info Asymmetric Encryption In Asymmetric-key Encryption, we use two keys a public key and private key. The public key is used to encrypt the data and the private key is used to decrypt the data. By the name, the public key can be public (can be sent to anyone who needs to send data). No one has your private key, so no one the middle can read your data. RSA Steps: Import rsa library Generate public and private keys with rsa.newkeys() method. Encode the string to byte string. Then encrypt the byte string with the public key. Then the encrypted string can be decrypted with the private key. The public key can only be used for encryption and the private can only be used for decryption. import rsa publicKey , privateKey = rsa . newkeys ( 512 ) message = \"Encrypt this message\" # string must must be encoded to byte string before encryption encMessage = rsa . encrypt ( message . encode (), publicKey ) # Decrypting decMessage = rsa . decrypt ( encMessage , privateKey ) . decode () More Info Homomorphic Encryption Python-Paillier Documentation A Python 3 library implementing the Paillier Partially Homomorphic Encryption. The homomorphic properties of the paillier crypto system are: Encrypted numbers can be multiplied by a non encrypted scalar. Encrypted numbers can be added together. Encrypted numbers can be added to non encrypted scalars. Elliptic Curve Cryptography Documentation","title":"Encryption"},{"location":"crypto/encryption/#encryption","text":"Encryption is a way of scrambling data so that only authorized parties can understand the information. In technical terms, it is the process of converting human-readable plaintext to incomprehensible text, also known as ciphertext. In simpler terms, encryption takes readable data and alters it so that it appears random. Encryption requires the use of a cryptographic key: a set of mathematical values that both the sender and the recipient of an encrypted message agree on. Definition","title":"Encryption"},{"location":"crypto/encryption/#symmetric-encryption","text":"In symmetric-key encryption, the data is encoded and decoded with the same key. This is the easiest way of encryption, but also less secure. The receiver needs the key for decryption, so a safe way need for transferring keys. Anyone with the key can read the data in the middle.","title":"Symmetric Encryption"},{"location":"crypto/encryption/#fernet","text":"Steps Import Fernet Then generate an encryption key, that can be used for encryption and decryption. Convert the string to byte string, so that it can be encrypted. Instance the Fernet class with the encryption key. Then encrypt the string with Fernet instance. Then it can be decrypted with Fernet class instance and it should be instanced with the same key used for encryption. from cryptography.fernet import Fernet message = \"Encrypt this message\" key = Fernet . generate_key () fernet = Fernet ( key ) # string must must be encoded to byte string before encryption encMessage = fernet . encrypt ( message . encode ()) # Decrypting decMessage = fernet . decrypt ( encMessage ) . decode () More Info","title":"Fernet"},{"location":"crypto/encryption/#asymmetric-encryption","text":"In Asymmetric-key Encryption, we use two keys a public key and private key. The public key is used to encrypt the data and the private key is used to decrypt the data. By the name, the public key can be public (can be sent to anyone who needs to send data). No one has your private key, so no one the middle can read your data.","title":"Asymmetric Encryption"},{"location":"crypto/encryption/#rsa","text":"Steps: Import rsa library Generate public and private keys with rsa.newkeys() method. Encode the string to byte string. Then encrypt the byte string with the public key. Then the encrypted string can be decrypted with the private key. The public key can only be used for encryption and the private can only be used for decryption. import rsa publicKey , privateKey = rsa . newkeys ( 512 ) message = \"Encrypt this message\" # string must must be encoded to byte string before encryption encMessage = rsa . encrypt ( message . encode (), publicKey ) # Decrypting decMessage = rsa . decrypt ( encMessage , privateKey ) . decode () More Info","title":"RSA"},{"location":"crypto/encryption/#homomorphic-encryption","text":"","title":"Homomorphic Encryption"},{"location":"crypto/encryption/#python-paillier","text":"Documentation A Python 3 library implementing the Paillier Partially Homomorphic Encryption. The homomorphic properties of the paillier crypto system are: Encrypted numbers can be multiplied by a non encrypted scalar. Encrypted numbers can be added together. Encrypted numbers can be added to non encrypted scalars.","title":"Python-Paillier"},{"location":"crypto/encryption/#elliptic-curve-cryptography","text":"","title":"Elliptic Curve Cryptography"},{"location":"crypto/encryption/#_1","text":"Documentation","title":""},{"location":"crypto/hashing/","text":"Hashing Cryptographic hashes are used in day-day life like in digital signatures, message authentication codes, manipulation detection, fingerprints, checksums (message integrity check), hash tables, password storage and much more. They are also used in sending messages over network for security or storing messages in databases. Definition Many encryption and hashing algorithms use the follow functions: encode() : Converts the string into bytes to be acceptable by hash function. digest() : Returns the encoded data in byte format. hexdigest() : Returns the encoded data in hexadecimal format. Hashing Algorithms Python hash() int_val = 4 str_val = 'GeeksforGeeks' flt_val = 24.56 # Printing the hash values. # Notice Integer value doesn't change # You'l have answer later in article. print ( \"The integer hash value is : \" + str ( hash ( int_val ))) print ( \"The string hash value is : \" + str ( hash ( str_val ))) print ( \"The float hash value is : \" + str ( hash ( flt_val ))) More Info MD5 Hash import hashlib # note the string is in byte format result = hashlib . md5 ( b 'information to hash' ) # alternatively... info_to_hash = \"information to hash\" result = hashlib . md5 ( info_to_hash . encode ()) # printing the equivalent byte value. print ( result . digest ()) # b'\\xf1\\xe0ix~\\xcetS\\x1d\\x11%Y\\x94\\\\hq' # printing the equivalent hexadecimal value. print ( result . hexdigest ()) # f1e069787ece74531d112559945c6871 SHA1 Hash SHA256 Hash import hashlib # initializing string str = \"GeeksforGeeks\" # then sending to SHA1() result = hashlib . sha1 ( str . encode ()) # printing the equivalent hexadecimal value. print ( result . hexdigest ()) # then sending to SHA256() result = hashlib . sha256 ( str . encode ()) print ( result . hexdigest ()) # then sending to SHA512() result = hashlib . sha512 ( str . encode ()) print ( result . hexdigest ()) # Of the same inputs, each algorithm produces these outputs # SHA1: 4175a37afd561152fb60c305d4fa6026b7e79856 # SHA256: f6071725e7ddeb434fb6b32b8ec4a2b14dd7db0d785347b2fb48f9975126178f # SHA512: 0d8fb9370a5bf7b892be4865cdf8b658a82209624e33ed71cae353b0df254a75db63d1baa35ad99f26f1b399c31f3c666a7fc67ecef3bdcdb7d60e8ada90b722 More Info","title":"Hashing"},{"location":"crypto/hashing/#hashing","text":"Cryptographic hashes are used in day-day life like in digital signatures, message authentication codes, manipulation detection, fingerprints, checksums (message integrity check), hash tables, password storage and much more. They are also used in sending messages over network for security or storing messages in databases. Definition Many encryption and hashing algorithms use the follow functions: encode() : Converts the string into bytes to be acceptable by hash function. digest() : Returns the encoded data in byte format. hexdigest() : Returns the encoded data in hexadecimal format.","title":"Hashing"},{"location":"crypto/hashing/#hashing-algorithms","text":"","title":"Hashing Algorithms"},{"location":"crypto/hashing/#python-hash","text":"int_val = 4 str_val = 'GeeksforGeeks' flt_val = 24.56 # Printing the hash values. # Notice Integer value doesn't change # You'l have answer later in article. print ( \"The integer hash value is : \" + str ( hash ( int_val ))) print ( \"The string hash value is : \" + str ( hash ( str_val ))) print ( \"The float hash value is : \" + str ( hash ( flt_val ))) More Info","title":"Python hash()"},{"location":"crypto/hashing/#md5-hash","text":"import hashlib # note the string is in byte format result = hashlib . md5 ( b 'information to hash' ) # alternatively... info_to_hash = \"information to hash\" result = hashlib . md5 ( info_to_hash . encode ()) # printing the equivalent byte value. print ( result . digest ()) # b'\\xf1\\xe0ix~\\xcetS\\x1d\\x11%Y\\x94\\\\hq' # printing the equivalent hexadecimal value. print ( result . hexdigest ()) # f1e069787ece74531d112559945c6871","title":"MD5 Hash"},{"location":"crypto/hashing/#sha1-hash","text":"","title":"SHA1 Hash"},{"location":"crypto/hashing/#sha256-hash","text":"import hashlib # initializing string str = \"GeeksforGeeks\" # then sending to SHA1() result = hashlib . sha1 ( str . encode ()) # printing the equivalent hexadecimal value. print ( result . hexdigest ()) # then sending to SHA256() result = hashlib . sha256 ( str . encode ()) print ( result . hexdigest ()) # then sending to SHA512() result = hashlib . sha512 ( str . encode ()) print ( result . hexdigest ()) # Of the same inputs, each algorithm produces these outputs # SHA1: 4175a37afd561152fb60c305d4fa6026b7e79856 # SHA256: f6071725e7ddeb434fb6b32b8ec4a2b14dd7db0d785347b2fb48f9975126178f # SHA512: 0d8fb9370a5bf7b892be4865cdf8b658a82209624e33ed71cae353b0df254a75db63d1baa35ad99f26f1b399c31f3c666a7fc67ecef3bdcdb7d60e8ada90b722 More Info","title":"SHA256 Hash"},{"location":"crypto/steganography/","text":"Steganography Definition BI Tools RawGraphs RAW Graphs is an open source data visualization framework built with the goal of making the visual representation of complex data easy for everyone. Primarily conceived as a tool for designers and vis geeks, RAW Graphs aims at providing a missing link between spreadsheet applications (e.g. Microsoft Excel, Apple Numbers, OpenRefine) and vector graphics editors (e.g. Adobe Illustrator, Inkscape, Sketch).","title":"Steganography"},{"location":"crypto/steganography/#steganography","text":"Definition","title":"Steganography"},{"location":"crypto/steganography/#bi-tools","text":"","title":"BI Tools"},{"location":"crypto/steganography/#rawgraphs","text":"RAW Graphs is an open source data visualization framework built with the goal of making the visual representation of complex data easy for everyone. Primarily conceived as a tool for designers and vis geeks, RAW Graphs aims at providing a missing link between spreadsheet applications (e.g. Microsoft Excel, Apple Numbers, OpenRefine) and vector graphics editors (e.g. Adobe Illustrator, Inkscape, Sketch).","title":"RawGraphs"},{"location":"data/binning/","text":"Binning Data When dealing with continuous numeric data, it is often helpful to bin the data into multiple buckets for further analysis. There are several different terms for binning including bucketing, discrete binning, discretization or quantization. One of the most common instances of binning is done behind the scenes for you when creating a histogram. The histogram below of customer sales data, shows how a continuous set of sales numbers can be divided into discrete bins (for example: $60,000 - $70,000) and then used to group and count account instances. Binning Data Pandas cut Pandas cut and qcut info is taken from pbpython Documentation cut_labels_4 = [ 'silver' , 'gold' , 'platinum' , 'diamond' ] cut_bins = [ 0 , 70000 , 100000 , 130000 , 200000 ] df [ 'cut_ex1' ] = pd . cut ( df [ 'ext price' ], bins = cut_bins , labels = cut_labels_4 ) account number name ext price cut_ex1 141962 Herman LLC 63626.03 silver 146832 Kiehn-Spinka 99608.77 gold 163416 Purdy-Kunde 77898.21 gold 218895 Kulas Inc 137351.96 diamond 239344 Stokes LLC 91535.92 gold Pandas qcut Documentation The pandas documentation describes qcut as a \u201cQuantile-based discretization function.\u201d This basically means that qcut tries to divide up the underlying data into equal sized bins. The function defines the bins using percentiles based on the distribution of the data, not the actual numeric edges of the bins. The simplest use of qcut is to define the number of quantiles and let pandas figure out how to divide up the data. In the example below, we tell pandas to create 4 equal sized groupings of the data. df [ 'quantile_ex_1' ] = pd . qcut ( df [ 'ext price' ], q = 4 ) df [ 'quantile_ex_2' ] = pd . qcut ( df [ 'ext price' ], q = 10 , precision = 0 ) df . head () account number name ext price cut_ex1 quantile_ex_2 141962 Herman LLC 63626.03 (55733.049000000006, 89137.708] (55732.0, 76471.0] 146832 Kiehn-Spinka 99608.77 (89137.708, 100271.535] (95908.0, 100272.0] 163416 Purdy-Kunde 77898.21 (55733.049000000006, 89137.708] (76471.0, 87168.0] 218895 Kulas Inc 137351.96 (110132.552, 184793.7] (124778.0, 184794.0] 239344 Stokes LLC 91535.92 (89137.708, 100271.535] (90686.0, 95908.0] Fisher-Jenks Algorithm Documentation What we are trying to do is identify natural groupings of numbers that are \u201cclose\u201d together while also maximizing the distance between the other groupings. Fisher developed a clustering algorithm that does this with 1 dimensional data (essentially a single list of numbers). In many ways it is similar to k-means clustering but is ultimately a simpler and faster algorithm because it only works on 1 dimensional data. Like k-means, you do need to specify the number of clusters. Therefore domain knowledge and understanding of the data are still essential to using this effectively. The algorithm uses an iterative approach to find the best groupings of numbers based on how close they are together (based on variance from the group\u2019s mean) while also trying to ensure the different groupings are as distinct as possible (by maximizing the group\u2019s variance between groups). import pandas as pd import jenkspy sales = { 'account' : [ 'Jones Inc' , 'Alpha Co' , 'Blue Inc' , 'Super Star Inc' , 'Wamo' , 'Next Gen' , 'Giga Co' , 'IniTech' , 'Beta LLC' ], 'Total' : [ 1500 , 2100 , 50 , 20 , 75 , 1100 , 950 , 1300 , 1400 ] } df = pd . DataFrame ( sales ) df . sort_values ( by = 'Total' ) which yields this dataframe In order to illustrate how natural breaks are found, we can start by contrasting it with how quantiles are determined. For example, what happens if we try to use pd.qcut with 2 quantiles? Will that give us a similar result? df [ 'quantile' ] = pd . qcut ( df [ 'Total' ], q = 2 , labels = [ 'bucket_1' , 'bucket_2' ]) Just to get one more example, we can see what 4 buckets would look like with natural breaks and with a quantile cut approach: df [ 'quantilev2' ] = pd . qcut ( df [ 'Total' ], q = 4 , labels = [ 'bucket_1' , 'bucket_2' , 'bucket_3' , 'bucket_4' ]) df [ 'cut_jenksv3' ] = pd . cut ( df [ 'Total' ], bins = jenkspy . jenks_breaks ( df [ 'Total' ], nb_class = 4 ), labels = [ 'bucket_1' , 'bucket_2' , 'bucket_3' , 'bucket_4' ], include_lowest = True ) df . sort_values ( by = 'Total' )","title":"Binning"},{"location":"data/binning/#binning-data","text":"When dealing with continuous numeric data, it is often helpful to bin the data into multiple buckets for further analysis. There are several different terms for binning including bucketing, discrete binning, discretization or quantization. One of the most common instances of binning is done behind the scenes for you when creating a histogram. The histogram below of customer sales data, shows how a continuous set of sales numbers can be divided into discrete bins (for example: $60,000 - $70,000) and then used to group and count account instances.","title":"Binning Data"},{"location":"data/binning/#binning-data_1","text":"","title":"Binning Data"},{"location":"data/binning/#pandas-cut","text":"Pandas cut and qcut info is taken from pbpython Documentation cut_labels_4 = [ 'silver' , 'gold' , 'platinum' , 'diamond' ] cut_bins = [ 0 , 70000 , 100000 , 130000 , 200000 ] df [ 'cut_ex1' ] = pd . cut ( df [ 'ext price' ], bins = cut_bins , labels = cut_labels_4 ) account number name ext price cut_ex1 141962 Herman LLC 63626.03 silver 146832 Kiehn-Spinka 99608.77 gold 163416 Purdy-Kunde 77898.21 gold 218895 Kulas Inc 137351.96 diamond 239344 Stokes LLC 91535.92 gold","title":"Pandas cut"},{"location":"data/binning/#pandas-qcut","text":"Documentation The pandas documentation describes qcut as a \u201cQuantile-based discretization function.\u201d This basically means that qcut tries to divide up the underlying data into equal sized bins. The function defines the bins using percentiles based on the distribution of the data, not the actual numeric edges of the bins. The simplest use of qcut is to define the number of quantiles and let pandas figure out how to divide up the data. In the example below, we tell pandas to create 4 equal sized groupings of the data. df [ 'quantile_ex_1' ] = pd . qcut ( df [ 'ext price' ], q = 4 ) df [ 'quantile_ex_2' ] = pd . qcut ( df [ 'ext price' ], q = 10 , precision = 0 ) df . head () account number name ext price cut_ex1 quantile_ex_2 141962 Herman LLC 63626.03 (55733.049000000006, 89137.708] (55732.0, 76471.0] 146832 Kiehn-Spinka 99608.77 (89137.708, 100271.535] (95908.0, 100272.0] 163416 Purdy-Kunde 77898.21 (55733.049000000006, 89137.708] (76471.0, 87168.0] 218895 Kulas Inc 137351.96 (110132.552, 184793.7] (124778.0, 184794.0] 239344 Stokes LLC 91535.92 (89137.708, 100271.535] (90686.0, 95908.0]","title":"Pandas qcut"},{"location":"data/binning/#fisher-jenks-algorithm","text":"Documentation What we are trying to do is identify natural groupings of numbers that are \u201cclose\u201d together while also maximizing the distance between the other groupings. Fisher developed a clustering algorithm that does this with 1 dimensional data (essentially a single list of numbers). In many ways it is similar to k-means clustering but is ultimately a simpler and faster algorithm because it only works on 1 dimensional data. Like k-means, you do need to specify the number of clusters. Therefore domain knowledge and understanding of the data are still essential to using this effectively. The algorithm uses an iterative approach to find the best groupings of numbers based on how close they are together (based on variance from the group\u2019s mean) while also trying to ensure the different groupings are as distinct as possible (by maximizing the group\u2019s variance between groups). import pandas as pd import jenkspy sales = { 'account' : [ 'Jones Inc' , 'Alpha Co' , 'Blue Inc' , 'Super Star Inc' , 'Wamo' , 'Next Gen' , 'Giga Co' , 'IniTech' , 'Beta LLC' ], 'Total' : [ 1500 , 2100 , 50 , 20 , 75 , 1100 , 950 , 1300 , 1400 ] } df = pd . DataFrame ( sales ) df . sort_values ( by = 'Total' ) which yields this dataframe In order to illustrate how natural breaks are found, we can start by contrasting it with how quantiles are determined. For example, what happens if we try to use pd.qcut with 2 quantiles? Will that give us a similar result? df [ 'quantile' ] = pd . qcut ( df [ 'Total' ], q = 2 , labels = [ 'bucket_1' , 'bucket_2' ]) Just to get one more example, we can see what 4 buckets would look like with natural breaks and with a quantile cut approach: df [ 'quantilev2' ] = pd . qcut ( df [ 'Total' ], q = 4 , labels = [ 'bucket_1' , 'bucket_2' , 'bucket_3' , 'bucket_4' ]) df [ 'cut_jenksv3' ] = pd . cut ( df [ 'Total' ], bins = jenkspy . jenks_breaks ( df [ 'Total' ], nb_class = 4 ), labels = [ 'bucket_1' , 'bucket_2' , 'bucket_3' , 'bucket_4' ], include_lowest = True ) df . sort_values ( by = 'Total' )","title":"Fisher-Jenks Algorithm"},{"location":"data/data_scale/","text":"Data Scaling Small-Medium Data - 0mb - 100mb Large Data - 100mb - 1TB Big Data - 1TB+ Tools Pandas Documentation Read CSV file data in chunk size: The parameter essentially means the number of rows to be read into a dataframe at any single time in order to fit into the local memory. import pandas as pd # read the large csv file with specified chunksize df_chunk = pd . read_csv ( r '../input/data.csv' , chunksize = 1_000_000 ) The operation above resulted in a TextFileReader object for iteration. Strictly speaking, df_chunk is not a dataframe but an object for further operation in the next step. chunk_list = [] # append each chunk df here # Each chunk is in df format for chunk in df_chunk : # perform data filtering chunk_filter = chunk_preprocessing ( chunk ) # Once the data filtering is done, append the chunk to list chunk_list . append ( chunk_filter ) # concat the list into dataframe df_concat = pd . concat ( chunk_list ) Filter out unimportant columns to save memory # Filter out unimportant columns df = df [[ 'col_1' , 'col_2' , 'col_3' , 'col_4' , 'col_5' , 'col_6' , 'col_7' , 'col_8' , 'col_9' , 'col_10' ]] Change dtypes for columns The simplest way to convert a pandas column of data to a different type is to use astype() . # Change the dtypes (int64 -> int32) df [[ 'col_1' , 'col_2' , 'col_3' , 'col_4' , 'col_5' ]] = df [[ 'col_1' , 'col_2' , 'col_3' , 'col_4' , 'col_5' ]] . astype ( 'int32' ) # Change the dtypes (float64 -> float32) df [[ 'col_6' , 'col_7' , 'col_8' , 'col_9' , 'col_10' ]] = df [[ 'col_6' , 'col_7' , 'col_8' , 'col_9' , 'col_10' ]] . astype ( 'float32' ) Modin Documentation Modin is a DataFrame for datasets from 1MB to 1TB+ Modin uses Ray or Dask to provide an effortless way to speed up your pandas notebooks, scripts, and libraries. Pandas traditionally loads data into a single CPU core, Modin will spread that dataset over multiple cores, this makes it easy to scale and also to build out multiple clusters. To use Modin, you do not need to know how many cores your system has and you do not need to specify how to distribute the data. In fact, you can continue using your previous pandas notebooks while experiencing a considerable speedup from Modin, even on a single machine. Once you\u2019ve changed your import statement, you\u2019re ready to use Modin just like you would pandas. If you don\u2019t have Ray or Dask installed, you will need to install Modin with one of the targets: pip install \"modin[ray]\" # Install Modin dependencies and Ray to run on Ray pip install \"modin[dask]\" # Install Modin dependencies and Dask to run on Dask pip install \"modin[all]\" # Install all of the above If you want to choose a specific compute engine to run on, you can set the environment variable MODIN_ENGINE and Modin will do computation with that engine: export MODIN_ENGINE = ray # Modin will use Ray export MODIN_ENGINE = dask # Modin will use Dask In pandas, you are only able to use one core at a time when you are doing computation of any kind. With Modin, you are able to use all of the CPU cores on your machine. Even in read_csv , we see large gains by efficiently distributing the work across your entire machine. import modin.pandas as pd df = pd . read_csv ( \"my_dataset.csv\" ) # 31.4 Seconds with Pandas # 7.73 Seconds with Modin Dask Documentation Dask is composed of two parts: - Dynamic task scheduling optimized for computation. This is similar to Airflow, Luigi, Celery, or Make, but optimized for interactive computational workloads. - \u201cBig Data\u201d collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of dynamic task schedulers. python -m pip install dask # Install only core parts of dask python -m pip install \"dask[complete]\" # Install everything python -m pip install \"dask[array]\" # Install requirements for dask array python -m pip install \"dask[dataframe]\" # Install requirements for dask dataframe python -m pip install \"dask[diagnostics]\" # Install requirements for dask diagnostics python -m pip install \"dask[distributed]\" # Install requirements for distributed dask Pola.rs Documentation Polars is written in Rust but has Python Pandas API Polars is a lightning fast DataFrame library/in-memory query engine. Its embarrassingly parallel execution, cache efficient algorithms and expressive API makes it perfect for efficient data wrangling, data pipelines, snappy APIs and so much more. import polars as pl q = ( pl . scan_csv ( \"iris.csv\" ) . filter ( pl . col ( \"sepal_length\" ) > 5 ) . groupby ( \"species\" ) . agg ( pl . all () . sum ()) ) df = q . collect ()","title":"Data Scale"},{"location":"data/data_scale/#data-scaling","text":"Small-Medium Data - 0mb - 100mb Large Data - 100mb - 1TB Big Data - 1TB+","title":"Data Scaling"},{"location":"data/data_scale/#tools","text":"","title":"Tools"},{"location":"data/data_scale/#pandas","text":"Documentation Read CSV file data in chunk size: The parameter essentially means the number of rows to be read into a dataframe at any single time in order to fit into the local memory. import pandas as pd # read the large csv file with specified chunksize df_chunk = pd . read_csv ( r '../input/data.csv' , chunksize = 1_000_000 ) The operation above resulted in a TextFileReader object for iteration. Strictly speaking, df_chunk is not a dataframe but an object for further operation in the next step. chunk_list = [] # append each chunk df here # Each chunk is in df format for chunk in df_chunk : # perform data filtering chunk_filter = chunk_preprocessing ( chunk ) # Once the data filtering is done, append the chunk to list chunk_list . append ( chunk_filter ) # concat the list into dataframe df_concat = pd . concat ( chunk_list ) Filter out unimportant columns to save memory # Filter out unimportant columns df = df [[ 'col_1' , 'col_2' , 'col_3' , 'col_4' , 'col_5' , 'col_6' , 'col_7' , 'col_8' , 'col_9' , 'col_10' ]] Change dtypes for columns The simplest way to convert a pandas column of data to a different type is to use astype() . # Change the dtypes (int64 -> int32) df [[ 'col_1' , 'col_2' , 'col_3' , 'col_4' , 'col_5' ]] = df [[ 'col_1' , 'col_2' , 'col_3' , 'col_4' , 'col_5' ]] . astype ( 'int32' ) # Change the dtypes (float64 -> float32) df [[ 'col_6' , 'col_7' , 'col_8' , 'col_9' , 'col_10' ]] = df [[ 'col_6' , 'col_7' , 'col_8' , 'col_9' , 'col_10' ]] . astype ( 'float32' )","title":"Pandas"},{"location":"data/data_scale/#modin","text":"Documentation Modin is a DataFrame for datasets from 1MB to 1TB+ Modin uses Ray or Dask to provide an effortless way to speed up your pandas notebooks, scripts, and libraries. Pandas traditionally loads data into a single CPU core, Modin will spread that dataset over multiple cores, this makes it easy to scale and also to build out multiple clusters. To use Modin, you do not need to know how many cores your system has and you do not need to specify how to distribute the data. In fact, you can continue using your previous pandas notebooks while experiencing a considerable speedup from Modin, even on a single machine. Once you\u2019ve changed your import statement, you\u2019re ready to use Modin just like you would pandas. If you don\u2019t have Ray or Dask installed, you will need to install Modin with one of the targets: pip install \"modin[ray]\" # Install Modin dependencies and Ray to run on Ray pip install \"modin[dask]\" # Install Modin dependencies and Dask to run on Dask pip install \"modin[all]\" # Install all of the above If you want to choose a specific compute engine to run on, you can set the environment variable MODIN_ENGINE and Modin will do computation with that engine: export MODIN_ENGINE = ray # Modin will use Ray export MODIN_ENGINE = dask # Modin will use Dask In pandas, you are only able to use one core at a time when you are doing computation of any kind. With Modin, you are able to use all of the CPU cores on your machine. Even in read_csv , we see large gains by efficiently distributing the work across your entire machine. import modin.pandas as pd df = pd . read_csv ( \"my_dataset.csv\" ) # 31.4 Seconds with Pandas # 7.73 Seconds with Modin","title":"Modin"},{"location":"data/data_scale/#dask","text":"Documentation Dask is composed of two parts: - Dynamic task scheduling optimized for computation. This is similar to Airflow, Luigi, Celery, or Make, but optimized for interactive computational workloads. - \u201cBig Data\u201d collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of dynamic task schedulers. python -m pip install dask # Install only core parts of dask python -m pip install \"dask[complete]\" # Install everything python -m pip install \"dask[array]\" # Install requirements for dask array python -m pip install \"dask[dataframe]\" # Install requirements for dask dataframe python -m pip install \"dask[diagnostics]\" # Install requirements for dask diagnostics python -m pip install \"dask[distributed]\" # Install requirements for distributed dask","title":"Dask"},{"location":"data/data_scale/#polars","text":"Documentation Polars is written in Rust but has Python Pandas API Polars is a lightning fast DataFrame library/in-memory query engine. Its embarrassingly parallel execution, cache efficient algorithms and expressive API makes it perfect for efficient data wrangling, data pipelines, snappy APIs and so much more. import polars as pl q = ( pl . scan_csv ( \"iris.csv\" ) . filter ( pl . col ( \"sepal_length\" ) > 5 ) . groupby ( \"species\" ) . agg ( pl . all () . sum ()) ) df = q . collect ()","title":"Pola.rs"},{"location":"data/eda/","text":"EDA Exploratory Data Analysis Exploratory Data Analysis (EDA) is used on the one hand to answer questions, test business assumptions, generate hypotheses for further analysis. On the other hand, you can also use it to prepare the data for modeling. The thing that these two probably have in common is a good knowledge of your data to either get the answers that you need or to develop an intuition for interpreting the results of future modeling. There are a lot of ways to reach these goals: you can get a basic description of the data, visualize it, identify patterns in it, identify challenges of using the data, etc. Data Exploration Tools Pandas Profiling Documentation Generates profile reports from a pandas DataFrame . The pandas df.describe() function is great but a little basic for serious exploratory data analysis. pandas_profiling extends the pandas DataFrame with df.profile_report() for quick data analysis. For each column the following statistics - if relevant for the column type - are presented in an interactive HTML report: Type inference : detect the types of columns in a dataframe. Essentials : type, unique values, missing values Quantile statistics : like minimum value, Q1, median, Q3, maximum, range, interquartile range Descriptive statistics : like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness Most frequent values : Histograms : Correlations : highlighting of highly correlated variables, Spearman, Pearson and Kendall matrices Missing values : matrix, count, heatmap and dendrogram of missing values Duplicate rows : Lists the most occurring duplicate rows Text analysis : learn about categories (Uppercase, Space), scripts (Latin, Cyrillic) and blocks (ASCII) of text data Basic Usage import numpy as np import pandas as pd from pandas_profiling import ProfileReport # build dataframe df = pd . DataFrame ( np . random . rand ( 100 , 5 ), columns = [ \"a\" , \"b\" , \"c\" , \"d\" , \"e\" ]) # generate a report profile = ProfileReport ( df , title = \"Pandas Profiling Report\" , explorative = True ) # Save report profile . to_file ( \"your_report.html\" ) # As a string json_data = profile . to_json () # As a file profile . to_file ( \"your_report.json\" ) Working with Larger datasets Pandas Profiling by default comprehensively summarizes the input dataset in a way that gives the most insights for data analysis. For small datasets these computations can be performed in real-time. For larger datasets, we have to decide upfront which calculations to make. Whether a computation scales to big data not only depends on it\u2019s complexity, but also on fast implementations that are available. pandas-profiling includes a minimal configuration file, with more expensive computations turned off by default. This is a great starting point for larger datasets. # minimal flag to True # Sample 10.000 rows sample = large_dataset . sample ( 10000 ) profile = ProfileReport ( sample , minimal = True ) profile . to_file ( \"output.html\" ) Privacy Features When dealing with sensitive data, such as private health records, sharing a report that includes a sample would violate patient\u2019s privacy. The following shorthand groups together various options so that only aggregate information is provided in the report. report = df . profile_report ( sensitive = True ) SweetViz Documentation Sweetviz is an open-source python auto-visualization library that generates a report, exploring the data with the help of high-density plots. It not only automates the EDA but is also used for comparing datasets and drawing inferences from it. A comparison of two datasets can be done by treating one as training and the other as testing. The Sweetviz library generates a report having: - An overview of the dataset - Variable properties - Categorical associations - Numerical associations - Most frequent, smallest, largest values for numerical features See Example Titanic Dataset import sweetviz import pandas as pd train = pd . read_csv ( \"train.csv\" ) test = pd . read_csv ( \"test.csv\" ) my_report = sweetviz . compare ([ train , \"Train\" ], [ test , \"Test\" ], \"Survived\" ) Running this command will perform the analysis and create the report object. To get the output, simply use the show_html() command: # Not providing a filename will default to SWEETVIZ_REPORT.html my_report . show_html ( \"Report.html\" ) More Details Here Dtale Documentation D-Tale is the combination of a Flask back-end and a React front-end to bring you an easy way to view & analyze Pandas data structures. It integrates seamlessly with ipython notebooks & python/ipython terminals. Currently this tool supports such Pandas objects as DataFrame, Series, MultiIndex, DatetimeIndex & RangeIndex. To start without any data right away you can run dtale.show() and open the browser to input a CSV or TSV file. import dtale import pandas as pd df = pd . DataFrame ([ dict ( a = 1 , b = 2 , c = 3 )]) # Assigning a reference to a running D-Tale process d = dtale . show ( df ) Data Exploration Manual Python Pandas import pandas as pd df = pd . read_csv ( \"path/to/file.csv\" ) df . describe () More to come...","title":"EDA"},{"location":"data/eda/#eda-exploratory-data-analysis","text":"Exploratory Data Analysis (EDA) is used on the one hand to answer questions, test business assumptions, generate hypotheses for further analysis. On the other hand, you can also use it to prepare the data for modeling. The thing that these two probably have in common is a good knowledge of your data to either get the answers that you need or to develop an intuition for interpreting the results of future modeling. There are a lot of ways to reach these goals: you can get a basic description of the data, visualize it, identify patterns in it, identify challenges of using the data, etc.","title":"EDA Exploratory Data Analysis"},{"location":"data/eda/#data-exploration-tools","text":"","title":"Data Exploration Tools"},{"location":"data/eda/#pandas-profiling","text":"Documentation Generates profile reports from a pandas DataFrame . The pandas df.describe() function is great but a little basic for serious exploratory data analysis. pandas_profiling extends the pandas DataFrame with df.profile_report() for quick data analysis. For each column the following statistics - if relevant for the column type - are presented in an interactive HTML report: Type inference : detect the types of columns in a dataframe. Essentials : type, unique values, missing values Quantile statistics : like minimum value, Q1, median, Q3, maximum, range, interquartile range Descriptive statistics : like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness Most frequent values : Histograms : Correlations : highlighting of highly correlated variables, Spearman, Pearson and Kendall matrices Missing values : matrix, count, heatmap and dendrogram of missing values Duplicate rows : Lists the most occurring duplicate rows Text analysis : learn about categories (Uppercase, Space), scripts (Latin, Cyrillic) and blocks (ASCII) of text data","title":"Pandas Profiling"},{"location":"data/eda/#basic-usage","text":"import numpy as np import pandas as pd from pandas_profiling import ProfileReport # build dataframe df = pd . DataFrame ( np . random . rand ( 100 , 5 ), columns = [ \"a\" , \"b\" , \"c\" , \"d\" , \"e\" ]) # generate a report profile = ProfileReport ( df , title = \"Pandas Profiling Report\" , explorative = True ) # Save report profile . to_file ( \"your_report.html\" ) # As a string json_data = profile . to_json () # As a file profile . to_file ( \"your_report.json\" )","title":"Basic Usage"},{"location":"data/eda/#working-with-larger-datasets","text":"Pandas Profiling by default comprehensively summarizes the input dataset in a way that gives the most insights for data analysis. For small datasets these computations can be performed in real-time. For larger datasets, we have to decide upfront which calculations to make. Whether a computation scales to big data not only depends on it\u2019s complexity, but also on fast implementations that are available. pandas-profiling includes a minimal configuration file, with more expensive computations turned off by default. This is a great starting point for larger datasets. # minimal flag to True # Sample 10.000 rows sample = large_dataset . sample ( 10000 ) profile = ProfileReport ( sample , minimal = True ) profile . to_file ( \"output.html\" )","title":"Working with Larger datasets"},{"location":"data/eda/#privacy-features","text":"When dealing with sensitive data, such as private health records, sharing a report that includes a sample would violate patient\u2019s privacy. The following shorthand groups together various options so that only aggregate information is provided in the report. report = df . profile_report ( sensitive = True )","title":"Privacy Features"},{"location":"data/eda/#sweetviz","text":"Documentation Sweetviz is an open-source python auto-visualization library that generates a report, exploring the data with the help of high-density plots. It not only automates the EDA but is also used for comparing datasets and drawing inferences from it. A comparison of two datasets can be done by treating one as training and the other as testing. The Sweetviz library generates a report having: - An overview of the dataset - Variable properties - Categorical associations - Numerical associations - Most frequent, smallest, largest values for numerical features See Example Titanic Dataset import sweetviz import pandas as pd train = pd . read_csv ( \"train.csv\" ) test = pd . read_csv ( \"test.csv\" ) my_report = sweetviz . compare ([ train , \"Train\" ], [ test , \"Test\" ], \"Survived\" ) Running this command will perform the analysis and create the report object. To get the output, simply use the show_html() command: # Not providing a filename will default to SWEETVIZ_REPORT.html my_report . show_html ( \"Report.html\" ) More Details Here","title":"SweetViz"},{"location":"data/eda/#dtale","text":"Documentation D-Tale is the combination of a Flask back-end and a React front-end to bring you an easy way to view & analyze Pandas data structures. It integrates seamlessly with ipython notebooks & python/ipython terminals. Currently this tool supports such Pandas objects as DataFrame, Series, MultiIndex, DatetimeIndex & RangeIndex. To start without any data right away you can run dtale.show() and open the browser to input a CSV or TSV file. import dtale import pandas as pd df = pd . DataFrame ([ dict ( a = 1 , b = 2 , c = 3 )]) # Assigning a reference to a running D-Tale process d = dtale . show ( df )","title":"Dtale"},{"location":"data/eda/#data-exploration-manual","text":"","title":"Data Exploration Manual"},{"location":"data/eda/#python-pandas","text":"import pandas as pd df = pd . read_csv ( \"path/to/file.csv\" ) df . describe () More to come...","title":"Python Pandas"},{"location":"data/fileformats/","text":"File Formats Data has to be stored somehow, here are some useful resources for popular data formats for the web as well as storage. CSV Documentation Files with .csv (Comma Separated Values) extension represent plain text files that contain records of data with comma separated values. Each line in a CSV file is a new record from the set of records contained in the file. Pretty straightforward and very common. CSV doesn't retain data types so storage space isn't optimal. Each record is located on a separate line, delimited by a line break (CRLF). For example: aaa,bbb,ccc CRLF zzz,yyy,xxx CRLF LOG Documentation A file with .log extension contains the list of plain text with timestamp. Usually, certain activity detail is logged by the softwares or operating systems to help the developers or users to track what was happening at a certain time period. Users can edit these files very easily by using any text editors. Usually the error reports or login activities are logged by the operating systems, but other softwares or web servers also generate log files to track visitors and to monitor bandwidth usage. SQLite Documentation A file with .sqlite extension is a lightweight SQL database file created with the SQLite software. It is a database in a file itself and implements a self-contained, full-featured, highly-reliable SQL database engine. SQLite database files can be used to share rich contents between systems by simple exchanging these files over the network. Almost all mobiles and computers use SQLite for storing and sharing of data, and is the choice of file format for cross-platform applications. Due to its compact use and easy usability, it comes bundled inside other applications. SQLite bindings exist for programming languages such as C, C#, C++, Java, PHP, and many others. SQLite in reality is a C-Language library that implements the SQLite RDBMS using the SQLite file format. With the evolution of new devices every single day, its file format has been kept backwards compatible to accommodate older devices. SQLite file format is seen as long-term archival format for the data. SQL Documentation A file with .sql extension is a Structured Query Language (SQL) file that contains code to work with relational databases. It is used to write SQL statements for CRUD (Create, Read, Update, and Delete) operations on databases. SQL files are common while working with desktop as well as web-based databases. There are several alternatives to SQL such as Java Persistence Query Language (JPQL), LINQ, HTSQL, 4D QL, and several others. SQL files can be opened by query editors of Microsoft SQL Server, MySQL and other plain text editors such as Notepad on Windows OS. SQL files are in plain text format and can comprise of several language elements. Multiple statements can be added to a single SQL file if their execution is possible without depending on each other. These SQL commands can be executed by query editors for carrying out CRUD operations. Feather Documentation Feather is a portable file format for storing Arrow tables or data frames (from languages like Python or R) that utilizes the Arrow IPC format internally. Feather was created early in the Arrow project as a proof of concept for fast, language-agnostic data frame storage for Python (pandas) and R. Additional Info Parquet Documentation Apache Parquet has the following characteristics: Self-describing Columnar format Language-independent Self-describing data embeds the schema or structure with the data itself. Hadoop use cases drive the growth of self-describing data formats, such as Parquet and JSON, and of NoSQL databases, such as HBase. These formats and databases are well suited for the agile and iterative development cycle of Hadoop applications and BI/analytics. Optimized for working with large files, Parquet arranges data in columns, putting related values in close proximity to each other to optimize query performance, minimize I/O, and facilitate compression. Parquet detects and encodes the same or similar data using a technique that conserves resources. Additional Info Avro Documentation Apache Avro is a language-neutral data serialization system. Avro has a schema-based system. A language-independent schema is associated with its read and write operations. Avro serializes the data which has a built-in schema. Avro serializes the data into a compact binary format, which can be deserialized by any application. Avro uses JSON format to declare the data structures. Presently, it supports languages such as Java, C, C++, C#, Python, and Ruby. JSONL Documentation JSON Lines is a convenient format for storing structured data that may be processed one record at a time. It works well with unix-style text processing tools and shell pipelines. It's a great format for log files. It's also a flexible format for passing messages between cooperating processes. The JSON Lines format has three requirements: - UTF-8 Encoding - JSON allows encoding Unicode strings with only ASCII escape sequences, however those escapes will be hard to read when viewed in a text editor. - Each Line is a Valid JSON Value - The most common values will be objects or arrays, but any JSON value is permitted. - Line Separator is \\n - This means '\\r\\n' is also supported because surrounding white space is implicitly ignored when parsing JSON values. The last character in the file may be a line separator, and it will be treated the same as if there was no line separator present. Data as JSONL: { \"name\" : \"Gilbert\" , \"wins\" : [[ \"straight\" , \"7\u2663\" ], [ \"one pair\" , \"10\u2665\" ]]} { \"name\" : \"Alexa\" , \"wins\" : [[ \"two pair\" , \"4\u2660\" ], [ \"two pair\" , \"9\u2660\" ]]} { \"name\" : \"May\" , \"wins\" : []} { \"name\" : \"Deloise\" , \"wins\" : [[ \"three of a kind\" , \"5\u2663\" ]]}","title":"Files"},{"location":"data/fileformats/#file-formats","text":"Data has to be stored somehow, here are some useful resources for popular data formats for the web as well as storage.","title":"File Formats"},{"location":"data/fileformats/#csv","text":"Documentation Files with .csv (Comma Separated Values) extension represent plain text files that contain records of data with comma separated values. Each line in a CSV file is a new record from the set of records contained in the file. Pretty straightforward and very common. CSV doesn't retain data types so storage space isn't optimal. Each record is located on a separate line, delimited by a line break (CRLF). For example: aaa,bbb,ccc CRLF zzz,yyy,xxx CRLF","title":"CSV"},{"location":"data/fileformats/#log","text":"Documentation A file with .log extension contains the list of plain text with timestamp. Usually, certain activity detail is logged by the softwares or operating systems to help the developers or users to track what was happening at a certain time period. Users can edit these files very easily by using any text editors. Usually the error reports or login activities are logged by the operating systems, but other softwares or web servers also generate log files to track visitors and to monitor bandwidth usage.","title":"LOG"},{"location":"data/fileformats/#sqlite","text":"Documentation A file with .sqlite extension is a lightweight SQL database file created with the SQLite software. It is a database in a file itself and implements a self-contained, full-featured, highly-reliable SQL database engine. SQLite database files can be used to share rich contents between systems by simple exchanging these files over the network. Almost all mobiles and computers use SQLite for storing and sharing of data, and is the choice of file format for cross-platform applications. Due to its compact use and easy usability, it comes bundled inside other applications. SQLite bindings exist for programming languages such as C, C#, C++, Java, PHP, and many others. SQLite in reality is a C-Language library that implements the SQLite RDBMS using the SQLite file format. With the evolution of new devices every single day, its file format has been kept backwards compatible to accommodate older devices. SQLite file format is seen as long-term archival format for the data.","title":"SQLite"},{"location":"data/fileformats/#sql","text":"Documentation A file with .sql extension is a Structured Query Language (SQL) file that contains code to work with relational databases. It is used to write SQL statements for CRUD (Create, Read, Update, and Delete) operations on databases. SQL files are common while working with desktop as well as web-based databases. There are several alternatives to SQL such as Java Persistence Query Language (JPQL), LINQ, HTSQL, 4D QL, and several others. SQL files can be opened by query editors of Microsoft SQL Server, MySQL and other plain text editors such as Notepad on Windows OS. SQL files are in plain text format and can comprise of several language elements. Multiple statements can be added to a single SQL file if their execution is possible without depending on each other. These SQL commands can be executed by query editors for carrying out CRUD operations.","title":"SQL"},{"location":"data/fileformats/#feather","text":"Documentation Feather is a portable file format for storing Arrow tables or data frames (from languages like Python or R) that utilizes the Arrow IPC format internally. Feather was created early in the Arrow project as a proof of concept for fast, language-agnostic data frame storage for Python (pandas) and R. Additional Info","title":"Feather"},{"location":"data/fileformats/#parquet","text":"Documentation Apache Parquet has the following characteristics: Self-describing Columnar format Language-independent Self-describing data embeds the schema or structure with the data itself. Hadoop use cases drive the growth of self-describing data formats, such as Parquet and JSON, and of NoSQL databases, such as HBase. These formats and databases are well suited for the agile and iterative development cycle of Hadoop applications and BI/analytics. Optimized for working with large files, Parquet arranges data in columns, putting related values in close proximity to each other to optimize query performance, minimize I/O, and facilitate compression. Parquet detects and encodes the same or similar data using a technique that conserves resources. Additional Info","title":"Parquet"},{"location":"data/fileformats/#avro","text":"Documentation Apache Avro is a language-neutral data serialization system. Avro has a schema-based system. A language-independent schema is associated with its read and write operations. Avro serializes the data which has a built-in schema. Avro serializes the data into a compact binary format, which can be deserialized by any application. Avro uses JSON format to declare the data structures. Presently, it supports languages such as Java, C, C++, C#, Python, and Ruby.","title":"Avro"},{"location":"data/fileformats/#jsonl","text":"Documentation JSON Lines is a convenient format for storing structured data that may be processed one record at a time. It works well with unix-style text processing tools and shell pipelines. It's a great format for log files. It's also a flexible format for passing messages between cooperating processes. The JSON Lines format has three requirements: - UTF-8 Encoding - JSON allows encoding Unicode strings with only ASCII escape sequences, however those escapes will be hard to read when viewed in a text editor. - Each Line is a Valid JSON Value - The most common values will be objects or arrays, but any JSON value is permitted. - Line Separator is \\n - This means '\\r\\n' is also supported because surrounding white space is implicitly ignored when parsing JSON values. The last character in the file may be a line separator, and it will be treated the same as if there was no line separator present. Data as JSONL: { \"name\" : \"Gilbert\" , \"wins\" : [[ \"straight\" , \"7\u2663\" ], [ \"one pair\" , \"10\u2665\" ]]} { \"name\" : \"Alexa\" , \"wins\" : [[ \"two pair\" , \"4\u2660\" ], [ \"two pair\" , \"9\u2660\" ]]} { \"name\" : \"May\" , \"wins\" : []} { \"name\" : \"Deloise\" , \"wins\" : [[ \"three of a kind\" , \"5\u2663\" ]]}","title":"JSONL"},{"location":"data/sorting/","text":"Sorting Algorithms Sorting is a basic building block that many other algorithms are built upon. It\u2019s related to several exciting ideas that you\u2019ll see throughout your programming career. Understanding how sorting algorithms in Python work behind the scenes is a fundamental step toward implementing correct and efficient algorithms that solve real-world problems. Python\u2019s Built-In Sorting Algorithms sorted() Documentation Basic Usage numbers = [ 6 , 9 , 3 , 1 ] numbers_tuple = ( 6 , 9 , 3 , 1 ) numbers_set = { 5 , 5 , 10 , 1 , 0 } string_number_value = '34521' string_value = 'I like to sort' numbers_sorted = sorted ( numbers ) # numbers_sorted # [1, 3, 6, 9] numbers_tuple_sorted = sorted ( numbers_tuple ) # numbers_tuple_sorted # [1, 3, 6, 9] numbers_set_sorted = sorted ( numbers_set ) # numbers_set_sorted # [0, 1, 5, 10] sorted_string_number = sorted ( string_number_value ) # sorted_string_number # ['1', '2', '3', '4', '5'] sorted_string = sorted ( string_value ) # sorted_string # [' ', ' ', ' ', 'I', 'e', 'i', 'k', 'l', 'o', 'o', 'r', 's', 't', 't'] Bubble Sort Since this implementation sorts the array in ascending order, each step \u201cbubbles\u201d the largest element to the end of the array. This means that each iteration takes fewer steps than the previous iteration because a continuously larger portion of the array is sorted. def bubble_sort ( array ): n = len ( array ) for i in range ( n ): # Create a flag that will allow the function to # terminate early if there's nothing left to sort already_sorted = True # Start looking at each item of the list one by one, # comparing it with its adjacent value. With each # iteration, the portion of the array that you look at # shrinks because the remaining items have already been # sorted. for j in range ( n - i - 1 ): if array [ j ] > array [ j + 1 ]: # If the item you're looking at is greater than its # adjacent value, then swap them array [ j ], array [ j + 1 ] = array [ j + 1 ], array [ j ] # Since you had to swap two elements, # set the `already_sorted` flag to `False` so the # algorithm doesn't finish prematurely already_sorted = False # If there were no swaps during the last iteration, # the array is already sorted, and you can terminate if already_sorted : break return array Insertion Sort Like bubble sort, the insertion sort algorithm is straightforward to implement and understand. But unlike bubble sort, it builds the sorted list one element at a time by comparing each item with the rest of the list and inserting it into its correct position. This \u201cinsertion\u201d procedure gives the algorithm its name. An excellent analogy to explain insertion sort is the way you would sort a deck of cards. Imagine that you\u2019re holding a group of cards in your hands, and you want to arrange them in order. You\u2019d start by comparing a single card step by step with the rest of the cards until you find its correct position. At that point, you\u2019d insert the card in the correct location and start over with a new card, repeating until all the cards in your hand were sorted. def insertion_sort ( array ): # Loop from the second element of the array until # the last element for i in range ( 1 , len ( array )): # This is the element we want to position in its # correct place key_item = array [ i ] # Initialize the variable that will be used to # find the correct position of the element referenced # by `key_item` j = i - 1 # Run through the list of items (the left # portion of the array) and find the correct position # of the element referenced by `key_item`. Do this only # if `key_item` is smaller than its adjacent values. while j >= 0 and array [ j ] > key_item : # Shift the value one position to the left # and reposition j to point to the next element # (from right to left) array [ j + 1 ] = array [ j ] j -= 1 # When you finish shifting the elements, you can position # `key_item` in its correct location array [ j + 1 ] = key_item return array Merge Sort Merge sort is a very efficient sorting algorithm. It\u2019s based on the divide-and-conquer approach, a powerful algorithmic technique used to solve complex problems. To properly understand divide and conquer, you should first understand the concept of recursion. Recursion involves breaking a problem down into smaller subproblems until they\u2019re small enough to manage. In programming, recursion is usually expressed by a function calling itself. def merge ( left , right ): # If the first array is empty, then nothing needs # to be merged, and you can return the second array as the result if len ( left ) == 0 : return right # If the second array is empty, then nothing needs # to be merged, and you can return the first array as the result if len ( right ) == 0 : return left result = [] index_left = index_right = 0 # Now go through both arrays until all the elements # make it into the resultant array while len ( result ) < len ( left ) + len ( right ): # The elements need to be sorted to add them to the # resultant array, so you need to decide whether to get # the next element from the first or the second array if left [ index_left ] <= right [ index_right ]: result . append ( left [ index_left ]) index_left += 1 else : result . append ( right [ index_right ]) index_right += 1 # If you reach the end of either array, then you can # add the remaining elements from the other array to # the result and break the loop if index_right == len ( right ): result += left [ index_left :] break if index_left == len ( left ): result += right [ index_right :] break return result QuickSort Just like merge sort, the Quicksort algorithm applies the divide-and-conquer principle to divide the input array into two lists, the first with small items and the second with large items. The algorithm then sorts both lists recursively until the resultant list is completely sorted. Dividing the input list is referred to as partitioning the list. Quicksort first selects a pivot element and partitions the list around the pivot, putting every smaller element into a low array and every larger element into a high array. Putting every element from the low list to the left of the pivot and every element from the high list to the right positions the pivot precisely where it needs to be in the final sorted list. This means that the function can now recursively apply the same procedure to low and then high until the entire list is sorted. from random import randint def quicksort ( array ): # If the input array contains fewer than two elements, # then return it as the result of the function if len ( array ) < 2 : return array low , same , high = [], [], [] # Select your `pivot` element randomly pivot = array [ randint ( 0 , len ( array ) - 1 )] for item in array : # Elements that are smaller than the `pivot` go to # the `low` list. Elements that are larger than # `pivot` go to the `high` list. Elements that are # equal to `pivot` go to the `same` list. if item < pivot : low . append ( item ) elif item == pivot : same . append ( item ) elif item > pivot : high . append ( item ) # The final result combines the sorted `low` list # with the `same` list and the sorted `high` list return quicksort ( low ) + same + quicksort ( high ) Timsort The Timsort algorithm is considered a hybrid sorting algorithm because it employs a best-of-both-worlds combination of insertion sort and merge sort. Timsort is near and dear to the Python community because it was created by Tim Peters in 2002 to be used as the standard sorting algorithm of the Python language. The main characteristic of Timsort is that it takes advantage of already-sorted elements that exist in most real-world datasets. These are called natural runs. The algorithm then iterates over the list, collecting the elements into runs and merging them into a single sorted list. def insertion_sort ( array , left = 0 , right = None ): if right is None : right = len ( array ) - 1 # Loop from the element indicated by # `left` until the element indicated by `right` for i in range ( left + 1 , right + 1 ): # This is the element we want to position in its # correct place key_item = array [ i ] # Initialize the variable that will be used to # find the correct position of the element referenced # by `key_item` j = i - 1 # Run through the list of items (the left # portion of the array) and find the correct position # of the element referenced by `key_item`. Do this only # if the `key_item` is smaller than its adjacent values. while j >= left and array [ j ] > key_item : # Shift the value one position to the left # and reposition `j` to point to the next element # (from right to left) array [ j + 1 ] = array [ j ] j -= 1 # When you finish shifting the elements, position # the `key_item` in its correct location array [ j + 1 ] = key_item return array","title":"Sorting"},{"location":"data/sorting/#sorting-algorithms","text":"Sorting is a basic building block that many other algorithms are built upon. It\u2019s related to several exciting ideas that you\u2019ll see throughout your programming career. Understanding how sorting algorithms in Python work behind the scenes is a fundamental step toward implementing correct and efficient algorithms that solve real-world problems.","title":"Sorting Algorithms"},{"location":"data/sorting/#pythons-built-in-sorting-algorithms","text":"","title":"Python\u2019s Built-In Sorting Algorithms"},{"location":"data/sorting/#sorted","text":"Documentation","title":"sorted()"},{"location":"data/sorting/#basic-usage","text":"numbers = [ 6 , 9 , 3 , 1 ] numbers_tuple = ( 6 , 9 , 3 , 1 ) numbers_set = { 5 , 5 , 10 , 1 , 0 } string_number_value = '34521' string_value = 'I like to sort' numbers_sorted = sorted ( numbers ) # numbers_sorted # [1, 3, 6, 9] numbers_tuple_sorted = sorted ( numbers_tuple ) # numbers_tuple_sorted # [1, 3, 6, 9] numbers_set_sorted = sorted ( numbers_set ) # numbers_set_sorted # [0, 1, 5, 10] sorted_string_number = sorted ( string_number_value ) # sorted_string_number # ['1', '2', '3', '4', '5'] sorted_string = sorted ( string_value ) # sorted_string # [' ', ' ', ' ', 'I', 'e', 'i', 'k', 'l', 'o', 'o', 'r', 's', 't', 't']","title":"Basic Usage"},{"location":"data/sorting/#bubble-sort","text":"Since this implementation sorts the array in ascending order, each step \u201cbubbles\u201d the largest element to the end of the array. This means that each iteration takes fewer steps than the previous iteration because a continuously larger portion of the array is sorted. def bubble_sort ( array ): n = len ( array ) for i in range ( n ): # Create a flag that will allow the function to # terminate early if there's nothing left to sort already_sorted = True # Start looking at each item of the list one by one, # comparing it with its adjacent value. With each # iteration, the portion of the array that you look at # shrinks because the remaining items have already been # sorted. for j in range ( n - i - 1 ): if array [ j ] > array [ j + 1 ]: # If the item you're looking at is greater than its # adjacent value, then swap them array [ j ], array [ j + 1 ] = array [ j + 1 ], array [ j ] # Since you had to swap two elements, # set the `already_sorted` flag to `False` so the # algorithm doesn't finish prematurely already_sorted = False # If there were no swaps during the last iteration, # the array is already sorted, and you can terminate if already_sorted : break return array","title":"Bubble Sort"},{"location":"data/sorting/#insertion-sort","text":"Like bubble sort, the insertion sort algorithm is straightforward to implement and understand. But unlike bubble sort, it builds the sorted list one element at a time by comparing each item with the rest of the list and inserting it into its correct position. This \u201cinsertion\u201d procedure gives the algorithm its name. An excellent analogy to explain insertion sort is the way you would sort a deck of cards. Imagine that you\u2019re holding a group of cards in your hands, and you want to arrange them in order. You\u2019d start by comparing a single card step by step with the rest of the cards until you find its correct position. At that point, you\u2019d insert the card in the correct location and start over with a new card, repeating until all the cards in your hand were sorted. def insertion_sort ( array ): # Loop from the second element of the array until # the last element for i in range ( 1 , len ( array )): # This is the element we want to position in its # correct place key_item = array [ i ] # Initialize the variable that will be used to # find the correct position of the element referenced # by `key_item` j = i - 1 # Run through the list of items (the left # portion of the array) and find the correct position # of the element referenced by `key_item`. Do this only # if `key_item` is smaller than its adjacent values. while j >= 0 and array [ j ] > key_item : # Shift the value one position to the left # and reposition j to point to the next element # (from right to left) array [ j + 1 ] = array [ j ] j -= 1 # When you finish shifting the elements, you can position # `key_item` in its correct location array [ j + 1 ] = key_item return array","title":"Insertion Sort"},{"location":"data/sorting/#merge-sort","text":"Merge sort is a very efficient sorting algorithm. It\u2019s based on the divide-and-conquer approach, a powerful algorithmic technique used to solve complex problems. To properly understand divide and conquer, you should first understand the concept of recursion. Recursion involves breaking a problem down into smaller subproblems until they\u2019re small enough to manage. In programming, recursion is usually expressed by a function calling itself. def merge ( left , right ): # If the first array is empty, then nothing needs # to be merged, and you can return the second array as the result if len ( left ) == 0 : return right # If the second array is empty, then nothing needs # to be merged, and you can return the first array as the result if len ( right ) == 0 : return left result = [] index_left = index_right = 0 # Now go through both arrays until all the elements # make it into the resultant array while len ( result ) < len ( left ) + len ( right ): # The elements need to be sorted to add them to the # resultant array, so you need to decide whether to get # the next element from the first or the second array if left [ index_left ] <= right [ index_right ]: result . append ( left [ index_left ]) index_left += 1 else : result . append ( right [ index_right ]) index_right += 1 # If you reach the end of either array, then you can # add the remaining elements from the other array to # the result and break the loop if index_right == len ( right ): result += left [ index_left :] break if index_left == len ( left ): result += right [ index_right :] break return result","title":"Merge Sort"},{"location":"data/sorting/#quicksort","text":"Just like merge sort, the Quicksort algorithm applies the divide-and-conquer principle to divide the input array into two lists, the first with small items and the second with large items. The algorithm then sorts both lists recursively until the resultant list is completely sorted. Dividing the input list is referred to as partitioning the list. Quicksort first selects a pivot element and partitions the list around the pivot, putting every smaller element into a low array and every larger element into a high array. Putting every element from the low list to the left of the pivot and every element from the high list to the right positions the pivot precisely where it needs to be in the final sorted list. This means that the function can now recursively apply the same procedure to low and then high until the entire list is sorted. from random import randint def quicksort ( array ): # If the input array contains fewer than two elements, # then return it as the result of the function if len ( array ) < 2 : return array low , same , high = [], [], [] # Select your `pivot` element randomly pivot = array [ randint ( 0 , len ( array ) - 1 )] for item in array : # Elements that are smaller than the `pivot` go to # the `low` list. Elements that are larger than # `pivot` go to the `high` list. Elements that are # equal to `pivot` go to the `same` list. if item < pivot : low . append ( item ) elif item == pivot : same . append ( item ) elif item > pivot : high . append ( item ) # The final result combines the sorted `low` list # with the `same` list and the sorted `high` list return quicksort ( low ) + same + quicksort ( high )","title":"QuickSort"},{"location":"data/sorting/#timsort","text":"The Timsort algorithm is considered a hybrid sorting algorithm because it employs a best-of-both-worlds combination of insertion sort and merge sort. Timsort is near and dear to the Python community because it was created by Tim Peters in 2002 to be used as the standard sorting algorithm of the Python language. The main characteristic of Timsort is that it takes advantage of already-sorted elements that exist in most real-world datasets. These are called natural runs. The algorithm then iterates over the list, collecting the elements into runs and merging them into a single sorted list. def insertion_sort ( array , left = 0 , right = None ): if right is None : right = len ( array ) - 1 # Loop from the element indicated by # `left` until the element indicated by `right` for i in range ( left + 1 , right + 1 ): # This is the element we want to position in its # correct place key_item = array [ i ] # Initialize the variable that will be used to # find the correct position of the element referenced # by `key_item` j = i - 1 # Run through the list of items (the left # portion of the array) and find the correct position # of the element referenced by `key_item`. Do this only # if the `key_item` is smaller than its adjacent values. while j >= left and array [ j ] > key_item : # Shift the value one position to the left # and reposition `j` to point to the next element # (from right to left) array [ j + 1 ] = array [ j ] j -= 1 # When you finish shifting the elements, position # the `key_item` in its correct location array [ j + 1 ] = key_item return array","title":"Timsort"},{"location":"datastorage/datastorage/","text":"Data Storage Index Empty for now...","title":"Data Storage Index"},{"location":"datastorage/datastorage/#data-storage-index","text":"Empty for now...","title":"Data Storage Index"},{"location":"etl/extract/","text":"ETL (Extract) Extract Transform Load is the process whereby some data is obtained, (extracted) cleaned, wrangled (transformed), and placed into a user-friendly data structure like a data frame (loaded). Extraction involves using some tool to pull data from a source, most commonly with API and webpages will involve using the Requests package Webpages (HTML) BeautifulSoup Documentation Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work. from bs4 import BeautifulSoup as bs html_doc = \"\"\" <html><head><title>BeautifulSoup Demo</title></head> <body> <p class=\"title\"><b>BS4 Demo title</b></p> <p class=\"story\">...</p> </body> </html> \"\"\" soup = bs ( html_doc , 'html.parser' ) When pulling from Requests you can use: from bs4 import BeautifulSoup as bs import requests response = requests . get ( \"http://some-url.com\" ) soup = bs ( response . content , 'html.parser' ) MechanicalSoup Documentation A Python library for automating interaction with websites. MechanicalSoup automatically stores and sends cookies, follows redirects, and can follow links and submit forms. It doesn\u2019t do Javascript. \"\"\" Example usage of MechanicalSoup to get the results from DuckDuckGo. \"\"\" import mechanicalsoup # Connect to duckduckgo browser = mechanicalsoup . StatefulBrowser ( user_agent = \"MechanicalSoup\" ) # Need to use the non JS version of DDG since Python can't render JS browser . open ( \"https://html.duckduckgo.com/html/\" ) # methods available to browser # browser.absolute_url( browser.get_cookiejar( browser.links( browser.put( browser.set_user_agent( # browser.add_soup( browser.get_current_form( browser.list_links( browser.raise_on_404 browser.set_verbose( # browser.close( browser.get_current_page( browser.new_control( browser.refresh( browser.soup_config # browser.download_link( browser.get_debug( browser.open( browser.request( browser.submit( # browser.find_link( browser.get_request_kwargs( browser.open_fake_page( browser.select_form( browser.submit_selected( # browser.follow_link( browser.get_url( browser.open_relative( browser.session browser.url # browser.form browser.get_verbose( browser.page( browser.set_cookiejar( # browser.get( browser.launch_browser( browser.post( browser.set_debug( # Fill-in the search form browser . select_form ( '#search_form_homepage' ) # this will open a browser to show the HTML currently selected in the browser object browser . launch_browser () browser [ \"q\" ] = \"MechanicalSoup\" browser . submit_selected () # Display the results for link in browser . page . select ( 'a.result__a' ): print ( link . text , '->' , link . attrs [ 'href' ]) Selenium Documentation Selenium Python bindings provides a simple API to write functional/acceptance tests using Selenium WebDriver. Through Selenium Python API you can access all functionalities of Selenium WebDriver in an intuitive way. from selenium import webdriver from selenium.webdriver.common.keys import Keys # The instance of Firefox WebDriver is created driver = webdriver . Firefox () # The driver.get method will navigate to a page given by the URL. # WebDriver will wait until the page has fully loaded (the \u201conload\u201d event has fired) # before returning control to your test or script driver . get ( \"http://www.python.org\" ) # assertion to confirm that title has \u201cPython\u201d word in it assert \"Python\" in driver . title # WebDriver offers a number of ways to find elements using one of the find_element_by_* methods. elem = driver . find_element_by_name ( \"q\" ) # we\u2019ll first clear any pre-populated text in the input field elem . clear () # we are sending keys, this is similar to entering keys using your keyboard elem . send_keys ( \"pycon\" ) elem . send_keys ( Keys . RETURN ) # ensure that some results are found, make an assertion assert \"No results found.\" not in driver . page_source # The driver.quit() will exit entire browser whereas drive.close() will close one tab driver . close () Some additional WebDrivers for Selenium include the following: webdriver.Firefox webdriver.FirefoxProfile webdriver.Chrome webdriver.ChromeOptions webdriver.Ie webdriver.Opera webdriver.PhantomJS webdriver.Remote webdriver.DesiredCapabilities webdriver.ActionChains webdriver.TouchActions webdriver.Proxy To use without actually opening a window and save CPU use Chrome driver in --headless mode: from selenium import webdriver from selenium.webdriver.chrome.options import Options # instantiate a chrome options object so you can set the size and headless preference chrome_options = Options () chrome_options . add_argument ( \"--headless\" ) APIs Databases SQLite3 Documentation import sqlite3 conn = sqlite3 . connect ( 'test_database' ) c = conn . cursor () c . execute ( ''' CREATE TABLE IF NOT EXISTS products ([product_id] INTEGER PRIMARY KEY, [product_name] TEXT, [price] INTEGER) ''' ) c . execute ( ''' INSERT INTO products (product_id, product_name, price) VALUES (1,'Computer',800), (2,'Printer',200), (3,'Tablet',300), (4,'Desk',450), (5,'Chair',150) ''' ) conn . commit () SQLAlchemy Documentation Viewing Tables import sqlalchemy as db engine = db . create_engine ( 'sqlite:///census.sqlite' ) connection = engine . connect () metadata = db . MetaData () census = db . Table ( 'census' , metadata , autoload = True , autoload_with = engine ) Querying Tables #Equivalent to 'SELECT * FROM census' query = db . select ([ census ]) # ResultProxy: The object returned by the .execute() method. # It can be used in a variety of ways to get the data returned by the query. ResultProxy = connection . execute ( query ) # ResultSet: The actual data asked for in the query when using # a fetch method such as .fetchall() on a ResultProxy. ResultSet = ResultProxy . fetchall () Additional Querying tutorial SQLAlchemy \u2014 Python Tutorial Pandas Documentation # import the modules import pandas as pd from sqlalchemy import create_engine # SQLAlchemy connectable cnx = create_engine ( 'sqlite:///contacts.db' ) . connect () # table named 'contacts' will be returned as a dataframe. df = pd . read_sql_table ( 'contacts' , cnx ) print ( df ) Flat Files Python Python has the ability to read many files when given a path. Using with open will close the file when the with statement closes. More parameters on reading files can be found here Real Python Tutorial with open ( 'dog_breeds.txt' , 'r' ) as reader : # Further file processing goes here additional parameters include: Character Meaning 'r' Open for reading (default) 'w' Open for writing, truncating (overwriting) the file first 'rb' or wb' Open in binary mode (read/write using byte data) Pandas Documentation Pandas can read many types of flat files including csv , parquet , xlsx , txt , pickle , clipboard , xml , html , json and others. import pandas as pd # Reading CSV df = pd . read_csv ( \"path/to/file.csv\" ) # Reading CSV Faster with Pyarrow in Pandas 1.4 df = pd . read_csv ( \"large.csv\" , engine = \"pyarrow\" ) # Reading Parquet file df = pd . read_parquet ( \"large.parquet\" ) # Reading Parquet file with faster parquet engine df = pd . read_parquet ( \"large.parquet\" , engine = \"fastparquet\" ) # Reading Excel df = pd . read_excel ( \"path/to/file.xlsx\" ) # Reading Excel with multiple sheets xls = pd . ExcelFile ( 'path_to_file.xls' ) df1 = pd . read_excel ( xls , 'Sheet1' ) df2 = pd . read_excel ( xls , 'Sheet2' ) # Reading JSON df = pd . read_json ( \"path/to/file.json\" ) # nested JSON often leaves the df in an undesirable state df = pd . json_normalize ( df [ 'data' ]) # this will flatten lists into columns df . explode ( 'col_of_lists' ) # Reading JSONL df = pd . read_json ( \"path/to/file.jsonl\" , lines = True )","title":"Extract"},{"location":"etl/extract/#etl-extract","text":"Extract Transform Load is the process whereby some data is obtained, (extracted) cleaned, wrangled (transformed), and placed into a user-friendly data structure like a data frame (loaded). Extraction involves using some tool to pull data from a source, most commonly with API and webpages will involve using the Requests package","title":"ETL (Extract)"},{"location":"etl/extract/#webpages-html","text":"","title":"Webpages (HTML)"},{"location":"etl/extract/#beautifulsoup","text":"Documentation Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work. from bs4 import BeautifulSoup as bs html_doc = \"\"\" <html><head><title>BeautifulSoup Demo</title></head> <body> <p class=\"title\"><b>BS4 Demo title</b></p> <p class=\"story\">...</p> </body> </html> \"\"\" soup = bs ( html_doc , 'html.parser' ) When pulling from Requests you can use: from bs4 import BeautifulSoup as bs import requests response = requests . get ( \"http://some-url.com\" ) soup = bs ( response . content , 'html.parser' )","title":"BeautifulSoup"},{"location":"etl/extract/#mechanicalsoup","text":"Documentation A Python library for automating interaction with websites. MechanicalSoup automatically stores and sends cookies, follows redirects, and can follow links and submit forms. It doesn\u2019t do Javascript. \"\"\" Example usage of MechanicalSoup to get the results from DuckDuckGo. \"\"\" import mechanicalsoup # Connect to duckduckgo browser = mechanicalsoup . StatefulBrowser ( user_agent = \"MechanicalSoup\" ) # Need to use the non JS version of DDG since Python can't render JS browser . open ( \"https://html.duckduckgo.com/html/\" ) # methods available to browser # browser.absolute_url( browser.get_cookiejar( browser.links( browser.put( browser.set_user_agent( # browser.add_soup( browser.get_current_form( browser.list_links( browser.raise_on_404 browser.set_verbose( # browser.close( browser.get_current_page( browser.new_control( browser.refresh( browser.soup_config # browser.download_link( browser.get_debug( browser.open( browser.request( browser.submit( # browser.find_link( browser.get_request_kwargs( browser.open_fake_page( browser.select_form( browser.submit_selected( # browser.follow_link( browser.get_url( browser.open_relative( browser.session browser.url # browser.form browser.get_verbose( browser.page( browser.set_cookiejar( # browser.get( browser.launch_browser( browser.post( browser.set_debug( # Fill-in the search form browser . select_form ( '#search_form_homepage' ) # this will open a browser to show the HTML currently selected in the browser object browser . launch_browser () browser [ \"q\" ] = \"MechanicalSoup\" browser . submit_selected () # Display the results for link in browser . page . select ( 'a.result__a' ): print ( link . text , '->' , link . attrs [ 'href' ])","title":"MechanicalSoup"},{"location":"etl/extract/#selenium","text":"Documentation Selenium Python bindings provides a simple API to write functional/acceptance tests using Selenium WebDriver. Through Selenium Python API you can access all functionalities of Selenium WebDriver in an intuitive way. from selenium import webdriver from selenium.webdriver.common.keys import Keys # The instance of Firefox WebDriver is created driver = webdriver . Firefox () # The driver.get method will navigate to a page given by the URL. # WebDriver will wait until the page has fully loaded (the \u201conload\u201d event has fired) # before returning control to your test or script driver . get ( \"http://www.python.org\" ) # assertion to confirm that title has \u201cPython\u201d word in it assert \"Python\" in driver . title # WebDriver offers a number of ways to find elements using one of the find_element_by_* methods. elem = driver . find_element_by_name ( \"q\" ) # we\u2019ll first clear any pre-populated text in the input field elem . clear () # we are sending keys, this is similar to entering keys using your keyboard elem . send_keys ( \"pycon\" ) elem . send_keys ( Keys . RETURN ) # ensure that some results are found, make an assertion assert \"No results found.\" not in driver . page_source # The driver.quit() will exit entire browser whereas drive.close() will close one tab driver . close () Some additional WebDrivers for Selenium include the following: webdriver.Firefox webdriver.FirefoxProfile webdriver.Chrome webdriver.ChromeOptions webdriver.Ie webdriver.Opera webdriver.PhantomJS webdriver.Remote webdriver.DesiredCapabilities webdriver.ActionChains webdriver.TouchActions webdriver.Proxy To use without actually opening a window and save CPU use Chrome driver in --headless mode: from selenium import webdriver from selenium.webdriver.chrome.options import Options # instantiate a chrome options object so you can set the size and headless preference chrome_options = Options () chrome_options . add_argument ( \"--headless\" )","title":"Selenium"},{"location":"etl/extract/#apis","text":"","title":"APIs"},{"location":"etl/extract/#databases","text":"","title":"Databases"},{"location":"etl/extract/#sqlite3","text":"Documentation import sqlite3 conn = sqlite3 . connect ( 'test_database' ) c = conn . cursor () c . execute ( ''' CREATE TABLE IF NOT EXISTS products ([product_id] INTEGER PRIMARY KEY, [product_name] TEXT, [price] INTEGER) ''' ) c . execute ( ''' INSERT INTO products (product_id, product_name, price) VALUES (1,'Computer',800), (2,'Printer',200), (3,'Tablet',300), (4,'Desk',450), (5,'Chair',150) ''' ) conn . commit ()","title":"SQLite3"},{"location":"etl/extract/#sqlalchemy","text":"Documentation","title":"SQLAlchemy"},{"location":"etl/extract/#viewing-tables","text":"import sqlalchemy as db engine = db . create_engine ( 'sqlite:///census.sqlite' ) connection = engine . connect () metadata = db . MetaData () census = db . Table ( 'census' , metadata , autoload = True , autoload_with = engine )","title":"Viewing Tables"},{"location":"etl/extract/#querying-tables","text":"#Equivalent to 'SELECT * FROM census' query = db . select ([ census ]) # ResultProxy: The object returned by the .execute() method. # It can be used in a variety of ways to get the data returned by the query. ResultProxy = connection . execute ( query ) # ResultSet: The actual data asked for in the query when using # a fetch method such as .fetchall() on a ResultProxy. ResultSet = ResultProxy . fetchall () Additional Querying tutorial SQLAlchemy \u2014 Python Tutorial","title":"Querying Tables"},{"location":"etl/extract/#pandas","text":"Documentation # import the modules import pandas as pd from sqlalchemy import create_engine # SQLAlchemy connectable cnx = create_engine ( 'sqlite:///contacts.db' ) . connect () # table named 'contacts' will be returned as a dataframe. df = pd . read_sql_table ( 'contacts' , cnx ) print ( df )","title":"Pandas"},{"location":"etl/extract/#flat-files","text":"","title":"Flat Files"},{"location":"etl/extract/#python","text":"Python has the ability to read many files when given a path. Using with open will close the file when the with statement closes. More parameters on reading files can be found here Real Python Tutorial with open ( 'dog_breeds.txt' , 'r' ) as reader : # Further file processing goes here additional parameters include: Character Meaning 'r' Open for reading (default) 'w' Open for writing, truncating (overwriting) the file first 'rb' or wb' Open in binary mode (read/write using byte data)","title":"Python"},{"location":"etl/extract/#pandas_1","text":"Documentation Pandas can read many types of flat files including csv , parquet , xlsx , txt , pickle , clipboard , xml , html , json and others. import pandas as pd # Reading CSV df = pd . read_csv ( \"path/to/file.csv\" ) # Reading CSV Faster with Pyarrow in Pandas 1.4 df = pd . read_csv ( \"large.csv\" , engine = \"pyarrow\" ) # Reading Parquet file df = pd . read_parquet ( \"large.parquet\" ) # Reading Parquet file with faster parquet engine df = pd . read_parquet ( \"large.parquet\" , engine = \"fastparquet\" ) # Reading Excel df = pd . read_excel ( \"path/to/file.xlsx\" ) # Reading Excel with multiple sheets xls = pd . ExcelFile ( 'path_to_file.xls' ) df1 = pd . read_excel ( xls , 'Sheet1' ) df2 = pd . read_excel ( xls , 'Sheet2' ) # Reading JSON df = pd . read_json ( \"path/to/file.json\" ) # nested JSON often leaves the df in an undesirable state df = pd . json_normalize ( df [ 'data' ]) # this will flatten lists into columns df . explode ( 'col_of_lists' ) # Reading JSONL df = pd . read_json ( \"path/to/file.jsonl\" , lines = True )","title":"Pandas"},{"location":"etl/load/","text":"ETL (Load) Flat Files Python Print Statement Documentation Give print a file keyword argument, where the value of the argument is a file stream. We can create a file stream using the open function # use file=open() to direct the output to the file location # using \"a\" appends or creates the file print ( \"Hello World!\" , file = open ( \"output.txt\" , \"a\" )) some_text = \"This is a string of something I want to save\" print ( some_text , file = open ( \"output.txt\" , \"a\" ))","title":"Load"},{"location":"etl/load/#etl-load","text":"","title":"ETL (Load)"},{"location":"etl/load/#flat-files","text":"","title":"Flat Files"},{"location":"etl/load/#python-print-statement","text":"Documentation Give print a file keyword argument, where the value of the argument is a file stream. We can create a file stream using the open function # use file=open() to direct the output to the file location # using \"a\" appends or creates the file print ( \"Hello World!\" , file = open ( \"output.txt\" , \"a\" )) some_text = \"This is a string of something I want to save\" print ( some_text , file = open ( \"output.txt\" , \"a\" ))","title":"Python Print Statement"},{"location":"etl/pipelines/","text":"ETL Pipelines An ETL pipeline is the set of processes used to move data from a source or multiple sources into a database such as a data warehouse. ETL stands for \u201cextract, transform, load,\u201d the three interdependent processes of data integration used to pull data from one database and move it to another. Once loaded, data can be used for reporting, analysis, and deriving actionable business insights. Luigi Documentation Luigi is a Python (2.7, 3.6, 3.7 tested) package that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization, handling failures, command line integration, and much more. \"\"\" You can run this example like this: .. code:: console $ luigi --module examples.hello_world examples.HelloWorldTask --local-scheduler If that does not work, see :ref:`CommandLine`. \"\"\" import luigi class HelloWorldTask ( luigi . Task ): task_namespace = 'examples' def run ( self ): print ( \" {task} says: Hello world!\" . format ( task = self . __class__ . __name__ )) if __name__ == '__main__' : luigi . run ([ 'examples.HelloWorldTask' , '--workers' , '1' , '--local-scheduler' ]) Bonobo Documentation Bonobo is a lightweight Extract-Transform-Load (ETL) framework for Python 3.5+. It provides tools for building data transformation pipelines, using plain python primitives, and executing them in parallel. Bonobo is the swiss army knife for everyday's data. import datetime import time import bonobo def extract (): \"\"\"Placeholder, change, rename, remove... \"\"\" for x in range ( 60 ): if x : time . sleep ( 1 ) yield datetime . datetime . now () def get_graph (): graph = bonobo . Graph () graph . add_chain ( extract , print ) return graph if __name__ == \"__main__\" : parser = bonobo . get_argument_parser () with bonobo . parse_args ( parser ): bonobo . run ( get_graph ()) Bonobo Documentation Bonobo is a lightweight Extract-Transform-Load (ETL) framework for Python 3.5+. It provides tools for building data transformation pipelines, using plain python primitives, and executing them in parallel. Bonobo is the swiss army knife for everyday's data. import datetime import time import bonobo def extract (): \"\"\"Placeholder, change, rename, remove... \"\"\" for x in range ( 60 ): if x : time . sleep ( 1 ) yield datetime . datetime . now () def get_graph (): graph = bonobo . Graph () graph . add_chain ( extract , print ) return graph if __name__ == \"__main__\" : parser = bonobo . get_argument_parser () with bonobo . parse_args ( parser ): bonobo . run ( get_graph ())","title":"Pipelines"},{"location":"etl/pipelines/#etl-pipelines","text":"An ETL pipeline is the set of processes used to move data from a source or multiple sources into a database such as a data warehouse. ETL stands for \u201cextract, transform, load,\u201d the three interdependent processes of data integration used to pull data from one database and move it to another. Once loaded, data can be used for reporting, analysis, and deriving actionable business insights.","title":"ETL Pipelines"},{"location":"etl/pipelines/#luigi","text":"Documentation Luigi is a Python (2.7, 3.6, 3.7 tested) package that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization, handling failures, command line integration, and much more. \"\"\" You can run this example like this: .. code:: console $ luigi --module examples.hello_world examples.HelloWorldTask --local-scheduler If that does not work, see :ref:`CommandLine`. \"\"\" import luigi class HelloWorldTask ( luigi . Task ): task_namespace = 'examples' def run ( self ): print ( \" {task} says: Hello world!\" . format ( task = self . __class__ . __name__ )) if __name__ == '__main__' : luigi . run ([ 'examples.HelloWorldTask' , '--workers' , '1' , '--local-scheduler' ])","title":"Luigi"},{"location":"etl/pipelines/#bonobo","text":"Documentation Bonobo is a lightweight Extract-Transform-Load (ETL) framework for Python 3.5+. It provides tools for building data transformation pipelines, using plain python primitives, and executing them in parallel. Bonobo is the swiss army knife for everyday's data. import datetime import time import bonobo def extract (): \"\"\"Placeholder, change, rename, remove... \"\"\" for x in range ( 60 ): if x : time . sleep ( 1 ) yield datetime . datetime . now () def get_graph (): graph = bonobo . Graph () graph . add_chain ( extract , print ) return graph if __name__ == \"__main__\" : parser = bonobo . get_argument_parser () with bonobo . parse_args ( parser ): bonobo . run ( get_graph ())","title":"Bonobo"},{"location":"etl/pipelines/#bonobo_1","text":"Documentation Bonobo is a lightweight Extract-Transform-Load (ETL) framework for Python 3.5+. It provides tools for building data transformation pipelines, using plain python primitives, and executing them in parallel. Bonobo is the swiss army knife for everyday's data. import datetime import time import bonobo def extract (): \"\"\"Placeholder, change, rename, remove... \"\"\" for x in range ( 60 ): if x : time . sleep ( 1 ) yield datetime . datetime . now () def get_graph (): graph = bonobo . Graph () graph . add_chain ( extract , print ) return graph if __name__ == \"__main__\" : parser = bonobo . get_argument_parser () with bonobo . parse_args ( parser ): bonobo . run ( get_graph ())","title":"Bonobo"},{"location":"etl/transform/","text":"ETL (Transform) Extract Transform Load is the process whereby some data is obtained, (extracted) cleaned, wrangled (transformed), and placed into a user-friendly data structure like a data frame (loaded). Transforming is Pandas JSON Documentation Often JSON files directly translate to pd.DataFrame but nested JSON usually leave the JSON array in a single column import json import pandas as pd from pandas.io.json import json_normalize #package for flattening json in pd df = pd . DataFrame ( json_file ) works_data = json_normalize ( data = df [ 'programs' ], record_path = 'works' , meta = [ 'id' , 'orchestra' , 'programID' , 'season' ]) # record_path is the column name you want to \"flatten\" # by that it makes JSON keys as a new # column name and places the value as the row value. # Also pass the parent metadata we wanted to append JSONL Documentation Pandas can work with JSONL as well import json import pandas as pd df = pd . read_json ( jsonl_file , lines = True ) # using lines=True lets Pandas read each line as a valid JSON file Additional Info","title":"Transform"},{"location":"etl/transform/#etl-transform","text":"Extract Transform Load is the process whereby some data is obtained, (extracted) cleaned, wrangled (transformed), and placed into a user-friendly data structure like a data frame (loaded). Transforming is","title":"ETL (Transform)"},{"location":"etl/transform/#pandas","text":"","title":"Pandas"},{"location":"etl/transform/#json","text":"Documentation Often JSON files directly translate to pd.DataFrame but nested JSON usually leave the JSON array in a single column import json import pandas as pd from pandas.io.json import json_normalize #package for flattening json in pd df = pd . DataFrame ( json_file ) works_data = json_normalize ( data = df [ 'programs' ], record_path = 'works' , meta = [ 'id' , 'orchestra' , 'programID' , 'season' ]) # record_path is the column name you want to \"flatten\" # by that it makes JSON keys as a new # column name and places the value as the row value. # Also pass the parent metadata we wanted to append","title":"JSON"},{"location":"etl/transform/#jsonl","text":"Documentation Pandas can work with JSONL as well import json import pandas as pd df = pd . read_json ( jsonl_file , lines = True ) # using lines=True lets Pandas read each line as a valid JSON file Additional Info","title":"JSONL"},{"location":"ml/ml/","text":"Machine Learning Definition Anomoly Detection Isolation Forests Documentation Recall that decision trees are built using information criteria such as Gini index or entropy. The obviously different groups are separated at the root of the tree and deeper into the branches, the subtler distinctions are identified. Based on randomly picked characteristics, an isolation forest processes the randomly subsampled data in a tree structure. Samples that reach further into the tree and require more cuts to separate them have a very little probability that they are anomalies. Likewise, samples that are found on the shorter branches of the tree are more likely to be anomalies, since the tree found it simpler to distinguish them from the other data. from sklearn.datasets import make_blobs from numpy import quantile , random , where from sklearn.ensemble import IsolationForest import matplotlib.pyplot as plt IF = IsolationForest ( n_estimators = 100 , contamination =. 03 ) predictions = IF . fit_predict ( X ) outlier_index = where ( predictions ==- 1 ) values = X [ outlier_index ] plt . scatter ( X [:, 0 ], X [:, 1 ]) plt . scatter ( values [:, 0 ], values [:, 1 ], color = 'y' ) plt . show () More Info Sqliteviz Sqliteviz is a single-page offline-first PWA for fully client-side visualisation of SQLite databases or CSV files. With sqliteviz you can: - run SQL queries against a SQLite database and create Plotly charts and pivot tables based on the result sets - import a CSV file into a SQLite database and visualize imported data - export result set to CSV file - manage inquiries and run them against different databases - import/export inquiries from/to a JSON file - export a modified SQLite database - use it offline from your OS application menu like any other desktop app Documentation Examples Use the Live App Streamlit Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes you can build and deploy powerful data apps. Documentation Examples import streamlit as st import pandas as pd import numpy as np st . title ( 'Uber pickups in NYC' ) DATE_COLUMN = 'date/time' DATA_URL = ( 'https://s3-us-west-2.amazonaws.com/' 'streamlit-demo-data/uber-raw-data-sep14.csv.gz' ) def load_data ( nrows ): data = pd . read_csv ( DATA_URL , nrows = nrows ) data [ DATE_COLUMN ] = pd . to_datetime ( data [ DATE_COLUMN ]) return data data = load_data ( 10000 ) Create a bar chart hist_values = np . histogram ( data [ DATE_COLUMN ] . dt . hour , bins = 24 , range = ( 0 , 24 ) )[ 0 ] st . bar_chart ( hist_values ) Plot data on a map with sliding filter hour_to_filter = st . slider ( 'hour' , 0 , 23 , 17 ) # min: 0h, max: 23h, default: 17h filtered_data = data [ data [ DATE_COLUMN ] . dt . hour == hour_to_filter ] st . subheader ( f 'Map of all pickups at { hour_to_filter } :00' ) st . map ( filtered_data ) Use the Live App LightDash Connect Lightdash to your dbt project, add metrics directly in your data transformation layer, then create and share your insights with your team. Documentation Examples Use the Live Demo App Apache Superset Superset is fast, lightweight, intuitive, and loaded with options that make it easy for users of all skill sets to explore and visualize their data, from simple line charts to highly detailed geospatial charts. Documentation Examples","title":"ML"},{"location":"ml/ml/#machine-learning","text":"Definition","title":"Machine Learning"},{"location":"ml/ml/#anomoly-detection","text":"","title":"Anomoly Detection"},{"location":"ml/ml/#isolation-forests","text":"Documentation Recall that decision trees are built using information criteria such as Gini index or entropy. The obviously different groups are separated at the root of the tree and deeper into the branches, the subtler distinctions are identified. Based on randomly picked characteristics, an isolation forest processes the randomly subsampled data in a tree structure. Samples that reach further into the tree and require more cuts to separate them have a very little probability that they are anomalies. Likewise, samples that are found on the shorter branches of the tree are more likely to be anomalies, since the tree found it simpler to distinguish them from the other data. from sklearn.datasets import make_blobs from numpy import quantile , random , where from sklearn.ensemble import IsolationForest import matplotlib.pyplot as plt IF = IsolationForest ( n_estimators = 100 , contamination =. 03 ) predictions = IF . fit_predict ( X ) outlier_index = where ( predictions ==- 1 ) values = X [ outlier_index ] plt . scatter ( X [:, 0 ], X [:, 1 ]) plt . scatter ( values [:, 0 ], values [:, 1 ], color = 'y' ) plt . show () More Info","title":"Isolation Forests"},{"location":"ml/ml/#sqliteviz","text":"Sqliteviz is a single-page offline-first PWA for fully client-side visualisation of SQLite databases or CSV files. With sqliteviz you can: - run SQL queries against a SQLite database and create Plotly charts and pivot tables based on the result sets - import a CSV file into a SQLite database and visualize imported data - export result set to CSV file - manage inquiries and run them against different databases - import/export inquiries from/to a JSON file - export a modified SQLite database - use it offline from your OS application menu like any other desktop app Documentation Examples Use the Live App","title":"Sqliteviz"},{"location":"ml/ml/#streamlit","text":"Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes you can build and deploy powerful data apps. Documentation Examples import streamlit as st import pandas as pd import numpy as np st . title ( 'Uber pickups in NYC' ) DATE_COLUMN = 'date/time' DATA_URL = ( 'https://s3-us-west-2.amazonaws.com/' 'streamlit-demo-data/uber-raw-data-sep14.csv.gz' ) def load_data ( nrows ): data = pd . read_csv ( DATA_URL , nrows = nrows ) data [ DATE_COLUMN ] = pd . to_datetime ( data [ DATE_COLUMN ]) return data data = load_data ( 10000 ) Create a bar chart hist_values = np . histogram ( data [ DATE_COLUMN ] . dt . hour , bins = 24 , range = ( 0 , 24 ) )[ 0 ] st . bar_chart ( hist_values ) Plot data on a map with sliding filter hour_to_filter = st . slider ( 'hour' , 0 , 23 , 17 ) # min: 0h, max: 23h, default: 17h filtered_data = data [ data [ DATE_COLUMN ] . dt . hour == hour_to_filter ] st . subheader ( f 'Map of all pickups at { hour_to_filter } :00' ) st . map ( filtered_data ) Use the Live App","title":"Streamlit"},{"location":"ml/ml/#lightdash","text":"Connect Lightdash to your dbt project, add metrics directly in your data transformation layer, then create and share your insights with your team. Documentation Examples Use the Live Demo App","title":"LightDash"},{"location":"ml/ml/#apache-superset","text":"Superset is fast, lightweight, intuitive, and loaded with options that make it easy for users of all skill sets to explore and visualize their data, from simple line charts to highly detailed geospatial charts. Documentation Examples","title":"Apache Superset"},{"location":"ml/nlp/","text":"Natural Language Processing Summarization Extractive Summary Documentation Extractive Summary: This method summarizes the text by selecting the most important subset of sentences from the original text. As the name suggests, it extracts the most important information from the text. This method does not have the capability of text generation by itself and hence the output will always have some part of the original text. from nltk.corpus import stopwords from nltk.tokenize import word_tokenize , sent_tokenize from collections import Counter stopWords = set ( stopwords . words ( \"english\" )) def Summary ( text : str , sensitivity : float = 1.2 ): words = [ word . lower () for word in word_tokenize ( text ) if word . lower () not in stopWords ] freqTable = dict ( Counter ( words )) sentences = [ sentence . lower () for sentence in sent_tokenize ( text )] sentenceValues = {} for sentence in sentences : sentenceValues [ sentence ] = 0 for word in sentence . split (): sentenceValues [ sentence ] += freqTable . get ( word , 0 ) average = int ( sum ( sentenceValues . values ()) / len ( sentenceValues )) summary = \" \" . join ( sentence for sentence in sentences if sentenceValues [ sentence ] > sensitivity * average ) return summary The above code removes stopwords from the text (the, and, or, at, etc.) then counts how frequently each word is used. This sort of becomes the score for a word. Then it loops through each word of each sentence and tallies up the score for the sentence. Finally it takes an average score for the sentences and builds the summary from the highest scoring sentences (as a multiple of the sensitivity param). Abstractive Summary Abstractive Summary: The idea behind this method is to understand the core context of the original text and produce new text based on this understanding. It can be compared to the way humans read and summarize text in their own way. The output of abstractive summary can have elements not present in the original text.","title":"Natural Language Processing"},{"location":"ml/nlp/#natural-language-processing","text":"","title":"Natural Language Processing"},{"location":"ml/nlp/#summarization","text":"","title":"Summarization"},{"location":"ml/nlp/#extractive-summary","text":"Documentation Extractive Summary: This method summarizes the text by selecting the most important subset of sentences from the original text. As the name suggests, it extracts the most important information from the text. This method does not have the capability of text generation by itself and hence the output will always have some part of the original text. from nltk.corpus import stopwords from nltk.tokenize import word_tokenize , sent_tokenize from collections import Counter stopWords = set ( stopwords . words ( \"english\" )) def Summary ( text : str , sensitivity : float = 1.2 ): words = [ word . lower () for word in word_tokenize ( text ) if word . lower () not in stopWords ] freqTable = dict ( Counter ( words )) sentences = [ sentence . lower () for sentence in sent_tokenize ( text )] sentenceValues = {} for sentence in sentences : sentenceValues [ sentence ] = 0 for word in sentence . split (): sentenceValues [ sentence ] += freqTable . get ( word , 0 ) average = int ( sum ( sentenceValues . values ()) / len ( sentenceValues )) summary = \" \" . join ( sentence for sentence in sentences if sentenceValues [ sentence ] > sensitivity * average ) return summary The above code removes stopwords from the text (the, and, or, at, etc.) then counts how frequently each word is used. This sort of becomes the score for a word. Then it loops through each word of each sentence and tallies up the score for the sentence. Finally it takes an average score for the sentences and builds the summary from the highest scoring sentences (as a multiple of the sensitivity param).","title":"Extractive Summary"},{"location":"ml/nlp/#abstractive-summary","text":"Abstractive Summary: The idea behind this method is to understand the core context of the original text and produce new text based on this understanding. It can be compared to the way humans read and summarize text in their own way. The output of abstractive summary can have elements not present in the original text.","title":"Abstractive Summary"},{"location":"schedule/scheduling/","text":"Scheduling Tasks On how to schedule tasks Dagster Documentation Dagster is a data orchestrator. It lets you define jobs in terms of the data flow between logical components called ops. These jobs can be developed locally and run anywhere. Dagster uses decorators like @op to decorate functions and @job to group @op into a single flow. from dagster import job , op @op def get_name (): return \"dagster\" @op def hello ( name : str ): print ( f \"Hello, { name } !\" ) @job def hello_dagster (): hello ( get_name ()) Then run with the follow command dagit -f hello_world.py Which will serve up a web UI Prefect Documentation Prefect Orion is the second-generation workflow orchestration engine from Prefect, now available as a technical preview. Orion has been designed from the ground up to handle the dynamic, scalable workloads that the modern data stack demands. Powered by a brand-new, asynchronous rules engine, it represents an enormous amount of research, development, and dedication to a simple idea: You should love your workflows again. from prefect import flow , task from typing import List import httpx @task ( retries = 3 ) def get_stars ( repo : str ): url = f \"https://api.github.com/repos/ { repo } \" count = httpx . get ( url ) . json ()[ \"stargazers_count\" ] print ( f \" { repo } has { count } stars!\" ) @flow ( name = \"Github Stars\" ) def github_stars ( repos : List [ str ]): for repo in repos : get_stars ( repo ) # run the flow! github_stars ([ \"PrefectHQ/Prefect\" , \"PrefectHQ/miter-design\" ]) Then run with the follow command prefect orion start Which will serve up a web UI Celery Documentation from celery import Celery app = Celery ( 'tasks' , broker = 'pyamqp://guest@localhost//' ) @app . task def add ( x , y ): return x + y Schedule Documentation Python job scheduling for humans. Run Python functions (or any other callable) periodically using a friendly syntax. A simple to use API for scheduling jobs, made for humans. In-process scheduler for periodic jobs. No extra processes needed! Very lightweight and no external dependencies. Excellent test coverage. Tested on Python 3.6, 3.7, 3.8 and 3.9 Schedule is not a \u2018one size fits all\u2019 scheduling library. This library is designed to be a simple solution for simple scheduling problems. You should probably look somewhere else if you need: Job persistence (remember schedule between restarts) Exact timing (sub-second precision execution) Concurrent execution (multiple threads) Localization (time zones, workdays or holidays) Schedule does not account for the time it takes for the job function to execute. To guarantee a stable execution schedule you need to move long-running jobs off the main-thread (where the scheduler runs). See Parallel execution for a sample implementation. Simple Single threaded execution: import schedule import time def job (): print ( \"I'm working...\" ) schedule . every ( 10 ) . minutes . do ( job ) schedule . every () . hour . do ( job ) schedule . every () . day . at ( \"10:30\" ) . do ( job ) schedule . every () . monday . do ( job ) schedule . every () . wednesday . at ( \"13:15\" ) . do ( job ) schedule . every () . minute . at ( \":17\" ) . do ( job ) while True : schedule . run_pending () time . sleep ( 1 ) Multi threaded execution: More Info import threading import time import schedule def job (): print ( \"I'm running on thread %s \" % threading . current_thread ()) def run_threaded ( job_func ): job_thread = threading . Thread ( target = job_func ) job_thread . start () schedule . every ( 10 ) . seconds . do ( run_threaded , job ) schedule . every ( 10 ) . seconds . do ( run_threaded , job ) schedule . every ( 10 ) . seconds . do ( run_threaded , job ) schedule . every ( 10 ) . seconds . do ( run_threaded , job ) schedule . every ( 10 ) . seconds . do ( run_threaded , job ) while 1 : schedule . run_pending () time . sleep ( 1 )","title":"Scheduling Tasks"},{"location":"schedule/scheduling/#scheduling-tasks","text":"On how to schedule tasks","title":"Scheduling Tasks"},{"location":"schedule/scheduling/#dagster","text":"Documentation Dagster is a data orchestrator. It lets you define jobs in terms of the data flow between logical components called ops. These jobs can be developed locally and run anywhere. Dagster uses decorators like @op to decorate functions and @job to group @op into a single flow. from dagster import job , op @op def get_name (): return \"dagster\" @op def hello ( name : str ): print ( f \"Hello, { name } !\" ) @job def hello_dagster (): hello ( get_name ()) Then run with the follow command dagit -f hello_world.py Which will serve up a web UI","title":"Dagster"},{"location":"schedule/scheduling/#prefect","text":"Documentation Prefect Orion is the second-generation workflow orchestration engine from Prefect, now available as a technical preview. Orion has been designed from the ground up to handle the dynamic, scalable workloads that the modern data stack demands. Powered by a brand-new, asynchronous rules engine, it represents an enormous amount of research, development, and dedication to a simple idea: You should love your workflows again. from prefect import flow , task from typing import List import httpx @task ( retries = 3 ) def get_stars ( repo : str ): url = f \"https://api.github.com/repos/ { repo } \" count = httpx . get ( url ) . json ()[ \"stargazers_count\" ] print ( f \" { repo } has { count } stars!\" ) @flow ( name = \"Github Stars\" ) def github_stars ( repos : List [ str ]): for repo in repos : get_stars ( repo ) # run the flow! github_stars ([ \"PrefectHQ/Prefect\" , \"PrefectHQ/miter-design\" ]) Then run with the follow command prefect orion start Which will serve up a web UI","title":"Prefect"},{"location":"schedule/scheduling/#celery","text":"Documentation from celery import Celery app = Celery ( 'tasks' , broker = 'pyamqp://guest@localhost//' ) @app . task def add ( x , y ): return x + y","title":"Celery"},{"location":"schedule/scheduling/#schedule","text":"Documentation Python job scheduling for humans. Run Python functions (or any other callable) periodically using a friendly syntax. A simple to use API for scheduling jobs, made for humans. In-process scheduler for periodic jobs. No extra processes needed! Very lightweight and no external dependencies. Excellent test coverage. Tested on Python 3.6, 3.7, 3.8 and 3.9 Schedule is not a \u2018one size fits all\u2019 scheduling library. This library is designed to be a simple solution for simple scheduling problems. You should probably look somewhere else if you need: Job persistence (remember schedule between restarts) Exact timing (sub-second precision execution) Concurrent execution (multiple threads) Localization (time zones, workdays or holidays) Schedule does not account for the time it takes for the job function to execute. To guarantee a stable execution schedule you need to move long-running jobs off the main-thread (where the scheduler runs). See Parallel execution for a sample implementation. Simple Single threaded execution: import schedule import time def job (): print ( \"I'm working...\" ) schedule . every ( 10 ) . minutes . do ( job ) schedule . every () . hour . do ( job ) schedule . every () . day . at ( \"10:30\" ) . do ( job ) schedule . every () . monday . do ( job ) schedule . every () . wednesday . at ( \"13:15\" ) . do ( job ) schedule . every () . minute . at ( \":17\" ) . do ( job ) while True : schedule . run_pending () time . sleep ( 1 ) Multi threaded execution: More Info import threading import time import schedule def job (): print ( \"I'm running on thread %s \" % threading . current_thread ()) def run_threaded ( job_func ): job_thread = threading . Thread ( target = job_func ) job_thread . start () schedule . every ( 10 ) . seconds . do ( run_threaded , job ) schedule . every ( 10 ) . seconds . do ( run_threaded , job ) schedule . every ( 10 ) . seconds . do ( run_threaded , job ) schedule . every ( 10 ) . seconds . do ( run_threaded , job ) schedule . every ( 10 ) . seconds . do ( run_threaded , job ) while 1 : schedule . run_pending () time . sleep ( 1 )","title":"Schedule"},{"location":"viz/viz/","text":"Visualization Credit All these plots, code, and explanations come from this website www.machinelearningplus.com Correlation The plots under correlation is used to visualize the relationship between 2 or more variables. That is, how does one variable change with respect to another. Scatter plot Scatterplot is a classic and fundamental plot used to study the relationship between two variables. If you have multiple groups in your data you may want to visualise each group in a different color. In matplotlib , you can conveniently do this using plt.scatterplot() . Show Code # Import dataset midwest = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/midwest_filter.csv\" ) # Prepare Data # Create as many colors as there are unique midwest['category'] categories = np . unique ( midwest [ 'category' ]) colors = [ plt . cm . tab10 ( i / float ( len ( categories ) - 1 )) for i in range ( len ( categories ))] # Draw Plot for Each Category plt . figure ( figsize = ( 16 , 10 ), dpi = 80 , facecolor = 'w' , edgecolor = 'k' ) for i , category in enumerate ( categories ): plt . scatter ( 'area' , 'poptotal' , data = midwest . loc [ midwest . category == category , :], s = 20 , c = colors [ i ], label = str ( category )) # Decorations plt . gca () . set ( xlim = ( 0.0 , 0.1 ), ylim = ( 0 , 90000 ), xlabel = 'Area' , ylabel = 'Population' ) plt . xticks ( fontsize = 12 ); plt . yticks ( fontsize = 12 ) plt . title ( \"Scatterplot of Midwest Area vs Population\" , fontsize = 22 ) plt . legend ( fontsize = 12 ) plt . show () Bubble plot with Encircling Sometimes you want to show a group of points within a boundary to emphasize their importance. In this example, you get the records from the dataframe that should be encircled and pass it to the encircle() described in the code below. Show Code from matplotlib import patches from scipy.spatial import ConvexHull import warnings ; warnings . simplefilter ( 'ignore' ) sns . set_style ( \"white\" ) # Step 1: Prepare Data midwest = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/midwest_filter.csv\" ) # As many colors as there are unique midwest['category'] categories = np . unique ( midwest [ 'category' ]) colors = [ plt . cm . tab10 ( i / float ( len ( categories ) - 1 )) for i in range ( len ( categories ))] # Step 2: Draw Scatterplot with unique color for each category fig = plt . figure ( figsize = ( 16 , 10 ), dpi = 80 , facecolor = 'w' , edgecolor = 'k' ) for i , category in enumerate ( categories ): plt . scatter ( 'area' , 'poptotal' , data = midwest . loc [ midwest . category == category , :], s = 'dot_size' , c = colors [ i ], label = str ( category ), edgecolors = 'black' , linewidths =. 5 ) # Step 3: Encircling # https://stackoverflow.com/questions/44575681/how-do-i-encircle-different-data-sets-in-scatter-plot def encircle ( x , y , ax = None , ** kw ): if not ax : ax = plt . gca () p = np . c_ [ x , y ] hull = ConvexHull ( p ) poly = plt . Polygon ( p [ hull . vertices ,:], ** kw ) ax . add_patch ( poly ) # Select data to be encircled midwest_encircle_data = midwest . loc [ midwest . state == 'IN' , :] # Draw polygon surrounding vertices encircle ( midwest_encircle_data . area , midwest_encircle_data . poptotal , ec = \"k\" , fc = \"gold\" , alpha = 0.1 ) encircle ( midwest_encircle_data . area , midwest_encircle_data . poptotal , ec = \"firebrick\" , fc = \"none\" , linewidth = 1.5 ) # Step 4: Decorations plt . gca () . set ( xlim = ( 0.0 , 0.1 ), ylim = ( 0 , 90000 ), xlabel = 'Area' , ylabel = 'Population' ) plt . xticks ( fontsize = 12 ); plt . yticks ( fontsize = 12 ) plt . title ( \"Bubble Plot with Encircling\" , fontsize = 22 ) plt . legend ( fontsize = 12 ) plt . show () Scatter plot with linear regression line of best fit If you want to understand how two variables change with respect to each other, the line of best fit is the way to go. The below plot shows how the line of best fit differs amongst various groups in the data. To disable the groupings and to just draw one line-of-best-fit for the entire dataset, remove the hue='cyl' parameter from the sns.lmplot() call below. Show Code # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) df_select = df . loc [ df . cyl . isin ([ 4 , 8 ]), :] # Plot sns . set_style ( \"white\" ) gridobj = sns . lmplot ( x = \"displ\" , y = \"hwy\" , hue = \"cyl\" , data = df_select , height = 7 , aspect = 1.6 , robust = True , palette = 'tab10' , scatter_kws = dict ( s = 60 , linewidths =. 7 , edgecolors = 'black' )) # Decorations gridobj . set ( xlim = ( 0.5 , 7.5 ), ylim = ( 0 , 50 )) plt . title ( \"Scatterplot with line of best fit grouped by number of cylinders\" , fontsize = 20 ) plt . show () Each regression line in its own column Alternately, you can show the best fit line for each group in its own column. You cando this by setting the col=groupingcolumn parameter inside the sns.lmplot() . Show Code # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) df_select = df . loc [ df . cyl . isin ([ 4 , 8 ]), :] # Each line in its own column sns . set_style ( \"white\" ) gridobj = sns . lmplot ( x = \"displ\" , y = \"hwy\" , data = df_select , height = 7 , robust = True , palette = 'Set1' , col = \"cyl\" , scatter_kws = dict ( s = 60 , linewidths =. 7 , edgecolors = 'black' )) # Decorations gridobj . set ( xlim = ( 0.5 , 7.5 ), ylim = ( 0 , 50 )) plt . show () Jittering with stripplot Often multiple datapoints have exactly the same X and Y values. As a result, multiple points get plotted over each other and hide. To avoid this, jitter the points slightly so you can visually see them. This is convenient to do using seaborn\u2019s stripplot() . Show Code # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) # Draw Stripplot fig , ax = plt . subplots ( figsize = ( 16 , 10 ), dpi = 80 ) sns . stripplot ( df . cty , df . hwy , jitter = 0.25 , size = 8 , ax = ax , linewidth =. 5 ) # Decorations plt . title ( 'Use jittered plots to avoid overlapping of points' , fontsize = 22 ) plt . show () Counts Plot Another option to avoid the problem of points overlap is the increase the size of the dot depending on how many points lie in that spot. So, larger the size of the point more is the concentration of points around that. Show Code # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) df_counts = df . groupby ([ 'hwy' , 'cty' ]) . size () . reset_index ( name = 'counts' ) # Draw Stripplot fig , ax = plt . subplots ( figsize = ( 16 , 10 ), dpi = 80 ) sns . stripplot ( df_counts . cty , df_counts . hwy , size = df_counts . counts * 2 , ax = ax ) # Decorations plt . title ( 'Counts Plot - Size of circle is bigger as more points overlap' , fontsize = 22 ) plt . show () Marginal Histogram Marginal histograms have a histogram along the X and Y axis variables. This is used to visualize the relationship between the X and Y along with the univariate distribution of the X and the Y individually. This plot if often used in exploratory data analysis (EDA). Show Code # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) # Create Fig and gridspec fig = plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) grid = plt . GridSpec ( 4 , 4 , hspace = 0.5 , wspace = 0.2 ) # Define the axes ax_main = fig . add_subplot ( grid [: - 1 , : - 1 ]) ax_right = fig . add_subplot ( grid [: - 1 , - 1 ], xticklabels = [], yticklabels = []) ax_bottom = fig . add_subplot ( grid [ - 1 , 0 : - 1 ], xticklabels = [], yticklabels = []) # Scatterplot on main ax ax_main . scatter ( 'displ' , 'hwy' , s = df . cty * 4 , c = df . manufacturer . astype ( 'category' ) . cat . codes , alpha =. 9 , data = df , cmap = \"tab10\" , edgecolors = 'gray' , linewidths =. 5 ) # histogram on the right ax_bottom . hist ( df . displ , 40 , histtype = 'stepfilled' , orientation = 'vertical' , color = 'deeppink' ) ax_bottom . invert_yaxis () # histogram in the bottom ax_right . hist ( df . hwy , 40 , histtype = 'stepfilled' , orientation = 'horizontal' , color = 'deeppink' ) # Decorations ax_main . set ( title = 'Scatterplot with Histograms \\n displ vs hwy' , xlabel = 'displ' , ylabel = 'hwy' ) ax_main . title . set_fontsize ( 20 ) for item in ([ ax_main . xaxis . label , ax_main . yaxis . label ] + ax_main . get_xticklabels () + ax_main . get_yticklabels ()): item . set_fontsize ( 14 ) xlabels = ax_main . get_xticks () . tolist () ax_main . set_xticklabels ( xlabels ) plt . show () Marginal Boxplot Marginal boxplot serves a similar purpose as marginal histogram. However, the boxplot helps to pinpoint the median, 25th and 75th percentiles of the X and the Y. Show Code # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) # Create Fig and gridspec fig = plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) grid = plt . GridSpec ( 4 , 4 , hspace = 0.5 , wspace = 0.2 ) # Define the axes ax_main = fig . add_subplot ( grid [: - 1 , : - 1 ]) ax_right = fig . add_subplot ( grid [: - 1 , - 1 ], xticklabels = [], yticklabels = []) ax_bottom = fig . add_subplot ( grid [ - 1 , 0 : - 1 ], xticklabels = [], yticklabels = []) # Scatterplot on main ax ax_main . scatter ( 'displ' , 'hwy' , s = df . cty * 5 , c = df . manufacturer . astype ( 'category' ) . cat . codes , alpha =. 9 , data = df , cmap = \"Set1\" , edgecolors = 'black' , linewidths =. 5 ) # Add a graph in each part sns . boxplot ( df . hwy , ax = ax_right , orient = \"v\" ) sns . boxplot ( df . displ , ax = ax_bottom , orient = \"h\" ) # Decorations ------------------ # Remove x axis name for the boxplot ax_bottom . set ( xlabel = '' ) ax_right . set ( ylabel = '' ) # Main Title, Xlabel and YLabel ax_main . set ( title = 'Scatterplot with Histograms \\n displ vs hwy' , xlabel = 'displ' , ylabel = 'hwy' ) # Set font size of different components ax_main . title . set_fontsize ( 20 ) for item in ([ ax_main . xaxis . label , ax_main . yaxis . label ] + ax_main . get_xticklabels () + ax_main . get_yticklabels ()): item . set_fontsize ( 14 ) plt . show () Correllogram Correlogram is used to visually see the correlation metric between all possible pairs of numeric variables in a given dataframe (or 2D array). Show Code # Import Dataset df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) # Plot plt . figure ( figsize = ( 12 , 10 ), dpi = 80 ) sns . heatmap ( df . corr (), xticklabels = df . corr () . columns , yticklabels = df . corr () . columns , cmap = 'RdYlGn' , center = 0 , annot = True ) # Decorations plt . title ( 'Correlogram of mtcars' , fontsize = 22 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . show () Pairwise Plot Pairwise plot is a favorite in exploratory analysis to understand the relationship between all possible pairs of numeric variables. It is a must have tool for bivariate analysis. Show Code # Load Dataset df = sns . load_dataset ( 'iris' ) # Plot plt . figure ( figsize = ( 10 , 8 ), dpi = 80 ) sns . pairplot ( df , kind = \"scatter\" , hue = \"species\" , plot_kws = dict ( s = 80 , edgecolor = \"white\" , linewidth = 2.5 )) plt . show () Show Code # Load Dataset df = sns . load_dataset ( 'iris' ) # Plot plt . figure ( figsize = ( 10 , 8 ), dpi = 80 ) sns . pairplot ( df , kind = \"reg\" , hue = \"species\" ) plt . show () Deviation Diverging Bars If you want to see how the items are varying based on a single metric and visualize the order and amount of this variance, the diverging bars is a great tool. It helps to quickly differentiate the performance of groups in your data and is quite intuitive and instantly conveys the point. Show Code # Prepare Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) x = df . loc [:, [ 'mpg' ]] df [ 'mpg_z' ] = ( x - x . mean ()) / x . std () df [ 'colors' ] = [ 'red' if x < 0 else 'green' for x in df [ 'mpg_z' ]] df . sort_values ( 'mpg_z' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot plt . figure ( figsize = ( 14 , 10 ), dpi = 80 ) plt . hlines ( y = df . index , xmin = 0 , xmax = df . mpg_z , color = df . colors , alpha = 0.4 , linewidth = 5 ) # Decorations plt . gca () . set ( ylabel = '$Model$' , xlabel = '$Mileage$' ) plt . yticks ( df . index , df . cars , fontsize = 12 ) plt . title ( 'Diverging Bars of Car Mileage' , fontdict = { 'size' : 20 }) plt . grid ( linestyle = '--' , alpha = 0.5 ) plt . show () Diverging Texts Diverging texts is similar to diverging bars and it preferred if you want to show the value of each items within the chart in a nice and presentable way. Show Code # Prepare Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) x = df . loc [:, [ 'mpg' ]] df [ 'mpg_z' ] = ( x - x . mean ()) / x . std () df [ 'colors' ] = [ 'red' if x < 0 else 'green' for x in df [ 'mpg_z' ]] df . sort_values ( 'mpg_z' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot plt . figure ( figsize = ( 14 , 14 ), dpi = 80 ) plt . hlines ( y = df . index , xmin = 0 , xmax = df . mpg_z ) for x , y , tex in zip ( df . mpg_z , df . index , df . mpg_z ): t = plt . text ( x , y , round ( tex , 2 ), horizontalalignment = 'right' if x < 0 else 'left' , verticalalignment = 'center' , fontdict = { 'color' : 'red' if x < 0 else 'green' , 'size' : 14 }) # Decorations plt . yticks ( df . index , df . cars , fontsize = 12 ) plt . title ( 'Diverging Text Bars of Car Mileage' , fontdict = { 'size' : 20 }) plt . grid ( linestyle = '--' , alpha = 0.5 ) plt . xlim ( - 2.5 , 2.5 ) plt . show () Diverging Dot Plot Divering dot plot is also similar to the diverging bars. However compared to diverging bars, the absence of bars reduces the amount of contrast and disparity between the groups. Show Code # Prepare Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) x = df . loc [:, [ 'mpg' ]] df [ 'mpg_z' ] = ( x - x . mean ()) / x . std () df [ 'colors' ] = [ 'red' if x < 0 else 'darkgreen' for x in df [ 'mpg_z' ]] df . sort_values ( 'mpg_z' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot plt . figure ( figsize = ( 14 , 16 ), dpi = 80 ) plt . scatter ( df . mpg_z , df . index , s = 450 , alpha =. 6 , color = df . colors ) for x , y , tex in zip ( df . mpg_z , df . index , df . mpg_z ): t = plt . text ( x , y , round ( tex , 1 ), horizontalalignment = 'center' , verticalalignment = 'center' , fontdict = { 'color' : 'white' }) # Decorations # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . yticks ( df . index , df . cars ) plt . title ( 'Diverging Dotplot of Car Mileage' , fontdict = { 'size' : 20 }) plt . xlabel ( '$Mileage$' ) plt . grid ( linestyle = '--' , alpha = 0.5 ) plt . xlim ( - 2.5 , 2.5 ) plt . show () Diverging Lollipop Chart with Markers Lollipop with markers provides a flexible way of visualizing the divergence by laying emphasis on any significant datapoints you want to bring attention to and give reasoning within the chart appropriately. Show Code # Prepare Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) x = df . loc [:, [ 'mpg' ]] df [ 'mpg_z' ] = ( x - x . mean ()) / x . std () df [ 'colors' ] = 'black' # color fiat differently df . loc [ df . cars == 'Fiat X1-9' , 'colors' ] = 'darkorange' df . sort_values ( 'mpg_z' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot import matplotlib.patches as patches plt . figure ( figsize = ( 14 , 16 ), dpi = 80 ) plt . hlines ( y = df . index , xmin = 0 , xmax = df . mpg_z , color = df . colors , alpha = 0.4 , linewidth = 1 ) plt . scatter ( df . mpg_z , df . index , color = df . colors , s = [ 600 if x == 'Fiat X1-9' else 300 for x in df . cars ], alpha = 0.6 ) plt . yticks ( df . index , df . cars ) plt . xticks ( fontsize = 12 ) # Annotate plt . annotate ( 'Mercedes Models' , xy = ( 0.0 , 11.0 ), xytext = ( 1.0 , 11 ), xycoords = 'data' , fontsize = 15 , ha = 'center' , va = 'center' , bbox = dict ( boxstyle = 'square' , fc = 'firebrick' ), arrowprops = dict ( arrowstyle = '-[, widthB=2.0, lengthB=1.5' , lw = 2.0 , color = 'steelblue' ), color = 'white' ) # Add Patches p1 = patches . Rectangle (( - 2.0 , - 1 ), width =. 3 , height = 3 , alpha =. 2 , facecolor = 'red' ) p2 = patches . Rectangle (( 1.5 , 27 ), width =. 8 , height = 5 , alpha =. 2 , facecolor = 'green' ) plt . gca () . add_patch ( p1 ) plt . gca () . add_patch ( p2 ) # Decorate plt . title ( 'Diverging Bars of Car Mileage' , fontdict = { 'size' : 20 }) plt . grid ( linestyle = '--' , alpha = 0.5 ) plt . show () Area Chart By coloring the area between the axis and the lines, the area chart throws more emphasis not just on the peaks and troughs but also the duration of the highs and lows. The longer the duration of the highs, the larger is the area under the line. Show Code import numpy as np import pandas as pd # Prepare Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/economics.csv\" , parse_dates = [ 'date' ]) . head ( 100 ) x = np . arange ( df . shape [ 0 ]) y_returns = ( df . psavert . diff () . fillna ( 0 ) / df . psavert . shift ( 1 )) . fillna ( 0 ) * 100 # Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) plt . fill_between ( x [ 1 :], y_returns [ 1 :], 0 , where = y_returns [ 1 :] >= 0 , facecolor = 'green' , interpolate = True , alpha = 0.7 ) plt . fill_between ( x [ 1 :], y_returns [ 1 :], 0 , where = y_returns [ 1 :] <= 0 , facecolor = 'red' , interpolate = True , alpha = 0.7 ) # Annotate plt . annotate ( 'Peak \\n 1975' , xy = ( 94.0 , 21.0 ), xytext = ( 88.0 , 28 ), bbox = dict ( boxstyle = 'square' , fc = 'firebrick' ), arrowprops = dict ( facecolor = 'steelblue' , shrink = 0.05 ), fontsize = 15 , color = 'white' ) # Decorations xtickvals = [ str ( m )[: 3 ] . upper () + \"-\" + str ( y ) for y , m in zip ( df . date . dt . year , df . date . dt . month_name ())] plt . gca () . set_xticks ( x [:: 6 ]) plt . gca () . set_xticklabels ( xtickvals [:: 6 ], rotation = 90 , fontdict = { 'horizontalalignment' : 'center' , 'verticalalignment' : 'center_baseline' }) plt . ylim ( - 35 , 35 ) plt . xlim ( 1 , 100 ) plt . title ( \"Month Economics Return %\" , fontsize = 22 ) plt . ylabel ( 'Monthly returns %' ) plt . grid ( alpha = 0.5 ) plt . show () Ranking Ordered Bar Chart Ordered bar chart conveys the rank order of the items effectively. But adding the value of the metric above the chart, the user gets the precise information from the chart itself. Show Code # Prepare Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) df = df_raw [[ 'cty' , 'manufacturer' ]] . groupby ( 'manufacturer' ) . apply ( lambda x : x . mean ()) df . sort_values ( 'cty' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot import matplotlib.patches as patches fig , ax = plt . subplots ( figsize = ( 16 , 10 ), facecolor = 'white' , dpi = 80 ) ax . vlines ( x = df . index , ymin = 0 , ymax = df . cty , color = 'firebrick' , alpha = 0.7 , linewidth = 20 ) # Annotate Text for i , cty in enumerate ( df . cty ): ax . text ( i , cty + 0.5 , round ( cty , 1 ), horizontalalignment = 'center' ) # Title, Label, Ticks and Ylim ax . set_title ( 'Bar Chart for Highway Mileage' , fontdict = { 'size' : 22 }) ax . set ( ylabel = 'Miles Per Gallon' , ylim = ( 0 , 30 )) plt . xticks ( df . index , df . manufacturer . str . upper (), rotation = 60 , horizontalalignment = 'right' , fontsize = 12 ) # Add patches to color the X axis labels p1 = patches . Rectangle (( . 57 , - 0.005 ), width =. 33 , height =. 13 , alpha =. 1 , facecolor = 'green' , transform = fig . transFigure ) p2 = patches . Rectangle (( . 124 , - 0.005 ), width =. 446 , height =. 13 , alpha =. 1 , facecolor = 'red' , transform = fig . transFigure ) fig . add_artist ( p1 ) fig . add_artist ( p2 ) plt . show () Lollipop Chart Lollipop chart serves a similar purpose as a ordered bar chart in a visually pleasing way. Show Code # Prepare Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) df = df_raw [[ 'cty' , 'manufacturer' ]] . groupby ( 'manufacturer' ) . apply ( lambda x : x . mean ()) df . sort_values ( 'cty' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot fig , ax = plt . subplots ( figsize = ( 16 , 10 ), dpi = 80 ) ax . vlines ( x = df . index , ymin = 0 , ymax = df . cty , color = 'firebrick' , alpha = 0.7 , linewidth = 2 ) ax . scatter ( x = df . index , y = df . cty , s = 75 , color = 'firebrick' , alpha = 0.7 ) # Title, Label, Ticks and Ylim ax . set_title ( 'Lollipop Chart for Highway Mileage' , fontdict = { 'size' : 22 }) ax . set_ylabel ( 'Miles Per Gallon' ) ax . set_xticks ( df . index ) ax . set_xticklabels ( df . manufacturer . str . upper (), rotation = 60 , fontdict = { 'horizontalalignment' : 'right' , 'size' : 12 }) ax . set_ylim ( 0 , 30 ) # Annotate for row in df . itertuples (): ax . text ( row . Index , row . cty +. 5 , s = round ( row . cty , 2 ), horizontalalignment = 'center' , verticalalignment = 'bottom' , fontsize = 14 ) plt . show () Dot Plot The dot plot conveys the rank order of the items. And since it is aligned along the horizontal axis, you can visualize how far the points are from each other more easily. Show Code # Prepare Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) df = df_raw [[ 'cty' , 'manufacturer' ]] . groupby ( 'manufacturer' ) . apply ( lambda x : x . mean ()) df . sort_values ( 'cty' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot fig , ax = plt . subplots ( figsize = ( 16 , 10 ), dpi = 80 ) ax . hlines ( y = df . index , xmin = 11 , xmax = 26 , color = 'gray' , alpha = 0.7 , linewidth = 1 , linestyles = 'dashdot' ) ax . scatter ( y = df . index , x = df . cty , s = 75 , color = 'firebrick' , alpha = 0.7 ) # Title, Label, Ticks and Ylim ax . set_title ( 'Dot Plot for Highway Mileage' , fontdict = { 'size' : 22 }) ax . set_xlabel ( 'Miles Per Gallon' ) ax . set_yticks ( df . index ) ax . set_yticklabels ( df . manufacturer . str . title (), fontdict = { 'horizontalalignment' : 'right' }) ax . set_xlim ( 10 , 27 ) plt . show () Slope Chart Slope chart is most suitable for comparing the \u2018Before\u2019 and \u2018After\u2019 positions of a given person/item. Show Code import matplotlib.lines as mlines # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/gdppercap.csv\" ) left_label = [ str ( c ) + ', ' + str ( round ( y )) for c , y in zip ( df . continent , df [ '1952' ])] right_label = [ str ( c ) + ', ' + str ( round ( y )) for c , y in zip ( df . continent , df [ '1957' ])] klass = [ 'red' if ( y1 - y2 ) < 0 else 'green' for y1 , y2 in zip ( df [ '1952' ], df [ '1957' ])] # draw line # https://stackoverflow.com/questions/36470343/how-to-draw-a-line-with-matplotlib/36479941 def newline ( p1 , p2 , color = 'black' ): ax = plt . gca () l = mlines . Line2D ([ p1 [ 0 ], p2 [ 0 ]], [ p1 [ 1 ], p2 [ 1 ]], color = 'red' if p1 [ 1 ] - p2 [ 1 ] > 0 else 'green' , marker = 'o' , markersize = 6 ) ax . add_line ( l ) return l fig , ax = plt . subplots ( 1 , 1 , figsize = ( 14 , 14 ), dpi = 80 ) # Vertical Lines ax . vlines ( x = 1 , ymin = 500 , ymax = 13000 , color = 'black' , alpha = 0.7 , linewidth = 1 , linestyles = 'dotted' ) ax . vlines ( x = 3 , ymin = 500 , ymax = 13000 , color = 'black' , alpha = 0.7 , linewidth = 1 , linestyles = 'dotted' ) # Points ax . scatter ( y = df [ '1952' ], x = np . repeat ( 1 , df . shape [ 0 ]), s = 10 , color = 'black' , alpha = 0.7 ) ax . scatter ( y = df [ '1957' ], x = np . repeat ( 3 , df . shape [ 0 ]), s = 10 , color = 'black' , alpha = 0.7 ) # Line Segmentsand Annotation for p1 , p2 , c in zip ( df [ '1952' ], df [ '1957' ], df [ 'continent' ]): newline ([ 1 , p1 ], [ 3 , p2 ]) ax . text ( 1 - 0.05 , p1 , c + ', ' + str ( round ( p1 )), horizontalalignment = 'right' , verticalalignment = 'center' , fontdict = { 'size' : 14 }) ax . text ( 3 + 0.05 , p2 , c + ', ' + str ( round ( p2 )), horizontalalignment = 'left' , verticalalignment = 'center' , fontdict = { 'size' : 14 }) # 'Before' and 'After' Annotations ax . text ( 1 - 0.05 , 13000 , 'BEFORE' , horizontalalignment = 'right' , verticalalignment = 'center' , fontdict = { 'size' : 18 , 'weight' : 700 }) ax . text ( 3 + 0.05 , 13000 , 'AFTER' , horizontalalignment = 'left' , verticalalignment = 'center' , fontdict = { 'size' : 18 , 'weight' : 700 }) # Decoration ax . set_title ( \"Slopechart: Comparing GDP Per Capita between 1952 vs 1957\" , fontdict = { 'size' : 22 }) ax . set ( xlim = ( 0 , 4 ), ylim = ( 0 , 14000 ), ylabel = 'Mean GDP Per Capita' ) ax . set_xticks ([ 1 , 3 ]) ax . set_xticklabels ([ \"1952\" , \"1957\" ]) plt . yticks ( np . arange ( 500 , 13000 , 2000 ), fontsize = 12 ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( . 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 0 ) plt . gca () . spines [ \"right\" ] . set_alpha ( . 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 0 ) plt . show () Dumbbell Plot Dumbbell plot conveys the \u2018before\u2019 and \u2018after\u2019 positions of various items along with the rank ordering of the items. Its very useful if you want to visualize the effect of a particular project / initiative on different objects. Show Code import matplotlib.lines as mlines # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/health.csv\" ) df . sort_values ( 'pct_2014' , inplace = True ) df . reset_index ( inplace = True ) # Func to draw line segment def newline ( p1 , p2 , color = 'black' ): ax = plt . gca () l = mlines . Line2D ([ p1 [ 0 ], p2 [ 0 ]], [ p1 [ 1 ], p2 [ 1 ]], color = 'skyblue' ) ax . add_line ( l ) return l # Figure and Axes fig , ax = plt . subplots ( 1 , 1 , figsize = ( 14 , 14 ), facecolor = '#f7f7f7' , dpi = 80 ) # Vertical Lines ax . vlines ( x =. 05 , ymin = 0 , ymax = 26 , color = 'black' , alpha = 1 , linewidth = 1 , linestyles = 'dotted' ) ax . vlines ( x =. 10 , ymin = 0 , ymax = 26 , color = 'black' , alpha = 1 , linewidth = 1 , linestyles = 'dotted' ) ax . vlines ( x =. 15 , ymin = 0 , ymax = 26 , color = 'black' , alpha = 1 , linewidth = 1 , linestyles = 'dotted' ) ax . vlines ( x =. 20 , ymin = 0 , ymax = 26 , color = 'black' , alpha = 1 , linewidth = 1 , linestyles = 'dotted' ) # Points ax . scatter ( y = df [ 'index' ], x = df [ 'pct_2013' ], s = 50 , color = '#0e668b' , alpha = 0.7 ) ax . scatter ( y = df [ 'index' ], x = df [ 'pct_2014' ], s = 50 , color = '#a3c4dc' , alpha = 0.7 ) # Line Segments for i , p1 , p2 in zip ( df [ 'index' ], df [ 'pct_2013' ], df [ 'pct_2014' ]): newline ([ p1 , i ], [ p2 , i ]) # Decoration ax . set_facecolor ( '#f7f7f7' ) ax . set_title ( \"Dumbell Chart: Pct Change - 2013 vs 2014\" , fontdict = { 'size' : 22 }) ax . set ( xlim = ( 0 , . 25 ), ylim = ( - 1 , 27 ), ylabel = 'Mean GDP Per Capita' ) ax . set_xticks ([ . 05 , . 1 , . 15 , . 20 ]) ax . set_xticklabels ([ '5%' , '15%' , '20%' , '25%' ]) ax . set_xticklabels ([ '5%' , '15%' , '20%' , '25%' ]) plt . show () Distribution Histogram for Continuous Variable Histogram shows the frequency distribution of a given variable. The below representation groups the frequency bars based on a categorical variable giving a greater insight about the continuous variable and the categorical variable in tandem. Show Code # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare data x_var = 'displ' groupby_var = 'class' df_agg = df . loc [:, [ x_var , groupby_var ]] . groupby ( groupby_var ) vals = [ df [ x_var ] . values . tolist () for i , df in df_agg ] # Draw plt . figure ( figsize = ( 16 , 9 ), dpi = 80 ) colors = [ plt . cm . Spectral ( i / float ( len ( vals ) - 1 )) for i in range ( len ( vals ))] n , bins , patches = plt . hist ( vals , 30 , stacked = True , density = False , color = colors [: len ( vals )]) # Decoration plt . legend ({ group : col for group , col in zip ( np . unique ( df [ groupby_var ]) . tolist (), colors [: len ( vals )])}) plt . title ( f \"Stacked Histogram of $ { x_var } $ colored by $ { groupby_var } $\" , fontsize = 22 ) plt . xlabel ( x_var ) plt . ylabel ( \"Frequency\" ) plt . ylim ( 0 , 25 ) plt . xticks ( ticks = bins [:: 3 ], labels = [ round ( b , 1 ) for b in bins [:: 3 ]]) plt . show () Histogram for Categorical Variable The histogram of a categorical variable shows the frequency distribution of a that variable. By coloring the bars, you can visualize the distribution in connection with another categorical variable representing the colors. Show Code # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare data x_var = 'manufacturer' groupby_var = 'class' df_agg = df . loc [:, [ x_var , groupby_var ]] . groupby ( groupby_var ) vals = [ df [ x_var ] . values . tolist () for i , df in df_agg ] # Draw plt . figure ( figsize = ( 16 , 9 ), dpi = 80 ) colors = [ plt . cm . Spectral ( i / float ( len ( vals ) - 1 )) for i in range ( len ( vals ))] n , bins , patches = plt . hist ( vals , df [ x_var ] . unique () . __len__ (), stacked = True , density = False , color = colors [: len ( vals )]) # Decoration plt . legend ({ group : col for group , col in zip ( np . unique ( df [ groupby_var ]) . tolist (), colors [: len ( vals )])}) plt . title ( f \"Stacked Histogram of $ { x_var } $ colored by $ { groupby_var } $\" , fontsize = 22 ) plt . xlabel ( x_var ) plt . ylabel ( \"Frequency\" ) plt . ylim ( 0 , 40 ) plt . xticks ( ticks = bins , labels = np . unique ( df [ x_var ]) . tolist (), rotation = 90 , horizontalalignment = 'left' ) plt . show () Density Plot Density plots are a commonly used tool visualise the distribution of a continuous variable. By grouping them by the \u2018response\u2019 variable, you can inspect the relationship between the X and the Y. The below case if for representational purpose to describe how the distribution of city mileage varies with respect the number of cylinders. Show Code # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Draw Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) sns . kdeplot ( df . loc [ df [ 'cyl' ] == 4 , \"cty\" ], shade = True , color = \"g\" , label = \"Cyl=4\" , alpha =. 7 ) sns . kdeplot ( df . loc [ df [ 'cyl' ] == 5 , \"cty\" ], shade = True , color = \"deeppink\" , label = \"Cyl=5\" , alpha =. 7 ) sns . kdeplot ( df . loc [ df [ 'cyl' ] == 6 , \"cty\" ], shade = True , color = \"dodgerblue\" , label = \"Cyl=6\" , alpha =. 7 ) sns . kdeplot ( df . loc [ df [ 'cyl' ] == 8 , \"cty\" ], shade = True , color = \"orange\" , label = \"Cyl=8\" , alpha =. 7 ) # Decoration plt . title ( 'Density Plot of City Mileage by n_Cylinders' , fontsize = 22 ) plt . legend () plt . show () s Density Curves with Histogram Density curve with histogram brings together the collective information conveyed by the two plots so you can have them both in a single figure instead of two. Show Code # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Draw Plot plt . figure ( figsize = ( 13 , 10 ), dpi = 80 ) sns . distplot ( df . loc [ df [ 'class' ] == 'compact' , \"cty\" ], color = \"dodgerblue\" , label = \"Compact\" , hist_kws = { 'alpha' : . 7 }, kde_kws = { 'linewidth' : 3 }) sns . distplot ( df . loc [ df [ 'class' ] == 'suv' , \"cty\" ], color = \"orange\" , label = \"SUV\" , hist_kws = { 'alpha' : . 7 }, kde_kws = { 'linewidth' : 3 }) sns . distplot ( df . loc [ df [ 'class' ] == 'minivan' , \"cty\" ], color = \"g\" , label = \"minivan\" , hist_kws = { 'alpha' : . 7 }, kde_kws = { 'linewidth' : 3 }) plt . ylim ( 0 , 0.35 ) # Decoration plt . title ( 'Density Plot of City Mileage by Vehicle Type' , fontsize = 22 ) plt . legend () plt . show () Joy Plot Joy Plot allows the density curves of different groups to overlap, it is a great way to visualize the distribution of a larger number of groups in relation to each other. It looks pleasing to the eye and conveys just the right information clearly. It can be easily built using the joypy package which is based on matplotlib . Show Code # !pip install joypy # Import Data mpg = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Draw Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) fig , axes = joypy . joyplot ( mpg , column = [ 'hwy' , 'cty' ], by = \"class\" , ylim = 'own' , figsize = ( 14 , 10 )) # Decoration plt . title ( 'Joy Plot of City and Highway Mileage by Class' , fontsize = 22 ) plt . show () Distributed Dot Plot Distributed dot plot shows the univariate distribution of points segmented by groups. The darker the points, more is the concentration of data points in that region. By coloring the median differently, the real positioning of the groups becomes apparent instantly. Show Code import matplotlib.patches as mpatches # Prepare Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) cyl_colors = { 4 : 'tab:red' , 5 : 'tab:green' , 6 : 'tab:blue' , 8 : 'tab:orange' } df_raw [ 'cyl_color' ] = df_raw . cyl . map ( cyl_colors ) # Mean and Median city mileage by make df = df_raw [[ 'cty' , 'manufacturer' ]] . groupby ( 'manufacturer' ) . apply ( lambda x : x . mean ()) df . sort_values ( 'cty' , ascending = False , inplace = True ) df . reset_index ( inplace = True ) df_median = df_raw [[ 'cty' , 'manufacturer' ]] . groupby ( 'manufacturer' ) . apply ( lambda x : x . median ()) # Draw horizontal lines fig , ax = plt . subplots ( figsize = ( 16 , 10 ), dpi = 80 ) ax . hlines ( y = df . index , xmin = 0 , xmax = 40 , color = 'gray' , alpha = 0.5 , linewidth =. 5 , linestyles = 'dashdot' ) # Draw the Dots for i , make in enumerate ( df . manufacturer ): df_make = df_raw . loc [ df_raw . manufacturer == make , :] ax . scatter ( y = np . repeat ( i , df_make . shape [ 0 ]), x = 'cty' , data = df_make , s = 75 , edgecolors = 'gray' , c = 'w' , alpha = 0.5 ) ax . scatter ( y = i , x = 'cty' , data = df_median . loc [ df_median . index == make , :], s = 75 , c = 'firebrick' ) # Annotate ax . text ( 33 , 13 , \"$red \\; dots \\; are \\; the \\: median$\" , fontdict = { 'size' : 12 }, color = 'firebrick' ) # Decorations red_patch = plt . plot ([],[], marker = \"o\" , ms = 10 , ls = \"\" , mec = None , color = 'firebrick' , label = \"Median\" ) plt . legend ( handles = red_patch ) ax . set_title ( 'Distribution of City Mileage by Make' , fontdict = { 'size' : 22 }) ax . set_xlabel ( 'Miles Per Gallon (City)' , alpha = 0.7 ) ax . set_yticks ( df . index ) ax . set_yticklabels ( df . manufacturer . str . title (), fontdict = { 'horizontalalignment' : 'right' }, alpha = 0.7 ) ax . set_xlim ( 1 , 40 ) plt . xticks ( alpha = 0.7 ) plt . gca () . spines [ \"top\" ] . set_visible ( False ) plt . gca () . spines [ \"bottom\" ] . set_visible ( False ) plt . gca () . spines [ \"right\" ] . set_visible ( False ) plt . gca () . spines [ \"left\" ] . set_visible ( False ) plt . grid ( axis = 'both' , alpha =. 4 , linewidth =. 1 ) plt . show () Box Plot Box plots are a great way to visualize the distribution, keeping the median, 25th 75th quartiles and the outliers in mind. However, you need to be careful about interpreting the size the boxes which can potentially distort the number of points contained within that group. So, manually providing the number of observations in each box can help overcome this drawback. For example, the first two boxes on the left have boxes of the same size even though they have 5 and 47 obs respectively. So writing the number of observations in that group becomes necessary. Show Code # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Draw Plot plt . figure ( figsize = ( 13 , 10 ), dpi = 80 ) sns . boxplot ( x = 'class' , y = 'hwy' , data = df , notch = False ) # Add N Obs inside boxplot (optional) def add_n_obs ( df , group_col , y ): medians_dict = { grp [ 0 ]: grp [ 1 ][ y ] . median () for grp in df . groupby ( group_col )} xticklabels = [ x . get_text () for x in plt . gca () . get_xticklabels ()] n_obs = df . groupby ( group_col )[ y ] . size () . values for ( x , xticklabel ), n_ob in zip ( enumerate ( xticklabels ), n_obs ): plt . text ( x , medians_dict [ xticklabel ] * 1.01 , \"#obs : \" + str ( n_ob ), horizontalalignment = 'center' , fontdict = { 'size' : 14 }, color = 'white' ) add_n_obs ( df , group_col = 'class' , y = 'hwy' ) # Decoration plt . title ( 'Box Plot of Highway Mileage by Vehicle Class' , fontsize = 22 ) plt . ylim ( 10 , 40 ) plt . show () Dot + Box Plot Dot + Box plot Conveys similar information as a boxplot split in groups. The dots, in addition, gives a sense of how many data points lie within each group. Show Code # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Draw Plot plt . figure ( figsize = ( 13 , 10 ), dpi = 80 ) sns . boxplot ( x = 'class' , y = 'hwy' , data = df , hue = 'cyl' ) sns . stripplot ( x = 'class' , y = 'hwy' , data = df , color = 'black' , size = 3 , jitter = 1 ) for i in range ( len ( df [ 'class' ] . unique ()) - 1 ): plt . vlines ( i +. 5 , 10 , 45 , linestyles = 'solid' , colors = 'gray' , alpha = 0.2 ) # Decoration plt . title ( 'Box Plot of Highway Mileage by Vehicle Class' , fontsize = 22 ) plt . legend ( title = 'Cylinders' ) plt . show () Violin Plot Violin plot is a visually pleasing alternative to box plots. The shape or area of the violin depends on the number of observations it holds. However, the violin plots can be harder to read and it not commonly used in professional settings. Show Code # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Draw Plot plt . figure ( figsize = ( 13 , 10 ), dpi = 80 ) sns . violinplot ( x = 'class' , y = 'hwy' , data = df , scale = 'width' , inner = 'quartile' ) # Decoration plt . title ( 'Violin Plot of Highway Mileage by Vehicle Class' , fontsize = 22 ) plt . show () Population Pyramid Population pyramid can be used to show either the distribution of the groups ordered by the volumne. Or it can also be used to show the stage-by-stage filtering of the population as it is used below to show how many people pass through each stage of a marketing funnel. Show Code # Read data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/email_campaign_funnel.csv\" ) # Draw Plot plt . figure ( figsize = ( 13 , 10 ), dpi = 80 ) group_col = 'Gender' order_of_bars = df . Stage . unique ()[:: - 1 ] colors = [ plt . cm . Spectral ( i / float ( len ( df [ group_col ] . unique ()) - 1 )) for i in range ( len ( df [ group_col ] . unique ()))] for c , group in zip ( colors , df [ group_col ] . unique ()): sns . barplot ( x = 'Users' , y = 'Stage' , data = df . loc [ df [ group_col ] == group , :], order = order_of_bars , color = c , label = group ) # Decorations plt . xlabel ( \"$Users$\" ) plt . ylabel ( \"Stage of Purchase\" ) plt . yticks ( fontsize = 12 ) plt . title ( \"Population Pyramid of the Marketing Funnel\" , fontsize = 22 ) plt . legend () plt . show () Categorical Plots Categorical plots provided by the seaborn library can be used to visualize the counts distribution of 2 ore more categorical variables in relation to each other. Show Code # Load Dataset titanic = sns . load_dataset ( \"titanic\" ) # Plot g = sns . catplot ( \"alive\" , col = \"deck\" , col_wrap = 4 , data = titanic [ titanic . deck . notnull ()], kind = \"count\" , height = 3.5 , aspect =. 8 , palette = 'tab20' ) fig . suptitle ( 'sf' ) plt . show () Show Code # Load Dataset titanic = sns . load_dataset ( \"titanic\" ) # Plot sns . catplot ( x = \"age\" , y = \"embark_town\" , hue = \"sex\" , col = \"class\" , data = titanic [ titanic . embark_town . notnull ()], orient = \"h\" , height = 5 , aspect = 1 , palette = \"tab10\" , kind = \"violin\" , dodge = True , cut = 0 , bw =. 2 ) Composition Waffle Chart The waffle chart can be created using the pywaffle package and is used to show the compositions of groups in a larger population. Show Code #! pip install pywaffle # Reference: https://stackoverflow.com/questions/41400136/how-to-do-waffle-charts-in-python-square-piechart from pywaffle import Waffle # Import df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare Data df = df_raw . groupby ( 'class' ) . size () . reset_index ( name = 'counts' ) n_categories = df . shape [ 0 ] colors = [ plt . cm . inferno_r ( i / float ( n_categories )) for i in range ( n_categories )] # Draw Plot and Decorate fig = plt . figure ( FigureClass = Waffle , plots = { '111' : { 'values' : df [ 'counts' ], 'labels' : [ \" {0} ( {1} )\" . format ( n [ 0 ], n [ 1 ]) for n in df [[ 'class' , 'counts' ]] . itertuples ()], 'legend' : { 'loc' : 'upper left' , 'bbox_to_anchor' : ( 1.05 , 1 ), 'fontsize' : 12 }, 'title' : { 'label' : '# Vehicles by Class' , 'loc' : 'center' , 'fontsize' : 18 } }, }, rows = 7 , colors = colors , figsize = ( 16 , 9 ) ) Show Code #! pip install pywaffle from pywaffle import Waffle # Import # df_raw = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\") # Prepare Data # By Class Data df_class = df_raw . groupby ( 'class' ) . size () . reset_index ( name = 'counts_class' ) n_categories = df_class . shape [ 0 ] colors_class = [ plt . cm . Set3 ( i / float ( n_categories )) for i in range ( n_categories )] # By Cylinders Data df_cyl = df_raw . groupby ( 'cyl' ) . size () . reset_index ( name = 'counts_cyl' ) n_categories = df_cyl . shape [ 0 ] colors_cyl = [ plt . cm . Spectral ( i / float ( n_categories )) for i in range ( n_categories )] # By Make Data df_make = df_raw . groupby ( 'manufacturer' ) . size () . reset_index ( name = 'counts_make' ) n_categories = df_make . shape [ 0 ] colors_make = [ plt . cm . tab20b ( i / float ( n_categories )) for i in range ( n_categories )] # Draw Plot and Decorate fig = plt . figure ( FigureClass = Waffle , plots = { '311' : { 'values' : df_class [ 'counts_class' ], 'labels' : [ \" {1} \" . format ( n [ 0 ], n [ 1 ]) for n in df_class [[ 'class' , 'counts_class' ]] . itertuples ()], 'legend' : { 'loc' : 'upper left' , 'bbox_to_anchor' : ( 1.05 , 1 ), 'fontsize' : 12 , 'title' : 'Class' }, 'title' : { 'label' : '# Vehicles by Class' , 'loc' : 'center' , 'fontsize' : 18 }, 'colors' : colors_class }, '312' : { 'values' : df_cyl [ 'counts_cyl' ], 'labels' : [ \" {1} \" . format ( n [ 0 ], n [ 1 ]) for n in df_cyl [[ 'cyl' , 'counts_cyl' ]] . itertuples ()], 'legend' : { 'loc' : 'upper left' , 'bbox_to_anchor' : ( 1.05 , 1 ), 'fontsize' : 12 , 'title' : 'Cyl' }, 'title' : { 'label' : '# Vehicles by Cyl' , 'loc' : 'center' , 'fontsize' : 18 }, 'colors' : colors_cyl }, '313' : { 'values' : df_make [ 'counts_make' ], 'labels' : [ \" {1} \" . format ( n [ 0 ], n [ 1 ]) for n in df_make [[ 'manufacturer' , 'counts_make' ]] . itertuples ()], 'legend' : { 'loc' : 'upper left' , 'bbox_to_anchor' : ( 1.05 , 1 ), 'fontsize' : 12 , 'title' : 'Manufacturer' }, 'title' : { 'label' : '# Vehicles by Make' , 'loc' : 'center' , 'fontsize' : 18 }, 'colors' : colors_make } }, rows = 9 , figsize = ( 16 , 14 ) ) Pie Chart Pie chart is a classic way to show the composition of groups. However, its not generally advisable to use nowadays because the area of the pie portions can sometimes become misleading. So, if you are to use pie chart, its highly recommended to explicitly write down the percentage or numbers for each portion of the pie. Show Code # Import df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare Data df = df_raw . groupby ( 'class' ) . size () # Make the plot with pandas df . plot ( kind = 'pie' , subplots = True , figsize = ( 8 , 8 ), dpi = 80 ) plt . title ( \"Pie Chart of Vehicle Class - Bad\" ) plt . ylabel ( \"\" ) plt . show () Show Code # Import df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare Data df = df_raw . groupby ( 'class' ) . size () . reset_index ( name = 'counts' ) # Draw Plot fig , ax = plt . subplots ( figsize = ( 12 , 7 ), subplot_kw = dict ( aspect = \"equal\" ), dpi = 80 ) data = df [ 'counts' ] categories = df [ 'class' ] explode = [ 0 , 0 , 0 , 0 , 0 , 0.1 , 0 ] def func ( pct , allvals ): absolute = int ( pct / 100. * np . sum ( allvals )) return \" {:.1f} % ( {:d} )\" . format ( pct , absolute ) wedges , texts , autotexts = ax . pie ( data , autopct = lambda pct : func ( pct , data ), textprops = dict ( color = \"w\" ), colors = plt . cm . Dark2 . colors , startangle = 140 , explode = explode ) # Decoration ax . legend ( wedges , categories , title = \"Vehicle Class\" , loc = \"center left\" , bbox_to_anchor = ( 1 , 0 , 0.5 , 1 )) plt . setp ( autotexts , size = 10 , weight = 700 ) ax . set_title ( \"Class of Vehicles: Pie Chart\" ) plt . show () Treemap Tree map is similar to a pie chart and it does a better work without misleading the contributions by each group. Show Code # pip install squarify import squarify # Import Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare Data df = df_raw . groupby ( 'class' ) . size () . reset_index ( name = 'counts' ) labels = df . apply ( lambda x : str ( x [ 0 ]) + \" \\n (\" + str ( x [ 1 ]) + \")\" , axis = 1 ) sizes = df [ 'counts' ] . values . tolist () colors = [ plt . cm . Spectral ( i / float ( len ( labels ))) for i in range ( len ( labels ))] # Draw Plot plt . figure ( figsize = ( 12 , 8 ), dpi = 80 ) squarify . plot ( sizes = sizes , label = labels , color = colors , alpha =. 8 ) # Decorate plt . title ( 'Treemap of Vechile Class' ) plt . axis ( 'off' ) plt . show () Bar Chart Bar chart is a classic way of visualizing items based on counts or any given metric. In below chart, I have used a different color for each item, but you might typically want to pick one color for all items unless you to color them by groups. The color names get stored inside all_colors in the code below. You can change the color of the bars by setting the color parameter in plt.plot() . Show Code import random # Import Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare Data df = df_raw . groupby ( 'manufacturer' ) . size () . reset_index ( name = 'counts' ) n = df [ 'manufacturer' ] . unique () . __len__ () + 1 all_colors = list ( plt . cm . colors . cnames . keys ()) random . seed ( 100 ) c = random . choices ( all_colors , k = n ) # Plot Bars plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) plt . bar ( df [ 'manufacturer' ], df [ 'counts' ], color = c , width =. 5 ) for i , val in enumerate ( df [ 'counts' ] . values ): plt . text ( i , val , float ( val ), horizontalalignment = 'center' , verticalalignment = 'bottom' , fontdict = { 'fontweight' : 500 , 'size' : 12 }) # Decoration plt . gca () . set_xticklabels ( df [ 'manufacturer' ], rotation = 60 , horizontalalignment = 'right' ) plt . title ( \"Number of Vehicles by Manaufacturers\" , fontsize = 22 ) plt . ylabel ( '# Vehicles' ) plt . ylim ( 0 , 45 ) plt . show () Change Time Series Plot Time series plot is used to visualise how a given metric changes over time. Here you can see how the Air Passenger traffic changed between 1949 and 1969. Show Code # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/AirPassengers.csv' ) # Draw Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) plt . plot ( 'date' , 'traffic' , data = df , color = 'tab:red' ) # Decoration plt . ylim ( 50 , 750 ) xtick_location = df . index . tolist ()[:: 12 ] xtick_labels = [ x [ - 4 :] for x in df . date . tolist ()[:: 12 ]] plt . xticks ( ticks = xtick_location , labels = xtick_labels , rotation = 0 , fontsize = 12 , horizontalalignment = 'center' , alpha =. 7 ) plt . yticks ( fontsize = 12 , alpha =. 7 ) plt . title ( \"Air Passengers Traffic (1949 - 1969)\" , fontsize = 22 ) plt . grid ( axis = 'both' , alpha =. 3 ) # Remove borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0.0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( 0.3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0.0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( 0.3 ) plt . show () Time Series with Peaks and Troughs Annotated The below time series plots all the the peaks and troughs and annotates the occurence of selected special events. Show Code # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/AirPassengers.csv' ) # Get the Peaks and Troughs data = df [ 'traffic' ] . values doublediff = np . diff ( np . sign ( np . diff ( data ))) peak_locations = np . where ( doublediff == - 2 )[ 0 ] + 1 doublediff2 = np . diff ( np . sign ( np . diff ( - 1 * data ))) trough_locations = np . where ( doublediff2 == - 2 )[ 0 ] + 1 # Draw Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) plt . plot ( 'date' , 'traffic' , data = df , color = 'tab:blue' , label = 'Air Traffic' ) plt . scatter ( df . date [ peak_locations ], df . traffic [ peak_locations ], marker = mpl . markers . CARETUPBASE , color = 'tab:green' , s = 100 , label = 'Peaks' ) plt . scatter ( df . date [ trough_locations ], df . traffic [ trough_locations ], marker = mpl . markers . CARETDOWNBASE , color = 'tab:red' , s = 100 , label = 'Troughs' ) # Annotate for t , p in zip ( trough_locations [ 1 :: 5 ], peak_locations [:: 3 ]): plt . text ( df . date [ p ], df . traffic [ p ] + 15 , df . date [ p ], horizontalalignment = 'center' , color = 'darkgreen' ) plt . text ( df . date [ t ], df . traffic [ t ] - 35 , df . date [ t ], horizontalalignment = 'center' , color = 'darkred' ) # Decoration plt . ylim ( 50 , 750 ) xtick_location = df . index . tolist ()[:: 6 ] xtick_labels = df . date . tolist ()[:: 6 ] plt . xticks ( ticks = xtick_location , labels = xtick_labels , rotation = 90 , fontsize = 12 , alpha =. 7 ) plt . title ( \"Peak and Troughs of Air Passengers Traffic (1949 - 1969)\" , fontsize = 22 ) plt . yticks ( fontsize = 12 , alpha =. 7 ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( . 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( . 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . legend ( loc = 'upper left' ) plt . grid ( axis = 'y' , alpha =. 3 ) plt . show () Autocorrelation (ACF) and Partial Autocorrelation (PACF) Plot The ACF plot shows the correlation of the time series with its own lags. Each vertical line (on the autocorrelation plot) represents the correlation between the series and its lag starting from lag 0. The blue shaded region in the plot is the significance level. Those lags that lie above the blue line are the significant lags. So how to interpret this? For AirPassengers, we see upto 14 lags have crossed the blue line and so are significant. This means, the Air Passengers traffic seen upto 14 years back has an influence on the traffic seen today. PACF on the other had shows the autocorrelation of any given lag (of time series) against the current series, but with the contributions of the lags-inbetween removed. Show Code from statsmodels.graphics.tsaplots import plot_acf , plot_pacf # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/AirPassengers.csv' ) # Draw Plot fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 ), dpi = 80 ) plot_acf ( df . traffic . tolist (), ax = ax1 , lags = 50 ) plot_pacf ( df . traffic . tolist (), ax = ax2 , lags = 20 ) # Decorate # lighten the borders ax1 . spines [ \"top\" ] . set_alpha ( . 3 ); ax2 . spines [ \"top\" ] . set_alpha ( . 3 ) ax1 . spines [ \"bottom\" ] . set_alpha ( . 3 ); ax2 . spines [ \"bottom\" ] . set_alpha ( . 3 ) ax1 . spines [ \"right\" ] . set_alpha ( . 3 ); ax2 . spines [ \"right\" ] . set_alpha ( . 3 ) ax1 . spines [ \"left\" ] . set_alpha ( . 3 ); ax2 . spines [ \"left\" ] . set_alpha ( . 3 ) # font size of tick labels ax1 . tick_params ( axis = 'both' , labelsize = 12 ) ax2 . tick_params ( axis = 'both' , labelsize = 12 ) plt . show () Cross Correlation plot Cross correlation plot shows the lags of two time series with each other. Show Code import statsmodels.tsa.stattools as stattools # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/mortality.csv' ) x = df [ 'mdeaths' ] y = df [ 'fdeaths' ] # Compute Cross Correlations ccs = stattools . ccf ( x , y )[: 100 ] nlags = len ( ccs ) # Compute the Significance level # ref: https://stats.stackexchange.com/questions/3115/cross-correlation-significance-in-r/3128#3128 conf_level = 2 / np . sqrt ( nlags ) # Draw Plot plt . figure ( figsize = ( 12 , 7 ), dpi = 80 ) plt . hlines ( 0 , xmin = 0 , xmax = 100 , color = 'gray' ) # 0 axis plt . hlines ( conf_level , xmin = 0 , xmax = 100 , color = 'gray' ) plt . hlines ( - conf_level , xmin = 0 , xmax = 100 , color = 'gray' ) plt . bar ( x = np . arange ( len ( ccs )), height = ccs , width =. 3 ) # Decoration plt . title ( '$Cross\\; Correlation\\; Plot:\\; mdeaths\\; vs\\; fdeaths$' , fontsize = 22 ) plt . xlim ( 0 , len ( ccs )) plt . show () Time Series Decomposition Plot Time series decomposition plot shows the break down of the time series into trend, seasonal and residual components. Show Code from statsmodels.tsa.seasonal import seasonal_decompose from dateutil.parser import parse # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/AirPassengers.csv' ) dates = pd . DatetimeIndex ([ parse ( d ) . strftime ( '%Y-%m-01' ) for d in df [ 'date' ]]) df . set_index ( dates , inplace = True ) # Decompose result = seasonal_decompose ( df [ 'traffic' ], model = 'multiplicative' ) # Plot plt . rcParams . update ({ 'figure.figsize' : ( 10 , 10 )}) result . plot () . suptitle ( 'Time Series Decomposition of Air Passengers' ) plt . show () Multiple Time Series You can plot multiple time series that measures the same value on the same chart as shown below. Show Code # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/mortality.csv' ) # Define the upper limit, lower limit, interval of Y axis and colors y_LL = 100 y_UL = int ( df . iloc [:, 1 :] . max () . max () * 1.1 ) y_interval = 400 mycolors = [ 'tab:red' , 'tab:blue' , 'tab:green' , 'tab:orange' ] # Draw Plot and Annotate fig , ax = plt . subplots ( 1 , 1 , figsize = ( 16 , 9 ), dpi = 80 ) columns = df . columns [ 1 :] for i , column in enumerate ( columns ): plt . plot ( df . date . values , df [ column ] . values , lw = 1.5 , color = mycolors [ i ]) plt . text ( df . shape [ 0 ] + 1 , df [ column ] . values [ - 1 ], column , fontsize = 14 , color = mycolors [ i ]) # Draw Tick lines for y in range ( y_LL , y_UL , y_interval ): plt . hlines ( y , xmin = 0 , xmax = 71 , colors = 'black' , alpha = 0.3 , linestyles = \"--\" , lw = 0.5 ) # Decorations plt . tick_params ( axis = \"both\" , which = \"both\" , bottom = False , top = False , labelbottom = True , left = False , right = False , labelleft = True ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . title ( 'Number of Deaths from Lung Diseases in the UK (1974-1979)' , fontsize = 22 ) plt . yticks ( range ( y_LL , y_UL , y_interval ), [ str ( y ) for y in range ( y_LL , y_UL , y_interval )], fontsize = 12 ) plt . xticks ( range ( 0 , df . shape [ 0 ], 12 ), df . date . values [:: 12 ], horizontalalignment = 'left' , fontsize = 12 ) plt . ylim ( y_LL , y_UL ) plt . xlim ( - 2 , 80 ) plt . show () Plotting with different scales using secondary Y axis If you want to show two time series that measures two different quantities at the same point in time, you can plot the second series againt the secondary Y axis on the right. Show Code # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/economics.csv\" ) x = df [ 'date' ] y1 = df [ 'psavert' ] y2 = df [ 'unemploy' ] # Plot Line1 (Left Y Axis) fig , ax1 = plt . subplots ( 1 , 1 , figsize = ( 16 , 9 ), dpi = 80 ) ax1 . plot ( x , y1 , color = 'tab:red' ) # Plot Line2 (Right Y Axis) ax2 = ax1 . twinx () # instantiate a second axes that shares the same x-axis ax2 . plot ( x , y2 , color = 'tab:blue' ) # Decorations # ax1 (left Y axis) ax1 . set_xlabel ( 'Year' , fontsize = 20 ) ax1 . tick_params ( axis = 'x' , rotation = 0 , labelsize = 12 ) ax1 . set_ylabel ( 'Personal Savings Rate' , color = 'tab:red' , fontsize = 20 ) ax1 . tick_params ( axis = 'y' , rotation = 0 , labelcolor = 'tab:red' ) ax1 . grid ( alpha =. 4 ) # ax2 (right Y axis) ax2 . set_ylabel ( \"# Unemployed (1000's)\" , color = 'tab:blue' , fontsize = 20 ) ax2 . tick_params ( axis = 'y' , labelcolor = 'tab:blue' ) ax2 . set_xticks ( np . arange ( 0 , len ( x ), 60 )) ax2 . set_xticklabels ( x [:: 60 ], rotation = 90 , fontdict = { 'fontsize' : 10 }) ax2 . set_title ( \"Personal Savings Rate vs Unemployed: Plotting in Secondary Y Axis\" , fontsize = 22 ) fig . tight_layout () plt . show () Time Series with Error Bands Time series with error bands can be constructed if you have a time series dataset with multiple observations for each time point (date / timestamp). Below you can see a couple of examples based on the orders coming in at various times of the day. And another example on the number of orders arriving over a duration of 45 days. In this approach, the mean of the number of orders is denoted by the white line. And a 95% confidence bands are computed and drawn around the mean. Show Code from scipy.stats import sem # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/user_orders_hourofday.csv\" ) df_mean = df . groupby ( 'order_hour_of_day' ) . quantity . mean () df_se = df . groupby ( 'order_hour_of_day' ) . quantity . apply ( sem ) . mul ( 1.96 ) # Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) plt . ylabel ( \"# Orders\" , fontsize = 16 ) x = df_mean . index plt . plot ( x , df_mean , color = \"white\" , lw = 2 ) plt . fill_between ( x , df_mean - df_se , df_mean + df_se , color = \"#3F5D7D\" ) # Decorations # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( 1 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( 1 ) plt . xticks ( x [:: 2 ], [ str ( d ) for d in x [:: 2 ]] , fontsize = 12 ) plt . title ( \"User Orders by Hour of Day (95 % c onfidence)\" , fontsize = 22 ) plt . xlabel ( \"Hour of Day\" ) s , e = plt . gca () . get_xlim () plt . xlim ( s , e ) # Draw Horizontal Tick lines for y in range ( 8 , 20 , 2 ): plt . hlines ( y , xmin = s , xmax = e , colors = 'black' , alpha = 0.5 , linestyles = \"--\" , lw = 0.5 ) plt . show () Show Code \"Data Source: https://www.kaggle.com/olistbr/brazilian-ecommerce#olist_orders_dataset.csv\" from dateutil.parser import parse from scipy.stats import sem # Import Data df_raw = pd . read_csv ( 'https://raw.githubusercontent.com/selva86/datasets/master/orders_45d.csv' , parse_dates = [ 'purchase_time' , 'purchase_date' ]) # Prepare Data: Daily Mean and SE Bands df_mean = df_raw . groupby ( 'purchase_date' ) . quantity . mean () df_se = df_raw . groupby ( 'purchase_date' ) . quantity . apply ( sem ) . mul ( 1.96 ) # Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) plt . ylabel ( \"# Daily Orders\" , fontsize = 16 ) x = [ d . date () . strftime ( '%Y-%m- %d ' ) for d in df_mean . index ] plt . plot ( x , df_mean , color = \"white\" , lw = 2 ) plt . fill_between ( x , df_mean - df_se , df_mean + df_se , color = \"#3F5D7D\" ) # Decorations # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( 1 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( 1 ) plt . xticks ( x [:: 6 ], [ str ( d ) for d in x [:: 6 ]] , fontsize = 12 ) plt . title ( \"Daily Order Quantity of Brazilian Retail with Error Bands (95 % c onfidence)\" , fontsize = 20 ) # Axis limits s , e = plt . gca () . get_xlim () plt . xlim ( s , e - 2 ) plt . ylim ( 4 , 10 ) # Draw Horizontal Tick lines for y in range ( 5 , 10 , 1 ): plt . hlines ( y , xmin = s , xmax = e , colors = 'black' , alpha = 0.5 , linestyles = \"--\" , lw = 0.5 ) plt . show () Stacked Area Chart Stacked area chart gives an visual representation of the extent of contribution from multiple time series so that it is easy to compare against each other. Show Code # Import Data df = pd . read_csv ( 'https://raw.githubusercontent.com/selva86/datasets/master/nightvisitors.csv' ) # Decide Colors mycolors = [ 'tab:red' , 'tab:blue' , 'tab:green' , 'tab:orange' , 'tab:brown' , 'tab:grey' , 'tab:pink' , 'tab:olive' ] # Draw Plot and Annotate fig , ax = plt . subplots ( 1 , 1 , figsize = ( 16 , 9 ), dpi = 80 ) columns = df . columns [ 1 :] labs = columns . values . tolist () # Prepare data x = df [ 'yearmon' ] . values . tolist () y0 = df [ columns [ 0 ]] . values . tolist () y1 = df [ columns [ 1 ]] . values . tolist () y2 = df [ columns [ 2 ]] . values . tolist () y3 = df [ columns [ 3 ]] . values . tolist () y4 = df [ columns [ 4 ]] . values . tolist () y5 = df [ columns [ 5 ]] . values . tolist () y6 = df [ columns [ 6 ]] . values . tolist () y7 = df [ columns [ 7 ]] . values . tolist () y = np . vstack ([ y0 , y2 , y4 , y6 , y7 , y5 , y1 , y3 ]) # Plot for each column labs = columns . values . tolist () ax = plt . gca () ax . stackplot ( x , y , labels = labs , colors = mycolors , alpha = 0.8 ) # Decorations ax . set_title ( 'Night Visitors in Australian Regions' , fontsize = 18 ) ax . set ( ylim = [ 0 , 100000 ]) ax . legend ( fontsize = 10 , ncol = 4 ) plt . xticks ( x [:: 5 ], fontsize = 10 , horizontalalignment = 'center' ) plt . yticks ( np . arange ( 10000 , 100000 , 20000 ), fontsize = 10 ) plt . xlim ( x [ 0 ], x [ - 1 ]) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . show () Area Chart UnStacked An unstacked area chart is used to visualize the progress (ups and downs) of two or more series with respect to each other. In the chart below, you can clearly see how the personal savings rate comes down as the median duration of unemployment increases. The unstacked area chart brings out this phenomenon nicely. Show Code # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/economics.csv\" ) # Prepare Data x = df [ 'date' ] . values . tolist () y1 = df [ 'psavert' ] . values . tolist () y2 = df [ 'uempmed' ] . values . tolist () mycolors = [ 'tab:red' , 'tab:blue' , 'tab:green' , 'tab:orange' , 'tab:brown' , 'tab:grey' , 'tab:pink' , 'tab:olive' ] columns = [ 'psavert' , 'uempmed' ] # Draw Plot fig , ax = plt . subplots ( 1 , 1 , figsize = ( 16 , 9 ), dpi = 80 ) ax . fill_between ( x , y1 = y1 , y2 = 0 , label = columns [ 1 ], alpha = 0.5 , color = mycolors [ 1 ], linewidth = 2 ) ax . fill_between ( x , y1 = y2 , y2 = 0 , label = columns [ 0 ], alpha = 0.5 , color = mycolors [ 0 ], linewidth = 2 ) # Decorations ax . set_title ( 'Personal Savings Rate vs Median Duration of Unemployment' , fontsize = 18 ) ax . set ( ylim = [ 0 , 30 ]) ax . legend ( loc = 'best' , fontsize = 12 ) plt . xticks ( x [:: 50 ], fontsize = 10 , horizontalalignment = 'center' ) plt . yticks ( np . arange ( 2.5 , 30.0 , 2.5 ), fontsize = 10 ) plt . xlim ( - 10 , x [ - 1 ]) # Draw Tick lines for y in np . arange ( 2.5 , 30.0 , 2.5 ): plt . hlines ( y , xmin = 0 , xmax = len ( x ), colors = 'black' , alpha = 0.3 , linestyles = \"--\" , lw = 0.5 ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . show () Calendar Heat Map Calendar map is an alternate and a less preferred option to visualise time based data compared to a time series. Though can be visually appealing, the numeric values are not quite evident. It is however effective in picturising the extreme values and holiday effects nicely. Show Code import matplotlib as mpl import calmap # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/yahoo.csv\" , parse_dates = [ 'date' ]) df . set_index ( 'date' , inplace = True ) # Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) calmap . calendarplot ( df [ '2014' ][ 'VIX.Close' ], fig_kws = { 'figsize' : ( 16 , 10 )}, yearlabel_kws = { 'color' : 'black' , 'fontsize' : 14 }, subplot_kws = { 'title' : 'Yahoo Stock Prices' }) plt . show () Seasonal Plot The seasonal plot can be used to compare how the time series performed at same day in the previous season (year / month / week etc). Show Code from dateutil.parser import parse # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/AirPassengers.csv' ) # Prepare data df [ 'year' ] = [ parse ( d ) . year for d in df . date ] df [ 'month' ] = [ parse ( d ) . strftime ( '%b' ) for d in df . date ] years = df [ 'year' ] . unique () # Draw Plot mycolors = [ 'tab:red' , 'tab:blue' , 'tab:green' , 'tab:orange' , 'tab:brown' , 'tab:grey' , 'tab:pink' , 'tab:olive' , 'deeppink' , 'steelblue' , 'firebrick' , 'mediumseagreen' ] plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) for i , y in enumerate ( years ): plt . plot ( 'month' , 'traffic' , data = df . loc [ df . year == y , :], color = mycolors [ i ], label = y ) plt . text ( df . loc [ df . year == y , :] . shape [ 0 ] -. 9 , df . loc [ df . year == y , 'traffic' ][ - 1 :] . values [ 0 ], y , fontsize = 12 , color = mycolors [ i ]) # Decoration plt . ylim ( 50 , 750 ) plt . xlim ( - 0.3 , 11 ) plt . ylabel ( '$Air Traffic$' ) plt . yticks ( fontsize = 12 , alpha =. 7 ) plt . title ( \"Monthly Seasonal Plot: Air Passengers Traffic (1949 - 1969)\" , fontsize = 22 ) plt . grid ( axis = 'y' , alpha =. 3 ) # Remove borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0.0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( 0.5 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0.0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( 0.5 ) # plt.legend(loc='upper right', ncol=2, fontsize=12) plt . show () Groups Dendrogram A Dendrogram groups similar points together based on a given distance metric and organizes them in tree like links based on the point\u2019s similarity. Show Code import scipy.cluster.hierarchy as shc # Import Data df = pd . read_csv ( 'https://raw.githubusercontent.com/selva86/datasets/master/USArrests.csv' ) # Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) plt . title ( \"USArrests Dendograms\" , fontsize = 22 ) dend = shc . dendrogram ( shc . linkage ( df [[ 'Murder' , 'Assault' , 'UrbanPop' , 'Rape' ]], method = 'ward' ), labels = df . State . values , color_threshold = 100 ) plt . xticks ( fontsize = 12 ) plt . show () Cluster Plot Cluster Plot canbe used to demarcate points that belong to the same cluster. Below is a representational example to group the US states into 5 groups based on the USArrests dataset. This cluster plot uses the \u2018murder\u2019 and \u2018assault\u2019 columns as X and Y axis. Alternately you can use the first to principal components as rthe X and Y axis. Show Code from sklearn.cluster import AgglomerativeClustering from scipy.spatial import ConvexHull # Import Data df = pd . read_csv ( 'https://raw.githubusercontent.com/selva86/datasets/master/USArrests.csv' ) # Agglomerative Clustering cluster = AgglomerativeClustering ( n_clusters = 5 , affinity = 'euclidean' , linkage = 'ward' ) cluster . fit_predict ( df [[ 'Murder' , 'Assault' , 'UrbanPop' , 'Rape' ]]) # Plot plt . figure ( figsize = ( 14 , 10 ), dpi = 80 ) plt . scatter ( df . iloc [:, 0 ], df . iloc [:, 1 ], c = cluster . labels_ , cmap = 'tab10' ) # Encircle def encircle ( x , y , ax = None , ** kw ): if not ax : ax = plt . gca () p = np . c_ [ x , y ] hull = ConvexHull ( p ) poly = plt . Polygon ( p [ hull . vertices ,:], ** kw ) ax . add_patch ( poly ) # Draw polygon surrounding vertices encircle ( df . loc [ cluster . labels_ == 0 , 'Murder' ], df . loc [ cluster . labels_ == 0 , 'Assault' ], ec = \"k\" , fc = \"gold\" , alpha = 0.2 , linewidth = 0 ) encircle ( df . loc [ cluster . labels_ == 1 , 'Murder' ], df . loc [ cluster . labels_ == 1 , 'Assault' ], ec = \"k\" , fc = \"tab:blue\" , alpha = 0.2 , linewidth = 0 ) encircle ( df . loc [ cluster . labels_ == 2 , 'Murder' ], df . loc [ cluster . labels_ == 2 , 'Assault' ], ec = \"k\" , fc = \"tab:red\" , alpha = 0.2 , linewidth = 0 ) encircle ( df . loc [ cluster . labels_ == 3 , 'Murder' ], df . loc [ cluster . labels_ == 3 , 'Assault' ], ec = \"k\" , fc = \"tab:green\" , alpha = 0.2 , linewidth = 0 ) encircle ( df . loc [ cluster . labels_ == 4 , 'Murder' ], df . loc [ cluster . labels_ == 4 , 'Assault' ], ec = \"k\" , fc = \"tab:orange\" , alpha = 0.2 , linewidth = 0 ) # Decorations plt . xlabel ( 'Murder' ); plt . xticks ( fontsize = 12 ) plt . ylabel ( 'Assault' ); plt . yticks ( fontsize = 12 ) plt . title ( 'Agglomerative Clustering of USArrests (5 Groups)' , fontsize = 22 ) plt . show () Andrews Curve Andrews Curve helps visualize if there are inherent groupings of the numerical features based on a given grouping. If the features (columns in the dataset) doesn\u2019t help discriminate the group ( cyl) , then the lines will not be well segregated as you see below. Show Code from pandas.plotting import andrews_curves # Import df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) df . drop ([ 'cars' , 'carname' ], axis = 1 , inplace = True ) # Plot plt . figure ( figsize = ( 12 , 9 ), dpi = 80 ) andrews_curves ( df , 'cyl' , colormap = 'Set1' ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . title ( 'Andrews Curves of mtcars' , fontsize = 22 ) plt . xlim ( - 3 , 3 ) plt . grid ( alpha = 0.3 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . show () Parallel Coordinates Parallel coordinates helps to visualize if a feature helps to segregate the groups effectively. If a segregation is effected, that feature is likely going to be very useful in predicting that group. Show Code from pandas.plotting import parallel_coordinates # Import Data df_final = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/diamonds_filter.csv\" ) # Plot plt . figure ( figsize = ( 12 , 9 ), dpi = 80 ) parallel_coordinates ( df_final , 'cut' , colormap = 'Dark2' ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . title ( 'Parallel Coordinated of Diamonds' , fontsize = 22 ) plt . grid ( alpha = 0.3 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . show ()","title":"Viz"},{"location":"viz/viz/#visualization-credit","text":"All these plots, code, and explanations come from this website www.machinelearningplus.com","title":"Visualization Credit"},{"location":"viz/viz/#correlation","text":"The plots under correlation is used to visualize the relationship between 2 or more variables. That is, how does one variable change with respect to another.","title":"Correlation"},{"location":"viz/viz/#scatter-plot","text":"Scatterplot is a classic and fundamental plot used to study the relationship between two variables. If you have multiple groups in your data you may want to visualise each group in a different color. In matplotlib , you can conveniently do this using plt.scatterplot() . Show Code # Import dataset midwest = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/midwest_filter.csv\" ) # Prepare Data # Create as many colors as there are unique midwest['category'] categories = np . unique ( midwest [ 'category' ]) colors = [ plt . cm . tab10 ( i / float ( len ( categories ) - 1 )) for i in range ( len ( categories ))] # Draw Plot for Each Category plt . figure ( figsize = ( 16 , 10 ), dpi = 80 , facecolor = 'w' , edgecolor = 'k' ) for i , category in enumerate ( categories ): plt . scatter ( 'area' , 'poptotal' , data = midwest . loc [ midwest . category == category , :], s = 20 , c = colors [ i ], label = str ( category )) # Decorations plt . gca () . set ( xlim = ( 0.0 , 0.1 ), ylim = ( 0 , 90000 ), xlabel = 'Area' , ylabel = 'Population' ) plt . xticks ( fontsize = 12 ); plt . yticks ( fontsize = 12 ) plt . title ( \"Scatterplot of Midwest Area vs Population\" , fontsize = 22 ) plt . legend ( fontsize = 12 ) plt . show ()","title":"Scatter plot"},{"location":"viz/viz/#bubble-plot-with-encircling","text":"Sometimes you want to show a group of points within a boundary to emphasize their importance. In this example, you get the records from the dataframe that should be encircled and pass it to the encircle() described in the code below. Show Code from matplotlib import patches from scipy.spatial import ConvexHull import warnings ; warnings . simplefilter ( 'ignore' ) sns . set_style ( \"white\" ) # Step 1: Prepare Data midwest = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/midwest_filter.csv\" ) # As many colors as there are unique midwest['category'] categories = np . unique ( midwest [ 'category' ]) colors = [ plt . cm . tab10 ( i / float ( len ( categories ) - 1 )) for i in range ( len ( categories ))] # Step 2: Draw Scatterplot with unique color for each category fig = plt . figure ( figsize = ( 16 , 10 ), dpi = 80 , facecolor = 'w' , edgecolor = 'k' ) for i , category in enumerate ( categories ): plt . scatter ( 'area' , 'poptotal' , data = midwest . loc [ midwest . category == category , :], s = 'dot_size' , c = colors [ i ], label = str ( category ), edgecolors = 'black' , linewidths =. 5 ) # Step 3: Encircling # https://stackoverflow.com/questions/44575681/how-do-i-encircle-different-data-sets-in-scatter-plot def encircle ( x , y , ax = None , ** kw ): if not ax : ax = plt . gca () p = np . c_ [ x , y ] hull = ConvexHull ( p ) poly = plt . Polygon ( p [ hull . vertices ,:], ** kw ) ax . add_patch ( poly ) # Select data to be encircled midwest_encircle_data = midwest . loc [ midwest . state == 'IN' , :] # Draw polygon surrounding vertices encircle ( midwest_encircle_data . area , midwest_encircle_data . poptotal , ec = \"k\" , fc = \"gold\" , alpha = 0.1 ) encircle ( midwest_encircle_data . area , midwest_encircle_data . poptotal , ec = \"firebrick\" , fc = \"none\" , linewidth = 1.5 ) # Step 4: Decorations plt . gca () . set ( xlim = ( 0.0 , 0.1 ), ylim = ( 0 , 90000 ), xlabel = 'Area' , ylabel = 'Population' ) plt . xticks ( fontsize = 12 ); plt . yticks ( fontsize = 12 ) plt . title ( \"Bubble Plot with Encircling\" , fontsize = 22 ) plt . legend ( fontsize = 12 ) plt . show ()","title":"Bubble plot with Encircling"},{"location":"viz/viz/#scatter-plot-with-linear-regression-line-of-best-fit","text":"If you want to understand how two variables change with respect to each other, the line of best fit is the way to go. The below plot shows how the line of best fit differs amongst various groups in the data. To disable the groupings and to just draw one line-of-best-fit for the entire dataset, remove the hue='cyl' parameter from the sns.lmplot() call below. Show Code # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) df_select = df . loc [ df . cyl . isin ([ 4 , 8 ]), :] # Plot sns . set_style ( \"white\" ) gridobj = sns . lmplot ( x = \"displ\" , y = \"hwy\" , hue = \"cyl\" , data = df_select , height = 7 , aspect = 1.6 , robust = True , palette = 'tab10' , scatter_kws = dict ( s = 60 , linewidths =. 7 , edgecolors = 'black' )) # Decorations gridobj . set ( xlim = ( 0.5 , 7.5 ), ylim = ( 0 , 50 )) plt . title ( \"Scatterplot with line of best fit grouped by number of cylinders\" , fontsize = 20 ) plt . show ()","title":"Scatter plot with linear regression line of best fit"},{"location":"viz/viz/#each-regression-line-in-its-own-column","text":"Alternately, you can show the best fit line for each group in its own column. You cando this by setting the col=groupingcolumn parameter inside the sns.lmplot() . Show Code # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) df_select = df . loc [ df . cyl . isin ([ 4 , 8 ]), :] # Each line in its own column sns . set_style ( \"white\" ) gridobj = sns . lmplot ( x = \"displ\" , y = \"hwy\" , data = df_select , height = 7 , robust = True , palette = 'Set1' , col = \"cyl\" , scatter_kws = dict ( s = 60 , linewidths =. 7 , edgecolors = 'black' )) # Decorations gridobj . set ( xlim = ( 0.5 , 7.5 ), ylim = ( 0 , 50 )) plt . show ()","title":"Each regression line in its own column"},{"location":"viz/viz/#jittering-with-stripplot","text":"Often multiple datapoints have exactly the same X and Y values. As a result, multiple points get plotted over each other and hide. To avoid this, jitter the points slightly so you can visually see them. This is convenient to do using seaborn\u2019s stripplot() . Show Code # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) # Draw Stripplot fig , ax = plt . subplots ( figsize = ( 16 , 10 ), dpi = 80 ) sns . stripplot ( df . cty , df . hwy , jitter = 0.25 , size = 8 , ax = ax , linewidth =. 5 ) # Decorations plt . title ( 'Use jittered plots to avoid overlapping of points' , fontsize = 22 ) plt . show ()","title":"Jittering with stripplot"},{"location":"viz/viz/#counts-plot","text":"Another option to avoid the problem of points overlap is the increase the size of the dot depending on how many points lie in that spot. So, larger the size of the point more is the concentration of points around that. Show Code # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) df_counts = df . groupby ([ 'hwy' , 'cty' ]) . size () . reset_index ( name = 'counts' ) # Draw Stripplot fig , ax = plt . subplots ( figsize = ( 16 , 10 ), dpi = 80 ) sns . stripplot ( df_counts . cty , df_counts . hwy , size = df_counts . counts * 2 , ax = ax ) # Decorations plt . title ( 'Counts Plot - Size of circle is bigger as more points overlap' , fontsize = 22 ) plt . show ()","title":"Counts Plot"},{"location":"viz/viz/#marginal-histogram","text":"Marginal histograms have a histogram along the X and Y axis variables. This is used to visualize the relationship between the X and Y along with the univariate distribution of the X and the Y individually. This plot if often used in exploratory data analysis (EDA). Show Code # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) # Create Fig and gridspec fig = plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) grid = plt . GridSpec ( 4 , 4 , hspace = 0.5 , wspace = 0.2 ) # Define the axes ax_main = fig . add_subplot ( grid [: - 1 , : - 1 ]) ax_right = fig . add_subplot ( grid [: - 1 , - 1 ], xticklabels = [], yticklabels = []) ax_bottom = fig . add_subplot ( grid [ - 1 , 0 : - 1 ], xticklabels = [], yticklabels = []) # Scatterplot on main ax ax_main . scatter ( 'displ' , 'hwy' , s = df . cty * 4 , c = df . manufacturer . astype ( 'category' ) . cat . codes , alpha =. 9 , data = df , cmap = \"tab10\" , edgecolors = 'gray' , linewidths =. 5 ) # histogram on the right ax_bottom . hist ( df . displ , 40 , histtype = 'stepfilled' , orientation = 'vertical' , color = 'deeppink' ) ax_bottom . invert_yaxis () # histogram in the bottom ax_right . hist ( df . hwy , 40 , histtype = 'stepfilled' , orientation = 'horizontal' , color = 'deeppink' ) # Decorations ax_main . set ( title = 'Scatterplot with Histograms \\n displ vs hwy' , xlabel = 'displ' , ylabel = 'hwy' ) ax_main . title . set_fontsize ( 20 ) for item in ([ ax_main . xaxis . label , ax_main . yaxis . label ] + ax_main . get_xticklabels () + ax_main . get_yticklabels ()): item . set_fontsize ( 14 ) xlabels = ax_main . get_xticks () . tolist () ax_main . set_xticklabels ( xlabels ) plt . show ()","title":"Marginal Histogram"},{"location":"viz/viz/#marginal-boxplot","text":"Marginal boxplot serves a similar purpose as marginal histogram. However, the boxplot helps to pinpoint the median, 25th and 75th percentiles of the X and the Y. Show Code # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) # Create Fig and gridspec fig = plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) grid = plt . GridSpec ( 4 , 4 , hspace = 0.5 , wspace = 0.2 ) # Define the axes ax_main = fig . add_subplot ( grid [: - 1 , : - 1 ]) ax_right = fig . add_subplot ( grid [: - 1 , - 1 ], xticklabels = [], yticklabels = []) ax_bottom = fig . add_subplot ( grid [ - 1 , 0 : - 1 ], xticklabels = [], yticklabels = []) # Scatterplot on main ax ax_main . scatter ( 'displ' , 'hwy' , s = df . cty * 5 , c = df . manufacturer . astype ( 'category' ) . cat . codes , alpha =. 9 , data = df , cmap = \"Set1\" , edgecolors = 'black' , linewidths =. 5 ) # Add a graph in each part sns . boxplot ( df . hwy , ax = ax_right , orient = \"v\" ) sns . boxplot ( df . displ , ax = ax_bottom , orient = \"h\" ) # Decorations ------------------ # Remove x axis name for the boxplot ax_bottom . set ( xlabel = '' ) ax_right . set ( ylabel = '' ) # Main Title, Xlabel and YLabel ax_main . set ( title = 'Scatterplot with Histograms \\n displ vs hwy' , xlabel = 'displ' , ylabel = 'hwy' ) # Set font size of different components ax_main . title . set_fontsize ( 20 ) for item in ([ ax_main . xaxis . label , ax_main . yaxis . label ] + ax_main . get_xticklabels () + ax_main . get_yticklabels ()): item . set_fontsize ( 14 ) plt . show ()","title":"Marginal Boxplot"},{"location":"viz/viz/#correllogram","text":"Correlogram is used to visually see the correlation metric between all possible pairs of numeric variables in a given dataframe (or 2D array). Show Code # Import Dataset df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) # Plot plt . figure ( figsize = ( 12 , 10 ), dpi = 80 ) sns . heatmap ( df . corr (), xticklabels = df . corr () . columns , yticklabels = df . corr () . columns , cmap = 'RdYlGn' , center = 0 , annot = True ) # Decorations plt . title ( 'Correlogram of mtcars' , fontsize = 22 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . show ()","title":"Correllogram"},{"location":"viz/viz/#pairwise-plot","text":"Pairwise plot is a favorite in exploratory analysis to understand the relationship between all possible pairs of numeric variables. It is a must have tool for bivariate analysis. Show Code # Load Dataset df = sns . load_dataset ( 'iris' ) # Plot plt . figure ( figsize = ( 10 , 8 ), dpi = 80 ) sns . pairplot ( df , kind = \"scatter\" , hue = \"species\" , plot_kws = dict ( s = 80 , edgecolor = \"white\" , linewidth = 2.5 )) plt . show () Show Code # Load Dataset df = sns . load_dataset ( 'iris' ) # Plot plt . figure ( figsize = ( 10 , 8 ), dpi = 80 ) sns . pairplot ( df , kind = \"reg\" , hue = \"species\" ) plt . show ()","title":"Pairwise Plot"},{"location":"viz/viz/#deviation","text":"","title":"Deviation"},{"location":"viz/viz/#diverging-bars","text":"If you want to see how the items are varying based on a single metric and visualize the order and amount of this variance, the diverging bars is a great tool. It helps to quickly differentiate the performance of groups in your data and is quite intuitive and instantly conveys the point. Show Code # Prepare Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) x = df . loc [:, [ 'mpg' ]] df [ 'mpg_z' ] = ( x - x . mean ()) / x . std () df [ 'colors' ] = [ 'red' if x < 0 else 'green' for x in df [ 'mpg_z' ]] df . sort_values ( 'mpg_z' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot plt . figure ( figsize = ( 14 , 10 ), dpi = 80 ) plt . hlines ( y = df . index , xmin = 0 , xmax = df . mpg_z , color = df . colors , alpha = 0.4 , linewidth = 5 ) # Decorations plt . gca () . set ( ylabel = '$Model$' , xlabel = '$Mileage$' ) plt . yticks ( df . index , df . cars , fontsize = 12 ) plt . title ( 'Diverging Bars of Car Mileage' , fontdict = { 'size' : 20 }) plt . grid ( linestyle = '--' , alpha = 0.5 ) plt . show ()","title":"Diverging Bars"},{"location":"viz/viz/#diverging-texts","text":"Diverging texts is similar to diverging bars and it preferred if you want to show the value of each items within the chart in a nice and presentable way. Show Code # Prepare Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) x = df . loc [:, [ 'mpg' ]] df [ 'mpg_z' ] = ( x - x . mean ()) / x . std () df [ 'colors' ] = [ 'red' if x < 0 else 'green' for x in df [ 'mpg_z' ]] df . sort_values ( 'mpg_z' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot plt . figure ( figsize = ( 14 , 14 ), dpi = 80 ) plt . hlines ( y = df . index , xmin = 0 , xmax = df . mpg_z ) for x , y , tex in zip ( df . mpg_z , df . index , df . mpg_z ): t = plt . text ( x , y , round ( tex , 2 ), horizontalalignment = 'right' if x < 0 else 'left' , verticalalignment = 'center' , fontdict = { 'color' : 'red' if x < 0 else 'green' , 'size' : 14 }) # Decorations plt . yticks ( df . index , df . cars , fontsize = 12 ) plt . title ( 'Diverging Text Bars of Car Mileage' , fontdict = { 'size' : 20 }) plt . grid ( linestyle = '--' , alpha = 0.5 ) plt . xlim ( - 2.5 , 2.5 ) plt . show ()","title":"Diverging Texts"},{"location":"viz/viz/#diverging-dot-plot","text":"Divering dot plot is also similar to the diverging bars. However compared to diverging bars, the absence of bars reduces the amount of contrast and disparity between the groups. Show Code # Prepare Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) x = df . loc [:, [ 'mpg' ]] df [ 'mpg_z' ] = ( x - x . mean ()) / x . std () df [ 'colors' ] = [ 'red' if x < 0 else 'darkgreen' for x in df [ 'mpg_z' ]] df . sort_values ( 'mpg_z' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot plt . figure ( figsize = ( 14 , 16 ), dpi = 80 ) plt . scatter ( df . mpg_z , df . index , s = 450 , alpha =. 6 , color = df . colors ) for x , y , tex in zip ( df . mpg_z , df . index , df . mpg_z ): t = plt . text ( x , y , round ( tex , 1 ), horizontalalignment = 'center' , verticalalignment = 'center' , fontdict = { 'color' : 'white' }) # Decorations # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . yticks ( df . index , df . cars ) plt . title ( 'Diverging Dotplot of Car Mileage' , fontdict = { 'size' : 20 }) plt . xlabel ( '$Mileage$' ) plt . grid ( linestyle = '--' , alpha = 0.5 ) plt . xlim ( - 2.5 , 2.5 ) plt . show ()","title":"Diverging Dot Plot"},{"location":"viz/viz/#diverging-lollipop-chart-with-markers","text":"Lollipop with markers provides a flexible way of visualizing the divergence by laying emphasis on any significant datapoints you want to bring attention to and give reasoning within the chart appropriately. Show Code # Prepare Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) x = df . loc [:, [ 'mpg' ]] df [ 'mpg_z' ] = ( x - x . mean ()) / x . std () df [ 'colors' ] = 'black' # color fiat differently df . loc [ df . cars == 'Fiat X1-9' , 'colors' ] = 'darkorange' df . sort_values ( 'mpg_z' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot import matplotlib.patches as patches plt . figure ( figsize = ( 14 , 16 ), dpi = 80 ) plt . hlines ( y = df . index , xmin = 0 , xmax = df . mpg_z , color = df . colors , alpha = 0.4 , linewidth = 1 ) plt . scatter ( df . mpg_z , df . index , color = df . colors , s = [ 600 if x == 'Fiat X1-9' else 300 for x in df . cars ], alpha = 0.6 ) plt . yticks ( df . index , df . cars ) plt . xticks ( fontsize = 12 ) # Annotate plt . annotate ( 'Mercedes Models' , xy = ( 0.0 , 11.0 ), xytext = ( 1.0 , 11 ), xycoords = 'data' , fontsize = 15 , ha = 'center' , va = 'center' , bbox = dict ( boxstyle = 'square' , fc = 'firebrick' ), arrowprops = dict ( arrowstyle = '-[, widthB=2.0, lengthB=1.5' , lw = 2.0 , color = 'steelblue' ), color = 'white' ) # Add Patches p1 = patches . Rectangle (( - 2.0 , - 1 ), width =. 3 , height = 3 , alpha =. 2 , facecolor = 'red' ) p2 = patches . Rectangle (( 1.5 , 27 ), width =. 8 , height = 5 , alpha =. 2 , facecolor = 'green' ) plt . gca () . add_patch ( p1 ) plt . gca () . add_patch ( p2 ) # Decorate plt . title ( 'Diverging Bars of Car Mileage' , fontdict = { 'size' : 20 }) plt . grid ( linestyle = '--' , alpha = 0.5 ) plt . show ()","title":"Diverging Lollipop Chart with Markers"},{"location":"viz/viz/#area-chart","text":"By coloring the area between the axis and the lines, the area chart throws more emphasis not just on the peaks and troughs but also the duration of the highs and lows. The longer the duration of the highs, the larger is the area under the line. Show Code import numpy as np import pandas as pd # Prepare Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/economics.csv\" , parse_dates = [ 'date' ]) . head ( 100 ) x = np . arange ( df . shape [ 0 ]) y_returns = ( df . psavert . diff () . fillna ( 0 ) / df . psavert . shift ( 1 )) . fillna ( 0 ) * 100 # Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) plt . fill_between ( x [ 1 :], y_returns [ 1 :], 0 , where = y_returns [ 1 :] >= 0 , facecolor = 'green' , interpolate = True , alpha = 0.7 ) plt . fill_between ( x [ 1 :], y_returns [ 1 :], 0 , where = y_returns [ 1 :] <= 0 , facecolor = 'red' , interpolate = True , alpha = 0.7 ) # Annotate plt . annotate ( 'Peak \\n 1975' , xy = ( 94.0 , 21.0 ), xytext = ( 88.0 , 28 ), bbox = dict ( boxstyle = 'square' , fc = 'firebrick' ), arrowprops = dict ( facecolor = 'steelblue' , shrink = 0.05 ), fontsize = 15 , color = 'white' ) # Decorations xtickvals = [ str ( m )[: 3 ] . upper () + \"-\" + str ( y ) for y , m in zip ( df . date . dt . year , df . date . dt . month_name ())] plt . gca () . set_xticks ( x [:: 6 ]) plt . gca () . set_xticklabels ( xtickvals [:: 6 ], rotation = 90 , fontdict = { 'horizontalalignment' : 'center' , 'verticalalignment' : 'center_baseline' }) plt . ylim ( - 35 , 35 ) plt . xlim ( 1 , 100 ) plt . title ( \"Month Economics Return %\" , fontsize = 22 ) plt . ylabel ( 'Monthly returns %' ) plt . grid ( alpha = 0.5 ) plt . show ()","title":"Area Chart"},{"location":"viz/viz/#ranking","text":"","title":"Ranking"},{"location":"viz/viz/#ordered-bar-chart","text":"Ordered bar chart conveys the rank order of the items effectively. But adding the value of the metric above the chart, the user gets the precise information from the chart itself. Show Code # Prepare Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) df = df_raw [[ 'cty' , 'manufacturer' ]] . groupby ( 'manufacturer' ) . apply ( lambda x : x . mean ()) df . sort_values ( 'cty' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot import matplotlib.patches as patches fig , ax = plt . subplots ( figsize = ( 16 , 10 ), facecolor = 'white' , dpi = 80 ) ax . vlines ( x = df . index , ymin = 0 , ymax = df . cty , color = 'firebrick' , alpha = 0.7 , linewidth = 20 ) # Annotate Text for i , cty in enumerate ( df . cty ): ax . text ( i , cty + 0.5 , round ( cty , 1 ), horizontalalignment = 'center' ) # Title, Label, Ticks and Ylim ax . set_title ( 'Bar Chart for Highway Mileage' , fontdict = { 'size' : 22 }) ax . set ( ylabel = 'Miles Per Gallon' , ylim = ( 0 , 30 )) plt . xticks ( df . index , df . manufacturer . str . upper (), rotation = 60 , horizontalalignment = 'right' , fontsize = 12 ) # Add patches to color the X axis labels p1 = patches . Rectangle (( . 57 , - 0.005 ), width =. 33 , height =. 13 , alpha =. 1 , facecolor = 'green' , transform = fig . transFigure ) p2 = patches . Rectangle (( . 124 , - 0.005 ), width =. 446 , height =. 13 , alpha =. 1 , facecolor = 'red' , transform = fig . transFigure ) fig . add_artist ( p1 ) fig . add_artist ( p2 ) plt . show ()","title":"Ordered Bar Chart"},{"location":"viz/viz/#lollipop-chart","text":"Lollipop chart serves a similar purpose as a ordered bar chart in a visually pleasing way. Show Code # Prepare Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) df = df_raw [[ 'cty' , 'manufacturer' ]] . groupby ( 'manufacturer' ) . apply ( lambda x : x . mean ()) df . sort_values ( 'cty' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot fig , ax = plt . subplots ( figsize = ( 16 , 10 ), dpi = 80 ) ax . vlines ( x = df . index , ymin = 0 , ymax = df . cty , color = 'firebrick' , alpha = 0.7 , linewidth = 2 ) ax . scatter ( x = df . index , y = df . cty , s = 75 , color = 'firebrick' , alpha = 0.7 ) # Title, Label, Ticks and Ylim ax . set_title ( 'Lollipop Chart for Highway Mileage' , fontdict = { 'size' : 22 }) ax . set_ylabel ( 'Miles Per Gallon' ) ax . set_xticks ( df . index ) ax . set_xticklabels ( df . manufacturer . str . upper (), rotation = 60 , fontdict = { 'horizontalalignment' : 'right' , 'size' : 12 }) ax . set_ylim ( 0 , 30 ) # Annotate for row in df . itertuples (): ax . text ( row . Index , row . cty +. 5 , s = round ( row . cty , 2 ), horizontalalignment = 'center' , verticalalignment = 'bottom' , fontsize = 14 ) plt . show ()","title":"Lollipop Chart"},{"location":"viz/viz/#dot-plot","text":"The dot plot conveys the rank order of the items. And since it is aligned along the horizontal axis, you can visualize how far the points are from each other more easily. Show Code # Prepare Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) df = df_raw [[ 'cty' , 'manufacturer' ]] . groupby ( 'manufacturer' ) . apply ( lambda x : x . mean ()) df . sort_values ( 'cty' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot fig , ax = plt . subplots ( figsize = ( 16 , 10 ), dpi = 80 ) ax . hlines ( y = df . index , xmin = 11 , xmax = 26 , color = 'gray' , alpha = 0.7 , linewidth = 1 , linestyles = 'dashdot' ) ax . scatter ( y = df . index , x = df . cty , s = 75 , color = 'firebrick' , alpha = 0.7 ) # Title, Label, Ticks and Ylim ax . set_title ( 'Dot Plot for Highway Mileage' , fontdict = { 'size' : 22 }) ax . set_xlabel ( 'Miles Per Gallon' ) ax . set_yticks ( df . index ) ax . set_yticklabels ( df . manufacturer . str . title (), fontdict = { 'horizontalalignment' : 'right' }) ax . set_xlim ( 10 , 27 ) plt . show ()","title":"Dot Plot"},{"location":"viz/viz/#slope-chart","text":"Slope chart is most suitable for comparing the \u2018Before\u2019 and \u2018After\u2019 positions of a given person/item. Show Code import matplotlib.lines as mlines # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/gdppercap.csv\" ) left_label = [ str ( c ) + ', ' + str ( round ( y )) for c , y in zip ( df . continent , df [ '1952' ])] right_label = [ str ( c ) + ', ' + str ( round ( y )) for c , y in zip ( df . continent , df [ '1957' ])] klass = [ 'red' if ( y1 - y2 ) < 0 else 'green' for y1 , y2 in zip ( df [ '1952' ], df [ '1957' ])] # draw line # https://stackoverflow.com/questions/36470343/how-to-draw-a-line-with-matplotlib/36479941 def newline ( p1 , p2 , color = 'black' ): ax = plt . gca () l = mlines . Line2D ([ p1 [ 0 ], p2 [ 0 ]], [ p1 [ 1 ], p2 [ 1 ]], color = 'red' if p1 [ 1 ] - p2 [ 1 ] > 0 else 'green' , marker = 'o' , markersize = 6 ) ax . add_line ( l ) return l fig , ax = plt . subplots ( 1 , 1 , figsize = ( 14 , 14 ), dpi = 80 ) # Vertical Lines ax . vlines ( x = 1 , ymin = 500 , ymax = 13000 , color = 'black' , alpha = 0.7 , linewidth = 1 , linestyles = 'dotted' ) ax . vlines ( x = 3 , ymin = 500 , ymax = 13000 , color = 'black' , alpha = 0.7 , linewidth = 1 , linestyles = 'dotted' ) # Points ax . scatter ( y = df [ '1952' ], x = np . repeat ( 1 , df . shape [ 0 ]), s = 10 , color = 'black' , alpha = 0.7 ) ax . scatter ( y = df [ '1957' ], x = np . repeat ( 3 , df . shape [ 0 ]), s = 10 , color = 'black' , alpha = 0.7 ) # Line Segmentsand Annotation for p1 , p2 , c in zip ( df [ '1952' ], df [ '1957' ], df [ 'continent' ]): newline ([ 1 , p1 ], [ 3 , p2 ]) ax . text ( 1 - 0.05 , p1 , c + ', ' + str ( round ( p1 )), horizontalalignment = 'right' , verticalalignment = 'center' , fontdict = { 'size' : 14 }) ax . text ( 3 + 0.05 , p2 , c + ', ' + str ( round ( p2 )), horizontalalignment = 'left' , verticalalignment = 'center' , fontdict = { 'size' : 14 }) # 'Before' and 'After' Annotations ax . text ( 1 - 0.05 , 13000 , 'BEFORE' , horizontalalignment = 'right' , verticalalignment = 'center' , fontdict = { 'size' : 18 , 'weight' : 700 }) ax . text ( 3 + 0.05 , 13000 , 'AFTER' , horizontalalignment = 'left' , verticalalignment = 'center' , fontdict = { 'size' : 18 , 'weight' : 700 }) # Decoration ax . set_title ( \"Slopechart: Comparing GDP Per Capita between 1952 vs 1957\" , fontdict = { 'size' : 22 }) ax . set ( xlim = ( 0 , 4 ), ylim = ( 0 , 14000 ), ylabel = 'Mean GDP Per Capita' ) ax . set_xticks ([ 1 , 3 ]) ax . set_xticklabels ([ \"1952\" , \"1957\" ]) plt . yticks ( np . arange ( 500 , 13000 , 2000 ), fontsize = 12 ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( . 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 0 ) plt . gca () . spines [ \"right\" ] . set_alpha ( . 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 0 ) plt . show ()","title":"Slope Chart"},{"location":"viz/viz/#dumbbell-plot","text":"Dumbbell plot conveys the \u2018before\u2019 and \u2018after\u2019 positions of various items along with the rank ordering of the items. Its very useful if you want to visualize the effect of a particular project / initiative on different objects. Show Code import matplotlib.lines as mlines # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/health.csv\" ) df . sort_values ( 'pct_2014' , inplace = True ) df . reset_index ( inplace = True ) # Func to draw line segment def newline ( p1 , p2 , color = 'black' ): ax = plt . gca () l = mlines . Line2D ([ p1 [ 0 ], p2 [ 0 ]], [ p1 [ 1 ], p2 [ 1 ]], color = 'skyblue' ) ax . add_line ( l ) return l # Figure and Axes fig , ax = plt . subplots ( 1 , 1 , figsize = ( 14 , 14 ), facecolor = '#f7f7f7' , dpi = 80 ) # Vertical Lines ax . vlines ( x =. 05 , ymin = 0 , ymax = 26 , color = 'black' , alpha = 1 , linewidth = 1 , linestyles = 'dotted' ) ax . vlines ( x =. 10 , ymin = 0 , ymax = 26 , color = 'black' , alpha = 1 , linewidth = 1 , linestyles = 'dotted' ) ax . vlines ( x =. 15 , ymin = 0 , ymax = 26 , color = 'black' , alpha = 1 , linewidth = 1 , linestyles = 'dotted' ) ax . vlines ( x =. 20 , ymin = 0 , ymax = 26 , color = 'black' , alpha = 1 , linewidth = 1 , linestyles = 'dotted' ) # Points ax . scatter ( y = df [ 'index' ], x = df [ 'pct_2013' ], s = 50 , color = '#0e668b' , alpha = 0.7 ) ax . scatter ( y = df [ 'index' ], x = df [ 'pct_2014' ], s = 50 , color = '#a3c4dc' , alpha = 0.7 ) # Line Segments for i , p1 , p2 in zip ( df [ 'index' ], df [ 'pct_2013' ], df [ 'pct_2014' ]): newline ([ p1 , i ], [ p2 , i ]) # Decoration ax . set_facecolor ( '#f7f7f7' ) ax . set_title ( \"Dumbell Chart: Pct Change - 2013 vs 2014\" , fontdict = { 'size' : 22 }) ax . set ( xlim = ( 0 , . 25 ), ylim = ( - 1 , 27 ), ylabel = 'Mean GDP Per Capita' ) ax . set_xticks ([ . 05 , . 1 , . 15 , . 20 ]) ax . set_xticklabels ([ '5%' , '15%' , '20%' , '25%' ]) ax . set_xticklabels ([ '5%' , '15%' , '20%' , '25%' ]) plt . show ()","title":"Dumbbell Plot"},{"location":"viz/viz/#distribution","text":"","title":"Distribution"},{"location":"viz/viz/#histogram-for-continuous-variable","text":"Histogram shows the frequency distribution of a given variable. The below representation groups the frequency bars based on a categorical variable giving a greater insight about the continuous variable and the categorical variable in tandem. Show Code # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare data x_var = 'displ' groupby_var = 'class' df_agg = df . loc [:, [ x_var , groupby_var ]] . groupby ( groupby_var ) vals = [ df [ x_var ] . values . tolist () for i , df in df_agg ] # Draw plt . figure ( figsize = ( 16 , 9 ), dpi = 80 ) colors = [ plt . cm . Spectral ( i / float ( len ( vals ) - 1 )) for i in range ( len ( vals ))] n , bins , patches = plt . hist ( vals , 30 , stacked = True , density = False , color = colors [: len ( vals )]) # Decoration plt . legend ({ group : col for group , col in zip ( np . unique ( df [ groupby_var ]) . tolist (), colors [: len ( vals )])}) plt . title ( f \"Stacked Histogram of $ { x_var } $ colored by $ { groupby_var } $\" , fontsize = 22 ) plt . xlabel ( x_var ) plt . ylabel ( \"Frequency\" ) plt . ylim ( 0 , 25 ) plt . xticks ( ticks = bins [:: 3 ], labels = [ round ( b , 1 ) for b in bins [:: 3 ]]) plt . show ()","title":"Histogram for Continuous Variable"},{"location":"viz/viz/#histogram-for-categorical-variable","text":"The histogram of a categorical variable shows the frequency distribution of a that variable. By coloring the bars, you can visualize the distribution in connection with another categorical variable representing the colors. Show Code # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare data x_var = 'manufacturer' groupby_var = 'class' df_agg = df . loc [:, [ x_var , groupby_var ]] . groupby ( groupby_var ) vals = [ df [ x_var ] . values . tolist () for i , df in df_agg ] # Draw plt . figure ( figsize = ( 16 , 9 ), dpi = 80 ) colors = [ plt . cm . Spectral ( i / float ( len ( vals ) - 1 )) for i in range ( len ( vals ))] n , bins , patches = plt . hist ( vals , df [ x_var ] . unique () . __len__ (), stacked = True , density = False , color = colors [: len ( vals )]) # Decoration plt . legend ({ group : col for group , col in zip ( np . unique ( df [ groupby_var ]) . tolist (), colors [: len ( vals )])}) plt . title ( f \"Stacked Histogram of $ { x_var } $ colored by $ { groupby_var } $\" , fontsize = 22 ) plt . xlabel ( x_var ) plt . ylabel ( \"Frequency\" ) plt . ylim ( 0 , 40 ) plt . xticks ( ticks = bins , labels = np . unique ( df [ x_var ]) . tolist (), rotation = 90 , horizontalalignment = 'left' ) plt . show ()","title":"Histogram for Categorical Variable"},{"location":"viz/viz/#density-plot","text":"Density plots are a commonly used tool visualise the distribution of a continuous variable. By grouping them by the \u2018response\u2019 variable, you can inspect the relationship between the X and the Y. The below case if for representational purpose to describe how the distribution of city mileage varies with respect the number of cylinders. Show Code # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Draw Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) sns . kdeplot ( df . loc [ df [ 'cyl' ] == 4 , \"cty\" ], shade = True , color = \"g\" , label = \"Cyl=4\" , alpha =. 7 ) sns . kdeplot ( df . loc [ df [ 'cyl' ] == 5 , \"cty\" ], shade = True , color = \"deeppink\" , label = \"Cyl=5\" , alpha =. 7 ) sns . kdeplot ( df . loc [ df [ 'cyl' ] == 6 , \"cty\" ], shade = True , color = \"dodgerblue\" , label = \"Cyl=6\" , alpha =. 7 ) sns . kdeplot ( df . loc [ df [ 'cyl' ] == 8 , \"cty\" ], shade = True , color = \"orange\" , label = \"Cyl=8\" , alpha =. 7 ) # Decoration plt . title ( 'Density Plot of City Mileage by n_Cylinders' , fontsize = 22 ) plt . legend () plt . show () s","title":"Density Plot"},{"location":"viz/viz/#density-curves-with-histogram","text":"Density curve with histogram brings together the collective information conveyed by the two plots so you can have them both in a single figure instead of two. Show Code # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Draw Plot plt . figure ( figsize = ( 13 , 10 ), dpi = 80 ) sns . distplot ( df . loc [ df [ 'class' ] == 'compact' , \"cty\" ], color = \"dodgerblue\" , label = \"Compact\" , hist_kws = { 'alpha' : . 7 }, kde_kws = { 'linewidth' : 3 }) sns . distplot ( df . loc [ df [ 'class' ] == 'suv' , \"cty\" ], color = \"orange\" , label = \"SUV\" , hist_kws = { 'alpha' : . 7 }, kde_kws = { 'linewidth' : 3 }) sns . distplot ( df . loc [ df [ 'class' ] == 'minivan' , \"cty\" ], color = \"g\" , label = \"minivan\" , hist_kws = { 'alpha' : . 7 }, kde_kws = { 'linewidth' : 3 }) plt . ylim ( 0 , 0.35 ) # Decoration plt . title ( 'Density Plot of City Mileage by Vehicle Type' , fontsize = 22 ) plt . legend () plt . show ()","title":"Density Curves with Histogram"},{"location":"viz/viz/#joy-plot","text":"Joy Plot allows the density curves of different groups to overlap, it is a great way to visualize the distribution of a larger number of groups in relation to each other. It looks pleasing to the eye and conveys just the right information clearly. It can be easily built using the joypy package which is based on matplotlib . Show Code # !pip install joypy # Import Data mpg = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Draw Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) fig , axes = joypy . joyplot ( mpg , column = [ 'hwy' , 'cty' ], by = \"class\" , ylim = 'own' , figsize = ( 14 , 10 )) # Decoration plt . title ( 'Joy Plot of City and Highway Mileage by Class' , fontsize = 22 ) plt . show ()","title":"Joy Plot"},{"location":"viz/viz/#distributed-dot-plot","text":"Distributed dot plot shows the univariate distribution of points segmented by groups. The darker the points, more is the concentration of data points in that region. By coloring the median differently, the real positioning of the groups becomes apparent instantly. Show Code import matplotlib.patches as mpatches # Prepare Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) cyl_colors = { 4 : 'tab:red' , 5 : 'tab:green' , 6 : 'tab:blue' , 8 : 'tab:orange' } df_raw [ 'cyl_color' ] = df_raw . cyl . map ( cyl_colors ) # Mean and Median city mileage by make df = df_raw [[ 'cty' , 'manufacturer' ]] . groupby ( 'manufacturer' ) . apply ( lambda x : x . mean ()) df . sort_values ( 'cty' , ascending = False , inplace = True ) df . reset_index ( inplace = True ) df_median = df_raw [[ 'cty' , 'manufacturer' ]] . groupby ( 'manufacturer' ) . apply ( lambda x : x . median ()) # Draw horizontal lines fig , ax = plt . subplots ( figsize = ( 16 , 10 ), dpi = 80 ) ax . hlines ( y = df . index , xmin = 0 , xmax = 40 , color = 'gray' , alpha = 0.5 , linewidth =. 5 , linestyles = 'dashdot' ) # Draw the Dots for i , make in enumerate ( df . manufacturer ): df_make = df_raw . loc [ df_raw . manufacturer == make , :] ax . scatter ( y = np . repeat ( i , df_make . shape [ 0 ]), x = 'cty' , data = df_make , s = 75 , edgecolors = 'gray' , c = 'w' , alpha = 0.5 ) ax . scatter ( y = i , x = 'cty' , data = df_median . loc [ df_median . index == make , :], s = 75 , c = 'firebrick' ) # Annotate ax . text ( 33 , 13 , \"$red \\; dots \\; are \\; the \\: median$\" , fontdict = { 'size' : 12 }, color = 'firebrick' ) # Decorations red_patch = plt . plot ([],[], marker = \"o\" , ms = 10 , ls = \"\" , mec = None , color = 'firebrick' , label = \"Median\" ) plt . legend ( handles = red_patch ) ax . set_title ( 'Distribution of City Mileage by Make' , fontdict = { 'size' : 22 }) ax . set_xlabel ( 'Miles Per Gallon (City)' , alpha = 0.7 ) ax . set_yticks ( df . index ) ax . set_yticklabels ( df . manufacturer . str . title (), fontdict = { 'horizontalalignment' : 'right' }, alpha = 0.7 ) ax . set_xlim ( 1 , 40 ) plt . xticks ( alpha = 0.7 ) plt . gca () . spines [ \"top\" ] . set_visible ( False ) plt . gca () . spines [ \"bottom\" ] . set_visible ( False ) plt . gca () . spines [ \"right\" ] . set_visible ( False ) plt . gca () . spines [ \"left\" ] . set_visible ( False ) plt . grid ( axis = 'both' , alpha =. 4 , linewidth =. 1 ) plt . show ()","title":"Distributed Dot Plot"},{"location":"viz/viz/#box-plot","text":"Box plots are a great way to visualize the distribution, keeping the median, 25th 75th quartiles and the outliers in mind. However, you need to be careful about interpreting the size the boxes which can potentially distort the number of points contained within that group. So, manually providing the number of observations in each box can help overcome this drawback. For example, the first two boxes on the left have boxes of the same size even though they have 5 and 47 obs respectively. So writing the number of observations in that group becomes necessary. Show Code # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Draw Plot plt . figure ( figsize = ( 13 , 10 ), dpi = 80 ) sns . boxplot ( x = 'class' , y = 'hwy' , data = df , notch = False ) # Add N Obs inside boxplot (optional) def add_n_obs ( df , group_col , y ): medians_dict = { grp [ 0 ]: grp [ 1 ][ y ] . median () for grp in df . groupby ( group_col )} xticklabels = [ x . get_text () for x in plt . gca () . get_xticklabels ()] n_obs = df . groupby ( group_col )[ y ] . size () . values for ( x , xticklabel ), n_ob in zip ( enumerate ( xticklabels ), n_obs ): plt . text ( x , medians_dict [ xticklabel ] * 1.01 , \"#obs : \" + str ( n_ob ), horizontalalignment = 'center' , fontdict = { 'size' : 14 }, color = 'white' ) add_n_obs ( df , group_col = 'class' , y = 'hwy' ) # Decoration plt . title ( 'Box Plot of Highway Mileage by Vehicle Class' , fontsize = 22 ) plt . ylim ( 10 , 40 ) plt . show ()","title":"Box Plot"},{"location":"viz/viz/#dot-box-plot","text":"Dot + Box plot Conveys similar information as a boxplot split in groups. The dots, in addition, gives a sense of how many data points lie within each group. Show Code # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Draw Plot plt . figure ( figsize = ( 13 , 10 ), dpi = 80 ) sns . boxplot ( x = 'class' , y = 'hwy' , data = df , hue = 'cyl' ) sns . stripplot ( x = 'class' , y = 'hwy' , data = df , color = 'black' , size = 3 , jitter = 1 ) for i in range ( len ( df [ 'class' ] . unique ()) - 1 ): plt . vlines ( i +. 5 , 10 , 45 , linestyles = 'solid' , colors = 'gray' , alpha = 0.2 ) # Decoration plt . title ( 'Box Plot of Highway Mileage by Vehicle Class' , fontsize = 22 ) plt . legend ( title = 'Cylinders' ) plt . show ()","title":"Dot + Box Plot"},{"location":"viz/viz/#violin-plot","text":"Violin plot is a visually pleasing alternative to box plots. The shape or area of the violin depends on the number of observations it holds. However, the violin plots can be harder to read and it not commonly used in professional settings. Show Code # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Draw Plot plt . figure ( figsize = ( 13 , 10 ), dpi = 80 ) sns . violinplot ( x = 'class' , y = 'hwy' , data = df , scale = 'width' , inner = 'quartile' ) # Decoration plt . title ( 'Violin Plot of Highway Mileage by Vehicle Class' , fontsize = 22 ) plt . show ()","title":"Violin Plot"},{"location":"viz/viz/#population-pyramid","text":"Population pyramid can be used to show either the distribution of the groups ordered by the volumne. Or it can also be used to show the stage-by-stage filtering of the population as it is used below to show how many people pass through each stage of a marketing funnel. Show Code # Read data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/email_campaign_funnel.csv\" ) # Draw Plot plt . figure ( figsize = ( 13 , 10 ), dpi = 80 ) group_col = 'Gender' order_of_bars = df . Stage . unique ()[:: - 1 ] colors = [ plt . cm . Spectral ( i / float ( len ( df [ group_col ] . unique ()) - 1 )) for i in range ( len ( df [ group_col ] . unique ()))] for c , group in zip ( colors , df [ group_col ] . unique ()): sns . barplot ( x = 'Users' , y = 'Stage' , data = df . loc [ df [ group_col ] == group , :], order = order_of_bars , color = c , label = group ) # Decorations plt . xlabel ( \"$Users$\" ) plt . ylabel ( \"Stage of Purchase\" ) plt . yticks ( fontsize = 12 ) plt . title ( \"Population Pyramid of the Marketing Funnel\" , fontsize = 22 ) plt . legend () plt . show ()","title":"Population Pyramid"},{"location":"viz/viz/#categorical-plots","text":"Categorical plots provided by the seaborn library can be used to visualize the counts distribution of 2 ore more categorical variables in relation to each other. Show Code # Load Dataset titanic = sns . load_dataset ( \"titanic\" ) # Plot g = sns . catplot ( \"alive\" , col = \"deck\" , col_wrap = 4 , data = titanic [ titanic . deck . notnull ()], kind = \"count\" , height = 3.5 , aspect =. 8 , palette = 'tab20' ) fig . suptitle ( 'sf' ) plt . show () Show Code # Load Dataset titanic = sns . load_dataset ( \"titanic\" ) # Plot sns . catplot ( x = \"age\" , y = \"embark_town\" , hue = \"sex\" , col = \"class\" , data = titanic [ titanic . embark_town . notnull ()], orient = \"h\" , height = 5 , aspect = 1 , palette = \"tab10\" , kind = \"violin\" , dodge = True , cut = 0 , bw =. 2 )","title":"Categorical Plots"},{"location":"viz/viz/#composition","text":"","title":"Composition"},{"location":"viz/viz/#waffle-chart","text":"The waffle chart can be created using the pywaffle package and is used to show the compositions of groups in a larger population. Show Code #! pip install pywaffle # Reference: https://stackoverflow.com/questions/41400136/how-to-do-waffle-charts-in-python-square-piechart from pywaffle import Waffle # Import df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare Data df = df_raw . groupby ( 'class' ) . size () . reset_index ( name = 'counts' ) n_categories = df . shape [ 0 ] colors = [ plt . cm . inferno_r ( i / float ( n_categories )) for i in range ( n_categories )] # Draw Plot and Decorate fig = plt . figure ( FigureClass = Waffle , plots = { '111' : { 'values' : df [ 'counts' ], 'labels' : [ \" {0} ( {1} )\" . format ( n [ 0 ], n [ 1 ]) for n in df [[ 'class' , 'counts' ]] . itertuples ()], 'legend' : { 'loc' : 'upper left' , 'bbox_to_anchor' : ( 1.05 , 1 ), 'fontsize' : 12 }, 'title' : { 'label' : '# Vehicles by Class' , 'loc' : 'center' , 'fontsize' : 18 } }, }, rows = 7 , colors = colors , figsize = ( 16 , 9 ) ) Show Code #! pip install pywaffle from pywaffle import Waffle # Import # df_raw = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\") # Prepare Data # By Class Data df_class = df_raw . groupby ( 'class' ) . size () . reset_index ( name = 'counts_class' ) n_categories = df_class . shape [ 0 ] colors_class = [ plt . cm . Set3 ( i / float ( n_categories )) for i in range ( n_categories )] # By Cylinders Data df_cyl = df_raw . groupby ( 'cyl' ) . size () . reset_index ( name = 'counts_cyl' ) n_categories = df_cyl . shape [ 0 ] colors_cyl = [ plt . cm . Spectral ( i / float ( n_categories )) for i in range ( n_categories )] # By Make Data df_make = df_raw . groupby ( 'manufacturer' ) . size () . reset_index ( name = 'counts_make' ) n_categories = df_make . shape [ 0 ] colors_make = [ plt . cm . tab20b ( i / float ( n_categories )) for i in range ( n_categories )] # Draw Plot and Decorate fig = plt . figure ( FigureClass = Waffle , plots = { '311' : { 'values' : df_class [ 'counts_class' ], 'labels' : [ \" {1} \" . format ( n [ 0 ], n [ 1 ]) for n in df_class [[ 'class' , 'counts_class' ]] . itertuples ()], 'legend' : { 'loc' : 'upper left' , 'bbox_to_anchor' : ( 1.05 , 1 ), 'fontsize' : 12 , 'title' : 'Class' }, 'title' : { 'label' : '# Vehicles by Class' , 'loc' : 'center' , 'fontsize' : 18 }, 'colors' : colors_class }, '312' : { 'values' : df_cyl [ 'counts_cyl' ], 'labels' : [ \" {1} \" . format ( n [ 0 ], n [ 1 ]) for n in df_cyl [[ 'cyl' , 'counts_cyl' ]] . itertuples ()], 'legend' : { 'loc' : 'upper left' , 'bbox_to_anchor' : ( 1.05 , 1 ), 'fontsize' : 12 , 'title' : 'Cyl' }, 'title' : { 'label' : '# Vehicles by Cyl' , 'loc' : 'center' , 'fontsize' : 18 }, 'colors' : colors_cyl }, '313' : { 'values' : df_make [ 'counts_make' ], 'labels' : [ \" {1} \" . format ( n [ 0 ], n [ 1 ]) for n in df_make [[ 'manufacturer' , 'counts_make' ]] . itertuples ()], 'legend' : { 'loc' : 'upper left' , 'bbox_to_anchor' : ( 1.05 , 1 ), 'fontsize' : 12 , 'title' : 'Manufacturer' }, 'title' : { 'label' : '# Vehicles by Make' , 'loc' : 'center' , 'fontsize' : 18 }, 'colors' : colors_make } }, rows = 9 , figsize = ( 16 , 14 ) )","title":"Waffle Chart"},{"location":"viz/viz/#pie-chart","text":"Pie chart is a classic way to show the composition of groups. However, its not generally advisable to use nowadays because the area of the pie portions can sometimes become misleading. So, if you are to use pie chart, its highly recommended to explicitly write down the percentage or numbers for each portion of the pie. Show Code # Import df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare Data df = df_raw . groupby ( 'class' ) . size () # Make the plot with pandas df . plot ( kind = 'pie' , subplots = True , figsize = ( 8 , 8 ), dpi = 80 ) plt . title ( \"Pie Chart of Vehicle Class - Bad\" ) plt . ylabel ( \"\" ) plt . show () Show Code # Import df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare Data df = df_raw . groupby ( 'class' ) . size () . reset_index ( name = 'counts' ) # Draw Plot fig , ax = plt . subplots ( figsize = ( 12 , 7 ), subplot_kw = dict ( aspect = \"equal\" ), dpi = 80 ) data = df [ 'counts' ] categories = df [ 'class' ] explode = [ 0 , 0 , 0 , 0 , 0 , 0.1 , 0 ] def func ( pct , allvals ): absolute = int ( pct / 100. * np . sum ( allvals )) return \" {:.1f} % ( {:d} )\" . format ( pct , absolute ) wedges , texts , autotexts = ax . pie ( data , autopct = lambda pct : func ( pct , data ), textprops = dict ( color = \"w\" ), colors = plt . cm . Dark2 . colors , startangle = 140 , explode = explode ) # Decoration ax . legend ( wedges , categories , title = \"Vehicle Class\" , loc = \"center left\" , bbox_to_anchor = ( 1 , 0 , 0.5 , 1 )) plt . setp ( autotexts , size = 10 , weight = 700 ) ax . set_title ( \"Class of Vehicles: Pie Chart\" ) plt . show ()","title":"Pie Chart"},{"location":"viz/viz/#treemap","text":"Tree map is similar to a pie chart and it does a better work without misleading the contributions by each group. Show Code # pip install squarify import squarify # Import Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare Data df = df_raw . groupby ( 'class' ) . size () . reset_index ( name = 'counts' ) labels = df . apply ( lambda x : str ( x [ 0 ]) + \" \\n (\" + str ( x [ 1 ]) + \")\" , axis = 1 ) sizes = df [ 'counts' ] . values . tolist () colors = [ plt . cm . Spectral ( i / float ( len ( labels ))) for i in range ( len ( labels ))] # Draw Plot plt . figure ( figsize = ( 12 , 8 ), dpi = 80 ) squarify . plot ( sizes = sizes , label = labels , color = colors , alpha =. 8 ) # Decorate plt . title ( 'Treemap of Vechile Class' ) plt . axis ( 'off' ) plt . show ()","title":"Treemap"},{"location":"viz/viz/#bar-chart","text":"Bar chart is a classic way of visualizing items based on counts or any given metric. In below chart, I have used a different color for each item, but you might typically want to pick one color for all items unless you to color them by groups. The color names get stored inside all_colors in the code below. You can change the color of the bars by setting the color parameter in plt.plot() . Show Code import random # Import Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare Data df = df_raw . groupby ( 'manufacturer' ) . size () . reset_index ( name = 'counts' ) n = df [ 'manufacturer' ] . unique () . __len__ () + 1 all_colors = list ( plt . cm . colors . cnames . keys ()) random . seed ( 100 ) c = random . choices ( all_colors , k = n ) # Plot Bars plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) plt . bar ( df [ 'manufacturer' ], df [ 'counts' ], color = c , width =. 5 ) for i , val in enumerate ( df [ 'counts' ] . values ): plt . text ( i , val , float ( val ), horizontalalignment = 'center' , verticalalignment = 'bottom' , fontdict = { 'fontweight' : 500 , 'size' : 12 }) # Decoration plt . gca () . set_xticklabels ( df [ 'manufacturer' ], rotation = 60 , horizontalalignment = 'right' ) plt . title ( \"Number of Vehicles by Manaufacturers\" , fontsize = 22 ) plt . ylabel ( '# Vehicles' ) plt . ylim ( 0 , 45 ) plt . show ()","title":"Bar Chart"},{"location":"viz/viz/#change","text":"","title":"Change"},{"location":"viz/viz/#time-series-plot","text":"Time series plot is used to visualise how a given metric changes over time. Here you can see how the Air Passenger traffic changed between 1949 and 1969. Show Code # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/AirPassengers.csv' ) # Draw Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) plt . plot ( 'date' , 'traffic' , data = df , color = 'tab:red' ) # Decoration plt . ylim ( 50 , 750 ) xtick_location = df . index . tolist ()[:: 12 ] xtick_labels = [ x [ - 4 :] for x in df . date . tolist ()[:: 12 ]] plt . xticks ( ticks = xtick_location , labels = xtick_labels , rotation = 0 , fontsize = 12 , horizontalalignment = 'center' , alpha =. 7 ) plt . yticks ( fontsize = 12 , alpha =. 7 ) plt . title ( \"Air Passengers Traffic (1949 - 1969)\" , fontsize = 22 ) plt . grid ( axis = 'both' , alpha =. 3 ) # Remove borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0.0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( 0.3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0.0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( 0.3 ) plt . show ()","title":"Time Series Plot"},{"location":"viz/viz/#time-series-with-peaks-and-troughs-annotated","text":"The below time series plots all the the peaks and troughs and annotates the occurence of selected special events. Show Code # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/AirPassengers.csv' ) # Get the Peaks and Troughs data = df [ 'traffic' ] . values doublediff = np . diff ( np . sign ( np . diff ( data ))) peak_locations = np . where ( doublediff == - 2 )[ 0 ] + 1 doublediff2 = np . diff ( np . sign ( np . diff ( - 1 * data ))) trough_locations = np . where ( doublediff2 == - 2 )[ 0 ] + 1 # Draw Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) plt . plot ( 'date' , 'traffic' , data = df , color = 'tab:blue' , label = 'Air Traffic' ) plt . scatter ( df . date [ peak_locations ], df . traffic [ peak_locations ], marker = mpl . markers . CARETUPBASE , color = 'tab:green' , s = 100 , label = 'Peaks' ) plt . scatter ( df . date [ trough_locations ], df . traffic [ trough_locations ], marker = mpl . markers . CARETDOWNBASE , color = 'tab:red' , s = 100 , label = 'Troughs' ) # Annotate for t , p in zip ( trough_locations [ 1 :: 5 ], peak_locations [:: 3 ]): plt . text ( df . date [ p ], df . traffic [ p ] + 15 , df . date [ p ], horizontalalignment = 'center' , color = 'darkgreen' ) plt . text ( df . date [ t ], df . traffic [ t ] - 35 , df . date [ t ], horizontalalignment = 'center' , color = 'darkred' ) # Decoration plt . ylim ( 50 , 750 ) xtick_location = df . index . tolist ()[:: 6 ] xtick_labels = df . date . tolist ()[:: 6 ] plt . xticks ( ticks = xtick_location , labels = xtick_labels , rotation = 90 , fontsize = 12 , alpha =. 7 ) plt . title ( \"Peak and Troughs of Air Passengers Traffic (1949 - 1969)\" , fontsize = 22 ) plt . yticks ( fontsize = 12 , alpha =. 7 ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( . 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( . 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . legend ( loc = 'upper left' ) plt . grid ( axis = 'y' , alpha =. 3 ) plt . show ()","title":"Time Series with Peaks and Troughs Annotated"},{"location":"viz/viz/#autocorrelation-acf-and-partial-autocorrelation-pacf-plot","text":"The ACF plot shows the correlation of the time series with its own lags. Each vertical line (on the autocorrelation plot) represents the correlation between the series and its lag starting from lag 0. The blue shaded region in the plot is the significance level. Those lags that lie above the blue line are the significant lags. So how to interpret this? For AirPassengers, we see upto 14 lags have crossed the blue line and so are significant. This means, the Air Passengers traffic seen upto 14 years back has an influence on the traffic seen today. PACF on the other had shows the autocorrelation of any given lag (of time series) against the current series, but with the contributions of the lags-inbetween removed. Show Code from statsmodels.graphics.tsaplots import plot_acf , plot_pacf # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/AirPassengers.csv' ) # Draw Plot fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 ), dpi = 80 ) plot_acf ( df . traffic . tolist (), ax = ax1 , lags = 50 ) plot_pacf ( df . traffic . tolist (), ax = ax2 , lags = 20 ) # Decorate # lighten the borders ax1 . spines [ \"top\" ] . set_alpha ( . 3 ); ax2 . spines [ \"top\" ] . set_alpha ( . 3 ) ax1 . spines [ \"bottom\" ] . set_alpha ( . 3 ); ax2 . spines [ \"bottom\" ] . set_alpha ( . 3 ) ax1 . spines [ \"right\" ] . set_alpha ( . 3 ); ax2 . spines [ \"right\" ] . set_alpha ( . 3 ) ax1 . spines [ \"left\" ] . set_alpha ( . 3 ); ax2 . spines [ \"left\" ] . set_alpha ( . 3 ) # font size of tick labels ax1 . tick_params ( axis = 'both' , labelsize = 12 ) ax2 . tick_params ( axis = 'both' , labelsize = 12 ) plt . show ()","title":"Autocorrelation (ACF) and Partial Autocorrelation (PACF) Plot"},{"location":"viz/viz/#cross-correlation-plot","text":"Cross correlation plot shows the lags of two time series with each other. Show Code import statsmodels.tsa.stattools as stattools # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/mortality.csv' ) x = df [ 'mdeaths' ] y = df [ 'fdeaths' ] # Compute Cross Correlations ccs = stattools . ccf ( x , y )[: 100 ] nlags = len ( ccs ) # Compute the Significance level # ref: https://stats.stackexchange.com/questions/3115/cross-correlation-significance-in-r/3128#3128 conf_level = 2 / np . sqrt ( nlags ) # Draw Plot plt . figure ( figsize = ( 12 , 7 ), dpi = 80 ) plt . hlines ( 0 , xmin = 0 , xmax = 100 , color = 'gray' ) # 0 axis plt . hlines ( conf_level , xmin = 0 , xmax = 100 , color = 'gray' ) plt . hlines ( - conf_level , xmin = 0 , xmax = 100 , color = 'gray' ) plt . bar ( x = np . arange ( len ( ccs )), height = ccs , width =. 3 ) # Decoration plt . title ( '$Cross\\; Correlation\\; Plot:\\; mdeaths\\; vs\\; fdeaths$' , fontsize = 22 ) plt . xlim ( 0 , len ( ccs )) plt . show ()","title":"Cross Correlation plot"},{"location":"viz/viz/#time-series-decomposition-plot","text":"Time series decomposition plot shows the break down of the time series into trend, seasonal and residual components. Show Code from statsmodels.tsa.seasonal import seasonal_decompose from dateutil.parser import parse # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/AirPassengers.csv' ) dates = pd . DatetimeIndex ([ parse ( d ) . strftime ( '%Y-%m-01' ) for d in df [ 'date' ]]) df . set_index ( dates , inplace = True ) # Decompose result = seasonal_decompose ( df [ 'traffic' ], model = 'multiplicative' ) # Plot plt . rcParams . update ({ 'figure.figsize' : ( 10 , 10 )}) result . plot () . suptitle ( 'Time Series Decomposition of Air Passengers' ) plt . show ()","title":"Time Series Decomposition Plot"},{"location":"viz/viz/#multiple-time-series","text":"You can plot multiple time series that measures the same value on the same chart as shown below. Show Code # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/mortality.csv' ) # Define the upper limit, lower limit, interval of Y axis and colors y_LL = 100 y_UL = int ( df . iloc [:, 1 :] . max () . max () * 1.1 ) y_interval = 400 mycolors = [ 'tab:red' , 'tab:blue' , 'tab:green' , 'tab:orange' ] # Draw Plot and Annotate fig , ax = plt . subplots ( 1 , 1 , figsize = ( 16 , 9 ), dpi = 80 ) columns = df . columns [ 1 :] for i , column in enumerate ( columns ): plt . plot ( df . date . values , df [ column ] . values , lw = 1.5 , color = mycolors [ i ]) plt . text ( df . shape [ 0 ] + 1 , df [ column ] . values [ - 1 ], column , fontsize = 14 , color = mycolors [ i ]) # Draw Tick lines for y in range ( y_LL , y_UL , y_interval ): plt . hlines ( y , xmin = 0 , xmax = 71 , colors = 'black' , alpha = 0.3 , linestyles = \"--\" , lw = 0.5 ) # Decorations plt . tick_params ( axis = \"both\" , which = \"both\" , bottom = False , top = False , labelbottom = True , left = False , right = False , labelleft = True ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . title ( 'Number of Deaths from Lung Diseases in the UK (1974-1979)' , fontsize = 22 ) plt . yticks ( range ( y_LL , y_UL , y_interval ), [ str ( y ) for y in range ( y_LL , y_UL , y_interval )], fontsize = 12 ) plt . xticks ( range ( 0 , df . shape [ 0 ], 12 ), df . date . values [:: 12 ], horizontalalignment = 'left' , fontsize = 12 ) plt . ylim ( y_LL , y_UL ) plt . xlim ( - 2 , 80 ) plt . show ()","title":"Multiple Time Series"},{"location":"viz/viz/#plotting-with-different-scales-using-secondary-y-axis","text":"If you want to show two time series that measures two different quantities at the same point in time, you can plot the second series againt the secondary Y axis on the right. Show Code # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/economics.csv\" ) x = df [ 'date' ] y1 = df [ 'psavert' ] y2 = df [ 'unemploy' ] # Plot Line1 (Left Y Axis) fig , ax1 = plt . subplots ( 1 , 1 , figsize = ( 16 , 9 ), dpi = 80 ) ax1 . plot ( x , y1 , color = 'tab:red' ) # Plot Line2 (Right Y Axis) ax2 = ax1 . twinx () # instantiate a second axes that shares the same x-axis ax2 . plot ( x , y2 , color = 'tab:blue' ) # Decorations # ax1 (left Y axis) ax1 . set_xlabel ( 'Year' , fontsize = 20 ) ax1 . tick_params ( axis = 'x' , rotation = 0 , labelsize = 12 ) ax1 . set_ylabel ( 'Personal Savings Rate' , color = 'tab:red' , fontsize = 20 ) ax1 . tick_params ( axis = 'y' , rotation = 0 , labelcolor = 'tab:red' ) ax1 . grid ( alpha =. 4 ) # ax2 (right Y axis) ax2 . set_ylabel ( \"# Unemployed (1000's)\" , color = 'tab:blue' , fontsize = 20 ) ax2 . tick_params ( axis = 'y' , labelcolor = 'tab:blue' ) ax2 . set_xticks ( np . arange ( 0 , len ( x ), 60 )) ax2 . set_xticklabels ( x [:: 60 ], rotation = 90 , fontdict = { 'fontsize' : 10 }) ax2 . set_title ( \"Personal Savings Rate vs Unemployed: Plotting in Secondary Y Axis\" , fontsize = 22 ) fig . tight_layout () plt . show ()","title":"Plotting with different scales using secondary Y axis"},{"location":"viz/viz/#time-series-with-error-bands","text":"Time series with error bands can be constructed if you have a time series dataset with multiple observations for each time point (date / timestamp). Below you can see a couple of examples based on the orders coming in at various times of the day. And another example on the number of orders arriving over a duration of 45 days. In this approach, the mean of the number of orders is denoted by the white line. And a 95% confidence bands are computed and drawn around the mean. Show Code from scipy.stats import sem # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/user_orders_hourofday.csv\" ) df_mean = df . groupby ( 'order_hour_of_day' ) . quantity . mean () df_se = df . groupby ( 'order_hour_of_day' ) . quantity . apply ( sem ) . mul ( 1.96 ) # Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) plt . ylabel ( \"# Orders\" , fontsize = 16 ) x = df_mean . index plt . plot ( x , df_mean , color = \"white\" , lw = 2 ) plt . fill_between ( x , df_mean - df_se , df_mean + df_se , color = \"#3F5D7D\" ) # Decorations # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( 1 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( 1 ) plt . xticks ( x [:: 2 ], [ str ( d ) for d in x [:: 2 ]] , fontsize = 12 ) plt . title ( \"User Orders by Hour of Day (95 % c onfidence)\" , fontsize = 22 ) plt . xlabel ( \"Hour of Day\" ) s , e = plt . gca () . get_xlim () plt . xlim ( s , e ) # Draw Horizontal Tick lines for y in range ( 8 , 20 , 2 ): plt . hlines ( y , xmin = s , xmax = e , colors = 'black' , alpha = 0.5 , linestyles = \"--\" , lw = 0.5 ) plt . show () Show Code \"Data Source: https://www.kaggle.com/olistbr/brazilian-ecommerce#olist_orders_dataset.csv\" from dateutil.parser import parse from scipy.stats import sem # Import Data df_raw = pd . read_csv ( 'https://raw.githubusercontent.com/selva86/datasets/master/orders_45d.csv' , parse_dates = [ 'purchase_time' , 'purchase_date' ]) # Prepare Data: Daily Mean and SE Bands df_mean = df_raw . groupby ( 'purchase_date' ) . quantity . mean () df_se = df_raw . groupby ( 'purchase_date' ) . quantity . apply ( sem ) . mul ( 1.96 ) # Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) plt . ylabel ( \"# Daily Orders\" , fontsize = 16 ) x = [ d . date () . strftime ( '%Y-%m- %d ' ) for d in df_mean . index ] plt . plot ( x , df_mean , color = \"white\" , lw = 2 ) plt . fill_between ( x , df_mean - df_se , df_mean + df_se , color = \"#3F5D7D\" ) # Decorations # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( 1 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( 1 ) plt . xticks ( x [:: 6 ], [ str ( d ) for d in x [:: 6 ]] , fontsize = 12 ) plt . title ( \"Daily Order Quantity of Brazilian Retail with Error Bands (95 % c onfidence)\" , fontsize = 20 ) # Axis limits s , e = plt . gca () . get_xlim () plt . xlim ( s , e - 2 ) plt . ylim ( 4 , 10 ) # Draw Horizontal Tick lines for y in range ( 5 , 10 , 1 ): plt . hlines ( y , xmin = s , xmax = e , colors = 'black' , alpha = 0.5 , linestyles = \"--\" , lw = 0.5 ) plt . show ()","title":"Time Series with Error Bands"},{"location":"viz/viz/#stacked-area-chart","text":"Stacked area chart gives an visual representation of the extent of contribution from multiple time series so that it is easy to compare against each other. Show Code # Import Data df = pd . read_csv ( 'https://raw.githubusercontent.com/selva86/datasets/master/nightvisitors.csv' ) # Decide Colors mycolors = [ 'tab:red' , 'tab:blue' , 'tab:green' , 'tab:orange' , 'tab:brown' , 'tab:grey' , 'tab:pink' , 'tab:olive' ] # Draw Plot and Annotate fig , ax = plt . subplots ( 1 , 1 , figsize = ( 16 , 9 ), dpi = 80 ) columns = df . columns [ 1 :] labs = columns . values . tolist () # Prepare data x = df [ 'yearmon' ] . values . tolist () y0 = df [ columns [ 0 ]] . values . tolist () y1 = df [ columns [ 1 ]] . values . tolist () y2 = df [ columns [ 2 ]] . values . tolist () y3 = df [ columns [ 3 ]] . values . tolist () y4 = df [ columns [ 4 ]] . values . tolist () y5 = df [ columns [ 5 ]] . values . tolist () y6 = df [ columns [ 6 ]] . values . tolist () y7 = df [ columns [ 7 ]] . values . tolist () y = np . vstack ([ y0 , y2 , y4 , y6 , y7 , y5 , y1 , y3 ]) # Plot for each column labs = columns . values . tolist () ax = plt . gca () ax . stackplot ( x , y , labels = labs , colors = mycolors , alpha = 0.8 ) # Decorations ax . set_title ( 'Night Visitors in Australian Regions' , fontsize = 18 ) ax . set ( ylim = [ 0 , 100000 ]) ax . legend ( fontsize = 10 , ncol = 4 ) plt . xticks ( x [:: 5 ], fontsize = 10 , horizontalalignment = 'center' ) plt . yticks ( np . arange ( 10000 , 100000 , 20000 ), fontsize = 10 ) plt . xlim ( x [ 0 ], x [ - 1 ]) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . show ()","title":"Stacked Area Chart"},{"location":"viz/viz/#area-chart-unstacked","text":"An unstacked area chart is used to visualize the progress (ups and downs) of two or more series with respect to each other. In the chart below, you can clearly see how the personal savings rate comes down as the median duration of unemployment increases. The unstacked area chart brings out this phenomenon nicely. Show Code # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/economics.csv\" ) # Prepare Data x = df [ 'date' ] . values . tolist () y1 = df [ 'psavert' ] . values . tolist () y2 = df [ 'uempmed' ] . values . tolist () mycolors = [ 'tab:red' , 'tab:blue' , 'tab:green' , 'tab:orange' , 'tab:brown' , 'tab:grey' , 'tab:pink' , 'tab:olive' ] columns = [ 'psavert' , 'uempmed' ] # Draw Plot fig , ax = plt . subplots ( 1 , 1 , figsize = ( 16 , 9 ), dpi = 80 ) ax . fill_between ( x , y1 = y1 , y2 = 0 , label = columns [ 1 ], alpha = 0.5 , color = mycolors [ 1 ], linewidth = 2 ) ax . fill_between ( x , y1 = y2 , y2 = 0 , label = columns [ 0 ], alpha = 0.5 , color = mycolors [ 0 ], linewidth = 2 ) # Decorations ax . set_title ( 'Personal Savings Rate vs Median Duration of Unemployment' , fontsize = 18 ) ax . set ( ylim = [ 0 , 30 ]) ax . legend ( loc = 'best' , fontsize = 12 ) plt . xticks ( x [:: 50 ], fontsize = 10 , horizontalalignment = 'center' ) plt . yticks ( np . arange ( 2.5 , 30.0 , 2.5 ), fontsize = 10 ) plt . xlim ( - 10 , x [ - 1 ]) # Draw Tick lines for y in np . arange ( 2.5 , 30.0 , 2.5 ): plt . hlines ( y , xmin = 0 , xmax = len ( x ), colors = 'black' , alpha = 0.3 , linestyles = \"--\" , lw = 0.5 ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . show ()","title":"Area Chart UnStacked"},{"location":"viz/viz/#calendar-heat-map","text":"Calendar map is an alternate and a less preferred option to visualise time based data compared to a time series. Though can be visually appealing, the numeric values are not quite evident. It is however effective in picturising the extreme values and holiday effects nicely. Show Code import matplotlib as mpl import calmap # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/yahoo.csv\" , parse_dates = [ 'date' ]) df . set_index ( 'date' , inplace = True ) # Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) calmap . calendarplot ( df [ '2014' ][ 'VIX.Close' ], fig_kws = { 'figsize' : ( 16 , 10 )}, yearlabel_kws = { 'color' : 'black' , 'fontsize' : 14 }, subplot_kws = { 'title' : 'Yahoo Stock Prices' }) plt . show ()","title":"Calendar Heat Map"},{"location":"viz/viz/#seasonal-plot","text":"The seasonal plot can be used to compare how the time series performed at same day in the previous season (year / month / week etc). Show Code from dateutil.parser import parse # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/AirPassengers.csv' ) # Prepare data df [ 'year' ] = [ parse ( d ) . year for d in df . date ] df [ 'month' ] = [ parse ( d ) . strftime ( '%b' ) for d in df . date ] years = df [ 'year' ] . unique () # Draw Plot mycolors = [ 'tab:red' , 'tab:blue' , 'tab:green' , 'tab:orange' , 'tab:brown' , 'tab:grey' , 'tab:pink' , 'tab:olive' , 'deeppink' , 'steelblue' , 'firebrick' , 'mediumseagreen' ] plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) for i , y in enumerate ( years ): plt . plot ( 'month' , 'traffic' , data = df . loc [ df . year == y , :], color = mycolors [ i ], label = y ) plt . text ( df . loc [ df . year == y , :] . shape [ 0 ] -. 9 , df . loc [ df . year == y , 'traffic' ][ - 1 :] . values [ 0 ], y , fontsize = 12 , color = mycolors [ i ]) # Decoration plt . ylim ( 50 , 750 ) plt . xlim ( - 0.3 , 11 ) plt . ylabel ( '$Air Traffic$' ) plt . yticks ( fontsize = 12 , alpha =. 7 ) plt . title ( \"Monthly Seasonal Plot: Air Passengers Traffic (1949 - 1969)\" , fontsize = 22 ) plt . grid ( axis = 'y' , alpha =. 3 ) # Remove borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0.0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( 0.5 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0.0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( 0.5 ) # plt.legend(loc='upper right', ncol=2, fontsize=12) plt . show ()","title":"Seasonal Plot"},{"location":"viz/viz/#groups","text":"","title":"Groups"},{"location":"viz/viz/#dendrogram","text":"A Dendrogram groups similar points together based on a given distance metric and organizes them in tree like links based on the point\u2019s similarity. Show Code import scipy.cluster.hierarchy as shc # Import Data df = pd . read_csv ( 'https://raw.githubusercontent.com/selva86/datasets/master/USArrests.csv' ) # Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) plt . title ( \"USArrests Dendograms\" , fontsize = 22 ) dend = shc . dendrogram ( shc . linkage ( df [[ 'Murder' , 'Assault' , 'UrbanPop' , 'Rape' ]], method = 'ward' ), labels = df . State . values , color_threshold = 100 ) plt . xticks ( fontsize = 12 ) plt . show ()","title":"Dendrogram"},{"location":"viz/viz/#cluster-plot","text":"Cluster Plot canbe used to demarcate points that belong to the same cluster. Below is a representational example to group the US states into 5 groups based on the USArrests dataset. This cluster plot uses the \u2018murder\u2019 and \u2018assault\u2019 columns as X and Y axis. Alternately you can use the first to principal components as rthe X and Y axis. Show Code from sklearn.cluster import AgglomerativeClustering from scipy.spatial import ConvexHull # Import Data df = pd . read_csv ( 'https://raw.githubusercontent.com/selva86/datasets/master/USArrests.csv' ) # Agglomerative Clustering cluster = AgglomerativeClustering ( n_clusters = 5 , affinity = 'euclidean' , linkage = 'ward' ) cluster . fit_predict ( df [[ 'Murder' , 'Assault' , 'UrbanPop' , 'Rape' ]]) # Plot plt . figure ( figsize = ( 14 , 10 ), dpi = 80 ) plt . scatter ( df . iloc [:, 0 ], df . iloc [:, 1 ], c = cluster . labels_ , cmap = 'tab10' ) # Encircle def encircle ( x , y , ax = None , ** kw ): if not ax : ax = plt . gca () p = np . c_ [ x , y ] hull = ConvexHull ( p ) poly = plt . Polygon ( p [ hull . vertices ,:], ** kw ) ax . add_patch ( poly ) # Draw polygon surrounding vertices encircle ( df . loc [ cluster . labels_ == 0 , 'Murder' ], df . loc [ cluster . labels_ == 0 , 'Assault' ], ec = \"k\" , fc = \"gold\" , alpha = 0.2 , linewidth = 0 ) encircle ( df . loc [ cluster . labels_ == 1 , 'Murder' ], df . loc [ cluster . labels_ == 1 , 'Assault' ], ec = \"k\" , fc = \"tab:blue\" , alpha = 0.2 , linewidth = 0 ) encircle ( df . loc [ cluster . labels_ == 2 , 'Murder' ], df . loc [ cluster . labels_ == 2 , 'Assault' ], ec = \"k\" , fc = \"tab:red\" , alpha = 0.2 , linewidth = 0 ) encircle ( df . loc [ cluster . labels_ == 3 , 'Murder' ], df . loc [ cluster . labels_ == 3 , 'Assault' ], ec = \"k\" , fc = \"tab:green\" , alpha = 0.2 , linewidth = 0 ) encircle ( df . loc [ cluster . labels_ == 4 , 'Murder' ], df . loc [ cluster . labels_ == 4 , 'Assault' ], ec = \"k\" , fc = \"tab:orange\" , alpha = 0.2 , linewidth = 0 ) # Decorations plt . xlabel ( 'Murder' ); plt . xticks ( fontsize = 12 ) plt . ylabel ( 'Assault' ); plt . yticks ( fontsize = 12 ) plt . title ( 'Agglomerative Clustering of USArrests (5 Groups)' , fontsize = 22 ) plt . show ()","title":"Cluster Plot"},{"location":"viz/viz/#andrews-curve","text":"Andrews Curve helps visualize if there are inherent groupings of the numerical features based on a given grouping. If the features (columns in the dataset) doesn\u2019t help discriminate the group ( cyl) , then the lines will not be well segregated as you see below. Show Code from pandas.plotting import andrews_curves # Import df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) df . drop ([ 'cars' , 'carname' ], axis = 1 , inplace = True ) # Plot plt . figure ( figsize = ( 12 , 9 ), dpi = 80 ) andrews_curves ( df , 'cyl' , colormap = 'Set1' ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . title ( 'Andrews Curves of mtcars' , fontsize = 22 ) plt . xlim ( - 3 , 3 ) plt . grid ( alpha = 0.3 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . show ()","title":"Andrews Curve"},{"location":"viz/viz/#parallel-coordinates","text":"Parallel coordinates helps to visualize if a feature helps to segregate the groups effectively. If a segregation is effected, that feature is likely going to be very useful in predicting that group. Show Code from pandas.plotting import parallel_coordinates # Import Data df_final = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/diamonds_filter.csv\" ) # Plot plt . figure ( figsize = ( 12 , 9 ), dpi = 80 ) parallel_coordinates ( df_final , 'cut' , colormap = 'Dark2' ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . title ( 'Parallel Coordinated of Diamonds' , fontsize = 22 ) plt . grid ( alpha = 0.3 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . show ()","title":"Parallel Coordinates"},{"location":"web/webscraping/","text":"Web Scraping More coming... This might just be Extract within ETL","title":"Web Scraping"},{"location":"web/webscraping/#web-scraping","text":"More coming... This might just be Extract within ETL","title":"Web Scraping"}]}